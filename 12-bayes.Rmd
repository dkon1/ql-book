\chapter{Bayes' amazing formula}
\chaptermark{Bayes' formula}

\begin{chapquote} {Oscar Wilde, \emph{The Decay of Lying}}
Man can believe the impossible, but man can never believe the improbable.
\end{chapquote} 

In the last chapter we defined the notion of independence and learned how to perform a statistical test to determine how likely a data set of two categorical variables is to have come from two independent random variables. This approach comes from the toolbox of classical ``frequentist'' statistics, which is taught in every statistics textbook in the world. It reduces statistical inference to a binary choice: reject or not reject the null hypothesis, based on the magic number called the p-value. However, this approach has deep problems, especially when applied mechanically and without understanding its limitations. Perhaps the most important limitation is that p-value based hypothesis testing does not incorporate any knowledge into its decision-making, aside from the given data. This may be reasonable at an early, exploratory stage of an experiment, but usually one has some \emph{prior knowledge} about the likelihood of the hypothesis being tested. This knowledge cannot influence the data and the calculation of the p-value, of course, but it can have a dramatic effect on the interpretation, or inference one draws from the test. In this chapter you will learn to do the following:

\begin{enumerate}

\item explain the effect or prior knowledge on interpretation of an experimental result

\item calculate post-test probability based on prior probability, test result, and conditional probabilities of the test being in error

\item use conditional statements to simulate random decisions with a given probability

\item explain why conclusions based on binary p-value testing are frequently wrong

\end{enumerate}


\section{Prior knowledge}
\label{sec:model7}
Suppose that a patient walks into a doctor's office, the doctor orders a pregnancy test, and the results indicate that the patient is pregnant. The doctor consults the published sensitivity and specificity values (which were described in the last chapter) to discover, for instance, that 99\% of positive pregnancy tests are correct. The doctor goes back to congratulate the patient with impending motherhood. Sound very reasonable, doesn't it? Would it still sound reasonable if the doctor knew that the patient is a biological male?

This is a slightly absurd example, of course, but it neatly illustrates the central point of this chapter: \textbf{prior knowledge has an effect on the inference from a test}, no matter how small or large the p-value and the power of the test. If a patient is male, his \index{probability!prior}\index{Bayesian!prior probability}\emph{prior probability} of being pregnant is 0, and that is not changed by a test, no matter how accurate (of course, no test is 100\% accurate). In this case, the positive pregnancy test must have been a false positive, even though it's unlikely, since the other possibility - that of a true positive - is impossible.

We all consider prior knowledge before coming to a conclusion. For instance, the credibility of a statement from a person very much depends on past performance: if the person is a habitual liar, you probably wouldn't put much stock in his or her words. On the other hand, if a person is known to be trustworthy, you might take their statement seriously even if it surprising. If your significant other has always been transparent and honest, even if you discover a suspiciously sexy message from someone else on his/her phone, you will listen to their explanation and consider alternative explanations, in other words, that this was a false positive. If they had abused your trust in the past, it's much more likely that this sexy text is actual evidence of cheating, and it's time to cut them loose!

In the context of science, the accumulation of knowledge is the basis of building scientific theories. Nothing in science is ever proven, unlike in mathematics, instead different statements have different degrees of certainty based on past experience. A theory that has been tested for years, for instance Newton's theory of gravity, with all experiments agreeing with its predictions (let's forget about relativistic corrections for a moment), is considered to have a very high likelihood and even called a Law of nature. If experimental data came along that challenged Newton's Law of gravity (as Einstein's relativity did) scientists would rightfully treat it much more skeptically than an experiment that agrees with prior experience. Carl Sagan summarized the effect of prior knowledge of evaluating evidence with the pithy phrase ``extraordinary claims require extraordinary evidence'' - that is, a claim that is highly unlikely based on past knowledge must be backed up by very strong data (extremely low p-value, or high power). 

%The perils of ignoring prior knowledge is illustrated in the xkcd cartoon shown in figure \ref{fig:xkcd_bayes}. In this case, the ``solar detector'' lies (makes an error) with probability 1/36, so when it tells you that the sun exploded, it has only 1/36 probability that it made a false positive error. However, based on our prior knowledge, the sun is extremely unlikely to blow up or disappear, so the alternative that the sun exploded and the detector is correct is even less likely. Although the cartoon portrays this as a disagreement between frequentists and Bayesians, it is more properly understood as a disagreement between mindless application of frequentist thought and everyone else. 

\section{Bayes' formula}
\label{sec:math7}
In this section we will formalize the process of incorporation of prior knowledge into probabilistic inference by going back to the notion of conditional probability in section \ref{sec:math6_1}. First, if you multiply both sides of the definition by $P(B)$, then we obtain the probability of the intersection of events $A$ and $B$:
$$P(A \cap B) = P(A|B) P(B); \;  P(A \cap B) = P(B|A) P(A) $$
Second, we can partition a sample space into two complementary sets, $A$ and $-A$, and then the set of $B$ can be partitioned into two parts, that intersect with $A$ and $-A$, respectively, so that the probability of $B$ is
$$P(B) = P(A \cap B) + P(-A\cap B)$$

The two formulas together lead to a very important result called the \emph{law of total probability} \citep{watkins-intro-stats}:
\begin{equation}
P(B) =  P(B|A) P(A) + P(B|-A)P(-A)
\label{eq:LTP}
\end{equation}
It may not be clear at first glance why this is useful: after all, we replaced something simple ($P(B)$) with something much more complex on the right hand side. You will see how this formula enables us to calculate quantities that are not otherwise accessible. 

\textbf{Example: probability of a negative test result}. Suppose we know that the probability of a patient having a disease is 1\% (called the \index{medical!prevalence}\emph{prevalence} of the disease in a population), and the sensitivity and specificity of the test are both 80\%. What is the probability of obtaining a negative test result for a randomly selected patient? Let us call $P(H) = 0.99$ the probability of a healthy patient and $P(D) = 0.01$ the probability of a diseased patient. Then:
$$ P(Neg) =  P(Neg | H) P(H) + P(Neg | D)P(D)  = $$
$$ = 0.8 \times 0.99 + 0.2 \times 0.01 = 0.794$$

There is still more gold in the hills of conditional probability! Take the first formula in this section, which expresses the probability $P(A \cap B) $ in two different ways.  Since the expressions are equal, we can combine them into one equation, and by dividing both sides by $P(B)$, we obtain what's known as \emph{Bayes' formula}:
$$ P(A|B) = \frac{P(B|A) P(A)}{P(B) }$$
The more useful version of Bayes' formula  \index{Bayes' formula} re-writes the denominator using the Law of total probability above, and we obtain:
\begin{equation}
P(A|B) = \frac{P(B|A)P(A)}{P(B|A) P(A) + P(B|-A)P(-A)}
\label{eq:Bayes}
\end{equation}
Bayes' formula gives us the probability of $A$ given $B$ from probabilities of $B$ given $A$ and given $-A$, and the prior (baseline) probability of $P(A)$. This is enormously useful when it is easy to calculate the conditionals one way and not the other. Among its many applications, it computes the effect of a test result with given sensitivity and specificity (conditional probabilities) on the probability of the hypothesis being true.

\subsection{positive and negative predictive values}
In reality, a doctor doesn't have the true information about the patient's health,  but rather the information from the test and hopefully some information about the population where she is working.  Let us assume we know the rate of false positives $P(Pos | H$) and the rate of false negatives $P(Neg | D)$, as well as the prevalence of the disease in the whole population $P(D)$. Then we can use Bayes' formula to answer the practical question, if the test result is positive, what is the probability the patient is actually sick\index{medical!testing}? This is called the  \index{test!positive predictive value}\emph{positive predictive value} of a test. The deep Bayesian fact is that one cannot make inferences about the health of the patient after the test without some \textbf{prior knowledge}, specifically the prevalence of the disease in the population:
$$ P(D | Pos) =  \frac{P(Pos|D)P(D)}{P(Pos|D) P(D) + P(Pos | H)P(H)}$$

\textbf{Example.} Suppose the test has a 0.01 probability of both false positive and false negatives, and the overall prevalence of the disease in the population 0.02. You may be surprised that from an epidemiological perspective, a positive result is far from definitive:
$$ P(D | Pos)  = \frac{0.99 \times 0.02}{0.99 \times 0.02 + 0.01 \times 0.98} = 0.67 $$
This is because the disease is so rare, that even though the test is quite accurate, there are going to be a lot of false positives (about 1/3 of the time) since 98\% of the patients are healthy.

We can also calculate the probability of a patient who tests negative of actually being healthy, which is called the  \index{test!negative predictive value}\emph{negative predictive value}. In this example, it is far more definitive:
$$ P(H | Neg)  = \frac{P(Neg|H)P(H)}{P(Neg|H) P(H) + P(Neg | D)P(D)} = $$
$$ = \frac{0.99 \times 0.98}{0.99 \times 0.98 + 0.01 \times 0.02} =  0.9998$$
This is again because this disease is quite rare in this population, so a negative test result is almost guaranteed to be correct. In another population, where disease is more prevalent, this may not be the case.

Figure \ref{fig:ch7_testing_tree} illustrates all of the possibilities of a binary medical test: positive (P) or negative (N) for a patient who is either healthy (H) or diseased (D). The four outcomes correspond to the four outcomes of tests we saw in section \ref{sec:bio6}: true positives are D\&P, false positives are H\&P, true negatives are H\&N and false negatives are D\&N. This allows us to calculate the positive predictive value, which is the probability that a positive result is correct. For the patient on the left with disease prevalence of 0.1, $PPV = TP/(TP+FP) = 0.098/(0.098+0.045) \approx 0.685$. For the patient on the right with disease prevalence of 0.01, $PPV = TP/(TP+FP) = 0.0098/(0.0098+0.0495) \approx 0.165$. The exact same test has a higher PPV for a patient who has a higher prior probability of having a disease.

The negative predictive value can be calculated in a similar manner:  $NPV = TN/(TN+FN) = 0.855/(0.855+0.002) \approx 0.998$. For the patient on the right with disease prevalence of 0.01, $NPV = TN/(TN+FN)  = 0.9405/(0.9405+0.0002) \approx 0.9998$. The exact same test has a higher NPV for a patient who has a lower prior probability of having a disease.

\begin{figure}[ht] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=2.5in]{ch7/prob_tree_tikz1.png} 
   \includegraphics[width=2.5in]{ch7/prob_tree_tikz2.png} 
   \caption{Probabilities of the four possible outcomes for patients with different disease prevalence using the same medical test with sensitivity (rate of true positives) of 0.98 and specificity (rate of true negatives) of 0.95: above for disease prevalence of 10\%, and below for prevalence of 1\%.}
   \label{fig:ch7_testing_tree}
\end{figure}

\exercise{Using the law of total probability, calculate $P(Pos)$, the probability that a randomly chosen person from the population tests positive for the disease and $P(Neg)$, the probability that a randomly chosen person tests negative for the disease.}

\exercise{Using Bayes' formula, find the probability that a patient who tested positive has the disease $P(D | Pos)$ and the probability that a patient who tested negative is healthy $P(H | Neg)$.}

\exercise{If the disease prevalence were $P(D) =0.5$, repeat the calculations to find the new $P(D | Pos)$ and $P(H | Neg)$.}

\exerciseshere{In the problem set in section \ref{sec:math6_2} you used the TB and X-ray test data in table \ref{tab:TB_test} to calculate the sensitivity and specificity of the test. Suppose that you have the knowledge that the prevalence of TB in a population is $P(D) = 0.001$, use the previously calculated sensitivity and specificity to answer the following questions about testing a patient from the population (not those used in the study in the table.)}

\section{Applications of Bayesian thinking}
\label{sec:bio7}
The essence of the Bayesian approach to statistics is that everything comes with a prior probability, or odds, as Bayesians like to express it. There is usually some prior knowledge one has to assess the odds that a hypothesis is true before doing an experiment, which is called the prior. If you don't explicitly assume the prior, then you've assumed it implicitly. For example, the naive answer for the positive predictive value that ignores the prior prevalence for a test with 99\% specificity and sensitivity is that it is also 99\%. This assumes that the patient has equal prior probability of disease and health, as you can verify using Bayes' formula. Even in the absence of any data for a particular population, this is an unlikely assumption for most diseases. So the Bayesian advice is: assign odds to everything to the best of your knowledge so you don't get played for a sucker. An excellent summary of the misuses of p-value and the Bayesian approach to interpretation of medical data, see \citep{goodman_toward_1999}.

\subsection{when too much testing is bad}
For many decades, doctors recommended early  \index{medical!cancer screening}\emph{cancer screening} in a major public health effort to help reduce the mortality rate from cancer. This makes sense because the prognosis is generally much better for cancer when it is detected early, since the tumor is small and has not yet metastasized. This approach has taken hold in the public imagination in the US, and given us pink-ribbon campaigns aimed at breast cancer awareness and celebrities advising everyone to get tested early and often.

More recently, large-scale studies have shown that that preventative screenings do not necessarily improve the survival rate of cancer patients. One study from Canada \citep{miller_twenty_2014} assigned almost  90,000 women randomly into two equal-size groups, by a randomization process illustrated in figure \ref{fig:mammography_screening}. One of the groups received yearly mammography screenings for 5 years and one did not. The women were between the ages of 40 and 59, and then study then tracked the participants for 25 years to see whether there was a difference in cancer mortality rates between the two groups.

The results may be surprising:  the mortality rates were very similar, and in fact, slightly more women died of breast cancer in the mammography group than in the control group (180 vs 171). At the same time, more cases of invasive breast cancers were diagnosed in the mammography group, which may mean either that early treatment did not make a difference or that many of those cases were false positives.

This may seem counterintuitive: isn't more information, however imperfect, better? Viewed from a \index{Bayesian!interpretation}Bayesian perspective, the results should be less than surprising. Since the prior probability of developing breast cancer in any given year is small, the positive predictive value of the test is likely low, and most of the positive results end up being false positives. A false positive result for breast cancer has a major negative impact on a person's life: it means more invasive testing, a lot of worrying, and sometimes unnecessary treatment with serious side effects. This does not mean, of course, that cancer screening is never useful, and the author is no way trying to offer medical advice. For patient populations with a higher prior probability such screening tests may in fact provide a substantial benefit. But this once again underscores the importance of taking into account prior knowledge.

\begin{figure}[h!] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=4.5in]{ch7/mammography_screening.png} 
   \caption{The study design for the large randomized screening trial of effectiveness of mammography screening; figure from \citep{miller_twenty_2014} used by permission.}
   \label{fig:mammography_screening}
\end{figure}


\subsection{reliability of scientific studies}
In 2005 John Ioannidis published a paper entitled ``Why most published research findings are wrong'' \citep{ioannidis_why_2005}. The paper, as you can see by its title, was intended to be provocative, but it is based solidly on the classic formula of Bayes. \index{scientific research reproducibility}The motivation for the paper came from the observation that too often in modern science, big, splashy studies that were published could not be reproduced or verified by other researchers. What could be behind this epidemic of questionable scientific work?\index{Bayesian!interpretation} 

\begin{figure}[h!] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=2.7in]{ch7/ioannidis_bayes.png} 
   \caption{The dependence of post-study probability (positive predictive value) on the pre-study odds, for different power of the study, and with different levels of bias; figure from \citep{ioannidis_why_2005} under CC-BY.}
   \label{fig:ioannidis_bayes}
\end{figure}

The problem as described by Ioannidis and many others, in a nutshell, is that unthinking use of traditional hypothesis testing leads to a high probability of false positive results being published. The paper outlines several ways in which this can occur.

First, there is the problem of prior knowledge. Too often, a hypothesis is tested and if the resultant p-value is less than some arbitrary threshold (very often 0.05, an absurdly high number), then the results are published. However, if one is testing a hypothesis with low prior probability, a positive hypothesis test result is very likely a false positive. Very often, modern biomedical research involves digging through a large amount of information, like an entire human genome, in search for associations between different genes and a phenotype, like a disease. It is \emph{a priori} unlikely that any specific gene is linked to a given phenotype, because most genes have very specific functions, and are expressed quite selectively, only at specific times or in specific types of cells. However, publishing such studies results in splashy headlines (''Scientists find a gene linked to autism!") and so a lot of false positive results are reported, only to be refuted later, in much less publicized studies.

The second problem compounds the first one: multiple research groups studying the same phenomenon. \index{test!p-value}This should be a good thing, but it can lead to a higher volume of false positive results. Suppose that 20 groups are all testing the same hypothesis, and are using the same p-value cutoff of 0.05 to decide whether their results is ``significant''. Even if their null hypothesis is true, and there is no effect, 1 out 20 groups is likely to obtain a p-value less than 0.05, simply by random variation. What do you think that group will do? Yes, they should compare its results with the other groups, or try to repeat the experiment multiple times. But repeating experiments is costly and boring, and telling your competitors about your results can lead to your getting scooped. Better publish fast!

The third problem is even more insidious: bias in the experimental work, either conscious or non. Some of it may be due to experimental design, like biased sampling, or defective instrumentation - no experiment is perfect. One big violation of good experimental design is known as p-value fishing: repeating the experiment, or increasing the sample size, until the p-value is below the desired threshold, and then stopping the experiment. Using such defective design dramatically lowers the likelihood that the result is a true positive. And of course there is actual fraud, or fudging of data, which contributes to some bogus results.

Ioannidis performed basic calculations of the probability that a published study is true (that is, that a positive reported result is a true positive), and how it is affected by pre-study (prior) probability, number of conducted studies on the same hypothesis, and the level of bias. His prediction is that for fairly typical scenario (e.g. pre-study probability of 10\%, ten groups working simultaneously, and a reasonable amount of bias) the probability that a published result is correct is less than 50\%. This effect is shown in figure \ref{fig:ioannidis_bayes}, taken from his paper. He then followed up with another paper \citep{ioannidis_ja_contradicted_2005} that investigated 49 top-cited medical research publications over a decade, and looked at whether follow-up studies could replicate the results, and found that a very significant fraction of their findings could not be replicated by subsequent investigations.

This might leave you with a rather bleak view of scientific research. Indeed, many in the community have been sounding the alarm about the lack of replicability of published results, and have proposed some basic remedies. Perhaps the most important is the issue that only positive results are deemed worthy of publication. If all of the 20 groups in the scenario above published their results, 19 would report no effect, and 1 would  report an effect, and the picture would be clear. There are some journals (e.g. PLOS One) which accept any methodologically sound submission, regardless of whether the result is positive or negative. Another remedy is to provide funding for research labs to repeat other groups' studies to test them. These steps are being implemented, and hopefully will eventually lead to an improvement in the reliability of published data. Even more importantly, educating scientists about basic probability ideas, such as Bayes' formula and the notion of prior knowledge, should improve the quality of inference and decrease the amount of questionable science.

\discussion{What are some of the examples of studies with low prior odds that Ioannidis uses as examples?}

\discussion{Due to human imperfection, there will always be some bias in conducting and reporting scientific results. What do you expect to be the typical level of bias (u in the paper)?}

\discussion{What does Ioannidis propose to remedy the problem of lack of reproducibility of studies? Will you take them into account when reading scientific publications or doing your own research?}

\discussionshere{The following questions refer to the paper ``Why most published research findings are wrong'' \citep{ioannidis_why_2005}.}


\section{Conditional statements and random simulations}
\sectionmark{Random simulations}
\label{sec:comp7}
Here we introduce \index{R programming!conditional statement}\emph{conditional statements}, which utilize \texttt{if} and \texttt{else} commands. The \texttt{if} command must be followed by a \index{R programming!logical test}\emph{logical test}: for instance \texttt{$a>b$} or \texttt{$ a == b$}; in the first case, the commands following the if statement (in brackets) are executed if the variable a is greater than b, and in the second case, they will be executed if a equals b. If the logical test is false, then the commands following the  \texttt{else} statement (in brackets) are executed. 

We can use a random number from the uniform random number generator to ``draw'' a patient from a population with a certain prevalence of the disease. For a given prevalence set by \texttt{prob} (e.g. 0.2),  if the random number is less than \texttt{prob}, the patient will be counted as diseased, and otherwise as healthy. This is implemented in the following script:
<<comp9-1, tidy=TRUE,tidy.opts=list(width.cutoff=40)>>=
  prob <- 0.2 # set probability of disease
  decider <- runif(1) # random number between 0 and 1
  Disease <- 0
  Healthy <- 0
  if (decider < prob) {
  	Disease <- Disease + 1 # increment number of diseased
  } else { 
  	Healthy <- Healthy + 1 # increment number of healthy
  }
@
This script only produces one patient, so at the end either Disease is 1, or Healthy is 1, while the other one is zero. In order to generate a ``sample'' of patients, we can use a for loop to repeat this random decision multiple times. The following script repeats the process a specified number of times:
<<comp9-2,tidy=TRUE,tidy.opts=list(width.cutoff=40)>>=
prob <- 0.2 # set probability of disease
Disease <- 0
Healthy <- 0
numpatients <- 100
for (i in 1:numpatients) {
  decider <- runif(1) # random number between 0 and 1
  if (decider < prob) { 
  	Disease <- Disease + 1 # increment number of diseased
  } else { 
  	Healthy <- Healthy + 1 # increment number of healthy
  }
}
print(Disease)
print(Healthy)	
@
This script produces a simulated sample of 100 patients out of a population with 20\% disease prevalence, and prints out how many are diseased and how many are healthy.

You can use nested conditional statements to simulate a number of hypothesis tests with a given sensitivity and specificity. The  hypothesis has a prior probability of being true, which we will simulate by drawing a random number and assigning our individual experiment a value (e.g. 0 or 1). The following script uses two conditional statements to make two random decisions: is the hypothesis actually true or false (e.g. is the patient sick or healthy) and what the outcome of the test is (reject or not reject the hypothesis) based on the sensitivity and specificity. Since the second decision depends on the first, you will need to nest the conditional statements for the test inside the conditional statements for the hypothesis. The following script simulates testing 100 randomly selected patients from a population with 20\% disease prevalence, and testing them with a test with 0.9 sensitivity (90\% of truly sick people test positive) and 0.99 specificity (99\% of truly healthy people test negative):
<<comp9-3, tidy=TRUE,tidy.opts=list(width.cutoff=50)>>=
  prob <- 0.2 # set probability of disease
  TP <- 0 # true positive number
  FP <- 0 # false positive number
  TN <- 0 # true negative number
  FN <- 0 # false negative number
  sens <- 0.9 # sensitivity of the test
  spec <- 0.99 # specificity of the test
  numpatients <- 100
  for (i in 1:numpatients) {
         decider <- runif(1) # random number between 0 and 1
         decider2 <- runif(1) #generate second random number
	 if (decider < prob) { # the patient is sick
  		if (decider2 < sens) {
			TP <- TP + 1 # increment the true positive
		} else {
			FN <- FN + 1 # increment the false negative
		}
  	} else { # patient is healthy
  		 if (decider2 < spec) {
			TN <- TN + 1 # increment the true negative
		} else {
			FP <- FP + 1 # increment the false positive
		}
	}
  }
  print(paste('true positive',TP))
  print(paste('true negative',TN))
  print(paste('false positive',FP))
  print(paste('false negative',FN))
@

Look carefully at the logical structure of the script, because it is easy to make a mistake. If the patient is sick, there are two possible test outcomes: a true positive or a false negative; if a patient is healthy, the two possibilities are a true negative or a false positive.

\debug{The first script is supposed to draw a random number between 0 and 1, and if it is less than 0.5, print "Heads!", otherwise print "Tails!"}

\debug{The second script is supposed to simulate a sample of 500 patients from a population with disease prevalence of 10\%, and saving their disease status ('sick' or 'healthy') in a vector.}

\debug{The third script is supposed to take the vector of disease status (assigned in the previous script) and run a hypothesis test on each one, with a false positive rate of 0.01 and false negative rate of 0.1, and record the results in a vector.}

\debugshere{Find the errors in the scripts presented below by copying each script into an R editor window, and debugging it until it runs and does what it is intended to do.}

<<debug7-1, eval=FALSE>>=
  if (decider < 0.5) {
	decider <- runif(1)
	print('Heads!')
 } else {}
 	decider <- runif(1)
	print['Tails!']
@

<<debug7-2, eval=FALSE>>=
numpatients <- 500 # set number of patients
prev <- 0.1 # set disease prevalence
for { (j in 1:numpatients)
  decider <- runif(1)
  if (decider < prev) { # a sick patient
    Status_Vector[i] <- 'sick'
  } 
}
@

<<debug7-3, eval=FALSE>>=
FPR < - 0.01 # set the false positive rate
FNR <- 0.1 # set the false negative rate
# pre-allocate vector of test results 
test.results<-rep('blah',numpatients) 
decider <- runif(1)
for (i in 1:numpatients) {
  if (Status_Vector=='sick') {
      if (decider < FNR) { #  false negative result
           test.results <- 'negative'
     }  else { #  true positive result
         test.results <- 'positive'
     }
   }    
   if (Status_Vector=='healthy') {
      if (decider < FPR) { #  false positive result
           test.results <- 'positive'
     }  else { # true negative result
         test.results <- 'negative'
     }
   } 
}
@


\section{Computational project}
\label{sec:proj7}
In this project we illustrate the ideas of the Ioannidis paper described above.  The basic idea is that if a hypothesis has a small prior probability of being true (e.g. looking through an entire genome for SNPs that are linked with a disease) then a positive result has a low predictive value. We will simulate this by controlling the prior probability of the hypothesis being true and the sensitivity and specificity of the test. The following script uses a random number to decide whether a particular SNP is linked to some disease, based on a prior probability.
<<proj7-1, tidy=TRUE ,tidy.opts=list(width.cutoff=50),eval=FALSE>>=
  prior.prob <- 0.2 # prob of SNP linked to disease
  decider <- runif(1) # random number between 0 and 1
  if (decider < prior.prob) { # if random number < prob
    link <- 1 # SNP is linked to disease
  } else { 
  	link <- 0 # SNP is independent of disease
  }
@
To simulate the whole experiment, we need to make two separate random decisions:
\begin{enumerate}
\item is the hypothesis actually true or false (e.g. is the randomly selected SNP linked to the disease)
\item is the test outcome positive (reject the hypothesis of no linkage) or negative (do not reject the hypothesis) 
\end{enumerate}

Since the second decision depends on the first, you need to \textbf{nest} the conditional statements by putting the one determining the test outcome inside the conditional statements deciding whether the hypothesis is true. This is shown in the following script, which simulates running a hypothesis test for linkage of SNP and disease with given sensitivity and specificity:
<<proj7-2, tidy=TRUE, tidy.opts=list(width.cutoff=50), eval=FALSE>>=
  spec <- 0.7 # set specificity
  sens <- 0.8 # set sensitivity
  TP <- 0
  FP <- 0
  TN <- 0
  FN <- 0
  decider <- runif(1) # random number between 0 and 1
  if (link==1) { 
    if (decider < sens) { # test correctly identifies the linkage
      TP <- TP+1 # true positive result
    } else {
      FN <- FN+1 # false negative result
    }  
  } else { 
    if (decider < spec) { # test correctly says there is no linkage
      TN <- TN+1 # true negative result
    } else { 
      FP <- FP+1 # false positive result
    }
  }
  print(paste("The number of true positives is",TP))
  print(paste("The number of true negatives is",TN))
  print(paste("The number of false positives is",FP))
  print(paste("The number of false negatives is",FN))
@


\subsection*{Tasks}
\begin{enumerate}
\item Use the second of the sample scripts provided above to simulate a test for a SNP which \textbf{is linked} to a disease (set link to 1) with specificity of 0.8 and sensitivity of 0.9. Put a for loop around the script to run it 100 times and count how many of the test results are true positives, false positives, true negatives, and false negatives are generated (hint: only two of those are possible for a true hypothesis). How many times does the hypothesis test make the correct decision? 

\item  Run the same script for a \textbf{false} hypothesis (set link to 0), with the same specificity and sensitivity of the test and count how many of the 100 test results are true positives, false positives, true negatives, and false negatives are generated (hint: only two of those are possible for a false hypothesis). How many times does the hypothesis test make the correct decision?

\item Use the first of the provided scripts to randomly simulate whether a particular SNP is linked with the prior probability 0.01 that the SNP is linked to the disease, and then follow with the second script to simulate the hypothesis test. Use the same sensitivity and specificity values as above and run the loop for 1000 trials. Based on your counts of the different test outcomes, report the positive predictive value of the test (the probability that the SNP is linked to disease, given that the test result is positive, or the fraction of true positives out of all positives).

\item Let us investigate the effect of changing the prior probability of the hypothesis being true. Change the prior probability to 0.1, use the same sensitivity and specificity values as before, and run the loop for 1000 trials and report the positive predictive value of the test. Now change the prior probability to 0.001, use the same sensitivity and specificity values as before, and run the loop for 1000 trials and report the positive predictive value of the test. How does the prior probability affect the positive predictive value? What implication does this have for testing a large number of hypotheses with low prior probabilities, such as thousands of SNPs in the human genome?

\item Let us investigate the effect of changing the sensitivity and specificity of the test. Set the prior probability to 0.1, set the same sensitivity to be 0.9 and the specificity to be 0.9, and run the loop for 1000 trials and report the positive predictive value of the test. Now change the sensitivity to 0.99 and specificity to 0.9, run the loop for 1000 trials and report the positive predictive value of the test.  Finally, set the sensitivity to be 0.8 and the specificity to be 0.99, and again and report the positive predictive value of the test. Which parameter (sensitivity or specificity) has the largest effect of the positive predictive value?
\end{enumerate}