\chapter{Stationary distributions of Markov chains}
\chaptermark{stationary distributions}

\begin{chapquote} {Samuel Beckett, \emph{Waiting for Godot}}
The tears of the world are a constant quantity. For each one who begins to weep somewhere else another stops. The same is true of the laugh.
\end{chapquote} 

In the last chapter we learned to compute the distributions of Markov models, bringing a measure of predictability to the randomness. Using repeated matrix multiplication we could compute the distribution for any given time, and observe how probability vectors evolve. You may have noticed that in the examples in the computational projects the probability vectors tended to approach some particular distribution and then essentially remain the same. It turns out that Markov chains have special \emph{stationary distributions} at which the transitions are perfectly balanced so the probabilities of each state remain the same. In this chapter we will study the stationary distributions of Markov chains and learn to do the following:
\begin{enumerate}

\item calculate the stationary distribution of a small Markov models on paper

\item how to tell whether a Markov chain converges to a stationary distribution 

\item run multiple simulations in R and observe convergence to a stationary distribution

\item the concepts of Hidden Markov Models

\end{enumerate}

\section{The origins of Markov chains: a feud and a poem}
\sectionmark{Origin of Markov chains}
\label{sec:model12}
The idea of chains of random variables that depend on each other was born of a feud. In the late 19th century probability theory had made great strides, both in theory and in ground-breaking applications to physics, like the work of Boltzmann in thermodynamics. Randomness and its mysteries become a fashionable topic of conversation outside of the confines of mathematics classrooms and conferences. Sociological studies were published that claimed to show that the behavior of a large number of people was predictable due to the law of large numbers.  The mathematician and self-styled philosopher P.A. Nekrasov published a paper in 1902 that made an  audacious leap of logic: he claimed that since human beings were subject to the law of large numbers, and the law of large numbers requires independence between constituent random variables, humans must have been endowed with free will, in agreement with his devout Russian Orthodox beliefs. The argument is questionable both mathematically and theologically, and it especially grated on another mathematician, \index{Markov!history}A.A. Markov \citep{hayes_first_2012}.

Markov was a great mathematician as well as a malcontent. In contrast with Nekrasov, he was neither a monarchist nor a devout Orthodox believer, and even asked to be excommunicated from the church after it expelled the great writer Tolstoy for heresy. Markov already disdained Nekrasov both personally and professionally, and the paper inspired him to action. After several years of work, he published a paper entitled ``An extension of the law of large numbers to quantities dependent on each other'' \citep{markov_extension_1906}, which founded the concept of Markov chains. As the title states, it provided a counterexample to Nekrasov's claim that the predictability of the behavior of large number of random variables implied their independence. Markov showed that \textbf{variables that depend on each other can also behave in a predictable manner in large numbers.}

\begin{figure}[h!] %  figure placement: here, top, bottom, or page
 \centering
\includegraphics[width=4in]{ch12/onegin_markov_diag.png} 
\caption{Two Markov models based on the text of \emph{Eugene Onegin}, with states denoting consonants (C) and vowels (V); model based on the Russian text is on the left and the one based on English on the right \citep{hayes_first_2012}; the first 4 lines of the poem in the original \citep{onegin-pushkin} and in English translation \citep{onegin-falen} are below the respective diagrams.}
   \label{fig:onegin_markov}
\end{figure}

In addition to inventing the mathematical concepts, Markov was the first to use his chains of random variables to make a Markov model. In his 1913 paper \citep{markov_example_1913}, he proposed a model based on the classic Russian poem \emph{Eugene Onegin} by \index{Pushkin, A.S.}A.S. Pushkin. To make the task manageable, he divided the letters into two categories: consonants and vowels, discarding spaces, punctuations, and two Russian letters which make no sound. To calculate the transition probabilities between the two states, Markov took the first 20,000 letters of the poem and counted by hand the fraction of vowels that were followed by vowels, and the fraction of consonants that were followed by consonants, and built the first two-state text-based model, foreshadowing models of bioinformatics used now to analyze genome structure.

Figure \ref{fig:onegin_markov} shows two models based on 20,000 letters of \emph{Eugene Onegin} in Russian and in English translation. The resulting transition probabilities are different than those computed by Markov in his paper: whereas in the English text the probability of a vowel following another vowel is 0.175, in the original Russian it is 0.128; in English the probability of a consonant following another consonant is 0.474, while in Russian it is 0.663. Clearly, Russian words contain more consonant clusters and fewer vowels next to each other. In both cases, the state of the previous letter affects the probability of the next letter being a vowel. Remarkably, the distribution of consonants and vowels is predictable in any sufficiently long piece of text: in English it is about 39\% vowels and 61\% consonants, and in Russian it is about 28\% vowels and 72\% consonants. This is an example of the main result of the first Markov chain paper: large numbers of interconnected random variables converge to a predictable distribution, called the \emph{stationary distribution}.

\section{Stationary distributions}
\label{sec:math12}
\subsection{definition of stationary distribution}
What happens when we extend our calculation of the probability distribution vectors of a Markov chain over a long time? Let us consider the cell cycle model with states Q and R. We have seen the state sequences of a single cell over time, so let us consider what happens to a population of cells. The basic question is: given that all the cells start out in a particular state (e.g. R), what fraction of cells is in state R after a certain number of time steps? Figure \ref{fig:math12_1} shows the result of propagating the QR model for 30 time steps, starting with two different initial distributions. You can try this at home yourself, starting with different initial distributions, and see that all of them converge over time to the same fraction of Q and R. This is called the \emph{stationary distribution} of the Markov chain.

\begin{figure}[h!] 
<<math12_1, fig.width=4.5, fig.height=4.5, out.width='0.49\\linewidth', fig.show='hold',echo=FALSE, >>=
M <- matrix (c(0.95,0.05,0.1,0.9),nrow=2)
Nstep <- 30
P<- matrix (0,nrow=2,ncol=Nstep+1)
P[ ,1] <- c(0,1)
for (i in 1:Nstep) {
	P[,i+1] <- M%*%P[,i]
	}
barplot(P,xlab='time',names.arg=0:Nstep,ylab='probability',main='QR model with initial state Q',cex=1.5, cex.axis=1.5,cex.lab=1.5)
P<- matrix (0,nrow=2,ncol=Nstep+1)
P[ ,1] <- c(1,0)
for (i in 1:Nstep) {
	P[,i+1] <- M%*%P[,i]
	}
barplot(P,xlab='time',names.arg=0:Nstep,ylab='probability',main='QR model with initial state R',cex=1.5, cex.axis=1.5,cex.lab=1.5)
@
   \caption{Probability distributions converge to the same distribution starting from two different initial distributions: a) $P(0) =(0,1)$ ; b) $P(0) =(1,0)$.}
   \label{fig:math12_1}
\end{figure} 

\begin{mosdef}
For a finite-state Markov model with transition matrix $M$, a \index{Markov!stationary distribution}\emph{stationary or equilibrium distribution} is a vector $\vec P_s$ that has all nonnegative elements which add up to 1, and satisfies
$$ \vec P_s = M \times  \vec P_s$$
\end{mosdef}
The definition says that a probability vector \textbf{which is unchanged by multiplication by the transition matrix} will remain stationary over time. \citep{feller_introduction_????}

\textbf{Example}. The stationary distribution vector can be calculated analytically from the definition. Let us find the stationary vector $\vec P_s$ for the QR cell model with components $P_Q$ and $P_R$ (the fractions of quiescent and replicating cells in the stationary distribution):
$$  \left(\begin{array}{c} P_Q  \\ P_R \end{array} \right) =\left(\begin{array}{cc}0.95 & 0.1 \\0.05 & 0.9\end{array}\right)  \left(\begin{array}{c} P_Q  \\ P_R \end{array}\right) =  \left(\begin{array}{c}0.95P_Q + 0.1P_R \\0.05P_Q + 0.9P_R \end{array}\right) $$
This means there are two equations to solve for two variables. It turns out that they are equivalent:
$$ 0.95P_Q + 0.1P_R = P_Q \Rightarrow 0.1P_R = 0.05P_Q \Rightarrow P_R = 0.5P_Q$$
$$ 0.05P_Q + 0.9P_R  = P_R \Rightarrow 0.05P_Q  = 0.1P_R  \Rightarrow  0.5P_Q =  P_R  $$
Both equations say that in the stationary distribution there are twice as many quiescent cells as replicating. If we add the condition that $P_Q+P_R = 1$, then we can have the exact solution:
$$\vec P_s =  \left(\begin{array}{c} P_Q  \\ P_R \end{array} \right)  =  \left(\begin{array}{c} \frac{2}{3}  \\ \frac{1}{3} \end{array} \right) $$
This says that in a large population of cells in the cell cycle model, a population with 2/3 quiescent and 1/3 replicating is stationary. This does not mean that each individual cell remains in the same state! Each cell still randomly transitions between the two states, but the number of cells switching to the quiescent state is balanced by the number of cell switching out of the state, so the net distribution remains the same. We will observe this using simulations with multiple individual cells in section \ref{sec:comp12}.

\exercise{Use the model in the transition diagram in figure \ref{fig:ch10_trans_diags}a.}

\exercise{Use the model in the transition diagram in figure \ref{fig:ch10_trans_diags}b.}

\exercise{Use the model in the transition diagram in figure \ref{fig:ch10_trans_diags}c.}

\exercise{Use the model in the transition diagram in figure \ref{fig:ch10_trans_diags}d.}

\exercise{An ion channel can be in either open or closed state. If it is open, then it has probability  0.1 of closing in 1 microsecond; if closed, it has probability 0.3 of opening in 1 microsecond.}

\exercise{An individual can be either susceptible or infected, the probability of infection for a susceptible person is 0.05 per day, and the probability an infected person becoming susceptible is 0.12 per day.}

\exercise{The genotype of an organism can be either normal (wild type) or mutant. Each generation, a wild type individual has probability 0.03 of having a mutant offspring, and a mutant has probability 0.005 of having a wild type offspring.}

\exercise{A gene is is either expressed (On) or not expressed (Off) by a stochastic mechanism. In the On state, it has probability 0.3 per minute of turning off, and in the Off state, it has probability 0.02 per minute of turning on.}

\exercise{The nAChR ion channel can be in one of three states: resting (R), closed with Ach bound (C), and open (O) with transition probabilities (per one microsecond): 0.04 (from R to C), 0.07 (from C to R), 0.12 (from C to O) and 0.02 (from O to C); the other transition probabilities are 0.}

\exercise{There are three kinds of vegetation in an ecosystem: grass, shrubs, and trees. Every year, 25\% of grassland  plots are converted to shrubs, 20\% of shrub plots are converted to trees, 8\% of trees are converted to shrubs, and 1\% of trees are converted to grass; the other transition probabilities are 0.}

\exerciseshere{For the following Markov models: a) write down the transition matrix $M$; b) find the stationary probability distribution on paper; c) use matrix multiplication in R to check that it satisfies the definition of stationary distribution.}

\subsection{condition for a unique stationary distribution}
We now know how to find stationary distributions from the definition, and we saw in the computer calculations how probability vectors converge to them over time.  However, just because there is a stationary distribution, does not mean that the model will converge to it. Here is a very simple example: a two-state model where transitions between the two states both have probability 1. Here is its transition matrix:
$$ M = \left(\begin{array}{cc}0  & 1 \\ 1& 0\end{array}\right)$$
It is clear that the vector $P_s = (0.5, 0.5)$ is a stationary distribution. However, if we start with an initial distribution that is different $P_s$, the distribution does not converge to any stationary vector. For instance, if the initial distribution is all in state 1, the distribution vector will keep flipping between $(1,0)$ and $(0,1)$ forever.

All Markov models have at least one stationary distribution, as we will see in the next chapter, but there could be more than one. Here is another simple example with two states where the transition probabilities are exactly zero:
$$ M = \left(\begin{array}{cc} 1  & 0 \\  0 & 1 \end{array}\right)$$
You can check for yourself that both vectors $(1,0)$ and $(0,1)$ satisfy the definition of stationary distribution. In fact, any vector is stationary, since nothing can leave either state, so there are infinitely many stationary distributions of this very boring model.

So can we avoid the weirdness of either cycling around endlessly, like in the first example, or having multiple stationary distributions? We first need to define a new concept of \emph{communication} between states in order to express the condition for a unique stationary distribution.
\begin{mosdef}
Two states $i$ and $j$ \index{Markov!communicating states}\emph{communicate} with each other if there is nonzero probability of transition from $i$ to $j$ after some number of time steps, as well as for probability of transition from $j$ to $i$.
\end{mosdef}
Determining whether two states communicate with each other can be done by checking the flow diagram of the model. As long as there is a path from state $i$ to state $j$ using nonzero probability transitions, and vice versa, the two states communicate. States that all communicate with each other form a \emph{class}, which is a mathematical term that I think is intuitive enough not to bother with formalities. 
\begin{mosdef}
A Markov chain is \emph{irreducible} if all its states belong to one class, that is, if all states communicate.
\end{mosdef}
Markov states can be divided into two types:
\begin{mosdef}
A state $i$ in a Markov chain is called \index{Markov!recurrent state}\emph{recurrent} if the random variable has probability 1 of returning to state $i$ at some future time. Otherwise it is called \index{Markov!transient state}\emph{transient}.
\end{mosdef}
\textbf{Example:} A Markov chain with two states where $X$ transitions from state 1 to 2 with probability 1, and stays in state 2 with probability 1, has the following transition matrix:
$$ M = \left(\begin{array}{cc} 0 & 0 \\ 1 & 1 \end{array}\right)$$
In this model, state 1 is transient, because there is no probability of returning to it, while state 2 is recurrent - in fact, it is a special kind of recurrent state called \index{Markov!absorbing state}  \emph{absorbing}. 

Finally, there is one more notion necessary to describe Markov states: periodicity. 
\begin{mosdef}
A state $i$ in a Markov chain is \emph{periodic with period n} if after some time $k$, the probability of return to the state is nonzero only for integer multiples of $n$. A state with periodicity 1 is called \index{Markov!aperiodic state} \emph{aperiodic}.
\end{mosdef}
\textbf{Example:} The same Markov chain introduced above is an example of periodicity, because here the probability of return to either state is nonzero only for multiples of 2:
$$ M = \left(\begin{array}{cc}0  & 1 \\ 1& 0\end{array}\right)$$
Notice that both of these states are recurrent as well as periodic, but they are not absorbing.

Finally, we have the tools to state the conditions that guarantee that  a Markov chain has only one stationary distribution and that all initial distributions converge to it. \citep{feller_introduction_????}
\begin{theorem}
A Markov chain in which all states communicate with each other (irreducible) and all the states are recurrent and aperiodic has a unique stationary distribution and all initial distributions converge to it.
\end{theorem}
The three different Markov chains in examples above do not satisfy the requirements of this theorem. The first one:
$$ M = \left(\begin{array}{cc}0  & 1 \\ 1& 0\end{array}\right)$$
has periodic states, whereas the theorem requires aperiodicity. The second one:
$$ M = \left(\begin{array}{cc} 1  & 0 \\  0 & 1 \end{array}\right)$$
has aperiodic states, but they do not communicate (the Markov chain is not irreducible). The third one:
$$ M = \left(\begin{array}{cc} 0  & 0 \\  1 & 1 \end{array}\right)$$
has two problems: the states don't communicate, and state 1 is not recurrent. 

However, the QR model that we have been using for the past three chapters, with transition matrix:
$$  M = \left(\begin{array}{cc}0.95 & 0.1 \\0.05 & 0.9\end{array}\right) $$
fits the bill, as it is irreducible, recurrent, and aperiodic, and as promised, it has a unique stationary distribution.

\begin{figure}[h!] %  figure placement: here, top, bottom, or page
   \centering
 \includegraphics[width=1.8in]{ch12/3state_model1.png} \\
 \includegraphics[width=1.8in]{ch12/3state_model2.png} \\
 \includegraphics[width=1.8in]{ch12/3state_model3.png} \\
 \includegraphics[width=1.8in]{ch12/3state_model4.png} 
  \caption{Transition diagrams for three-state Markov models with arrows indicating transitions with nonzero probabilities.}
   \label{fig:ch12_3state_examples}
\end{figure}


\exercise{Use the transition diagram in figure \ref{fig:ch12_3state_examples}a.}

\exercise{Use the transition diagram in figure \ref{fig:ch12_3state_examples}b.}

\exercise{Use the transition diagram in figure \ref{fig:ch12_3state_examples}c.}

\exercise{Use the transition diagram in figure \ref{fig:ch12_3state_examples}d.}

\exerciseshere{Use transition diagrams shown in figure \ref{fig:ch12_3state_examples}; arrows indicate possible transitions, all other transitions have probability zero. For each model do the following: a) classify each state as transient or recurrent; b) decide whether the states are periodic or aperiodic; c) conclude whether the Markov chain is ergodic and irreducible; d) does it have a unique stationary distribution?}


\section{Multiple random simulations in R}
\label{sec:comp12}
In this section we will perform simulations for multiple manifestations of a Markov model and observe how the proportions of different states evolve over time. We will simulate the QR cell cycle model from chapter 9, take the simulation code for simulating a single cell, and repeat it multiple times. We assume that different cells do not influence each other, so each simulation can be repeated without reference to the others.The best way to repeat the same operations multiple times is to use a for loop, so in the script below I put another for loop around the script for simulating a single cell, with the new indexing variable going through different cells. Within the outer loop there is the internal loop with the indexing variable going through time steps. The generated states (1 for Q and 2 for R) are stored in a matrix with the number of columns given by the number of cells, and the number of rows is the number of time steps plus one (to leave room for the initial state). 

<<comp12_1, tidy=TRUE>>=
prob1 <- 0.05 # transition probability from 1 to 2
prob2 <- 0.1 # transition probability from 2 to 1
nsteps <- 40 # number of time steps
ncells <- 30 # number of cells
states <- matrix(1,nrow=ncells,ncol=nsteps+1)# initialize states matrix 
for (j in 1:ncells) { # go through all the cells
      for (i in 1:nsteps) { # go through time steps
        decider <- runif(1) # generate a random number
        if (states[j,i]==1) { 
          if (decider < prob1) { # transition from 1 to 2
            states[j,i+1] <- 2 # new state is 2
          } else { 
            states[j,i+1] <- 1 # new state is 1
          }
        } else {
          if (decider < prob2) { #  transition from 2 to 1
            states[j,i+1] <- 1 # new state is 1
          } else { 
            states[j,i+1] <- 2 # new state is 2
          }
        }
      } 
    }
@

The number of cells in state 1 and 2 at each time step are tabulated and then plotted using \texttt{barplot()} in a separate script that isn't shown, with the results in figure \ref{fig:comp12_1} for two simulations with a total of 30 individual cells: starting with all cells in state 1 and starting with all states in state 2. Although in both plots the numbers of cell in Q tend to approach approximately 20 out of 30, the numbers bounce around a lot due to the inherent randomness of the simulation. One can make the frequencies less volatile by increasing the cell population.  Figure \ref{fig:comp12_2} shows the results of simulations with 200 cells, which show the numbers of cells in state 1 approach 2/3 of 200 (around 134), with considerably less variance. Finally, the counts for simulations with 1000 cells are shown in figure \ref{fig:comp12_3}, and the counts in that figure are much closer to what may be called convergence to the stationary distribution, which you can see by comparing them with the plots of the probability distribution vectors in figure \ref{fig:math12_1} in section \ref{sec:math12}. This illustrates what Markov called the law of large numbers for a chain of variables that depend on each other.

\begin{figure}[h!] 
<<comp12_2, fig.width=5, fig.height=5, out.width='0.49\\linewidth', fig.show='hold',echo=FALSE>>=
    state.count <- matrix(0,nrow=2,ncol=nsteps+1)
    for (k in 1:(nsteps+1)) {
	state.count[,k] <- tabulate(states[,k],nbins=2)
	}
barplot(state.count,main='frequency of states vs. time',xlab='time', names.arg=1:(nsteps+1),ylab='quiescent or replicating',cex=1.5, cex.axis=1.5,cex.lab=1.5)
states <- matrix(2,nrow=ncells,ncol=nsteps+1)# initialize states matrix 
for (j in 1:ncells) { # go through all the cells
      for (i in 1:nsteps) { # go through time steps
        decider <- runif(1) # generate a random number
        if (states[j,i]==1) { 
          if (decider < prob1) { # transition from 1 to 2
            states[j,i+1] <- 2 # new state is 2
          } else { 
            states[j,i+1] <- 1 # new state is still 1
          }
        } else {
          if (decider < prob2) { #  transition from 2 to 1
            states[j,i+1] <- 1 # new state is 1
          } else { 
            states[j,i+1] <- 2 # new state is still 2
          }
        }
      } 
    }
state.count <- matrix(0,nrow=2,ncol=nsteps+1)
for (k in 1:(nsteps+1)) {
	state.count[,k] <- tabulate(states[,k],nbins=2)
}
barplot(state.count,main='frequency of states vs. time',xlab='time', names.arg=1:(nsteps+1),ylab='quiescent or replicating',cex=1.5, cex.axis=1.5,cex.lab=1.5)
@
\caption{Counts of simulated cells in states Q and R out of a total count of 30 converge over time to the same distribution starting from: a) all in state Q ; b) all in state R.}
\label{fig:comp12_1}
\end{figure} 

\begin{figure}[h!] 
<<comp12_3, fig.width=5, fig.height=5, out.width='0.49\\linewidth', fig.show='hold', echo=FALSE, >>=
ncells <- 200 # set number of cells
states <- matrix(1,nrow=ncells,ncol=nsteps+1)# initialize states matrix 
for (j in 1:ncells) { # go through all the cells
      for (i in 1:nsteps) { # go through time steps
        decider <- runif(1) # generate a random number
        if (states[j,i]==1) { 
          if (decider < prob1) { # transition from 1 to 2
            states[j,i+1] <- 2 # new state is 2
          } else { 
            states[j,i+1] <- 1 # new state is still 1
          }
        } else {
          if (decider < prob2) { #  transition from 2 to 1
            states[j,i+1] <- 1 # new state is 1
          } else { 
            states[j,i+1] <- 2 # new state is still 2
          }
        }
      } 
    }
state.count <- matrix(0,nrow=2,ncol=nsteps+1)
 for (k in 1:(nsteps+1)) {
	state.count[,k] <- tabulate(states[,k],nbins=2)
}
barplot(state.count,main='frequency of states vs. time',xlab='time', names.arg=1:(nsteps+1),ylab='quiescent or replicating',cex=1.5, cex.axis=1.5,cex.lab=1.5)
states <- matrix(2,nrow=ncells,ncol=nsteps+1)# initialize states matrix 
for (j in 1:ncells) { # go through all the cells
      for (i in 1:nsteps) { # go through time steps
        decider <- runif(1) # generate a random number
        if (states[j,i]==1) { 
          if (decider < prob1) { # transition from 1 to 2
            states[j,i+1] <- 2 # new state is 2
          } else { 
            states[j,i+1] <- 1 # new state is still 1
          }
        } else {
          if (decider < prob2) { #  transition from 2 to 1
            states[j,i+1] <- 1 # new state is 1
          } else { 
            states[j,i+1] <- 2 # new state is still 2
          }
        }
      } 
    }
state.count <- matrix(0,nrow=2,ncol=nsteps+1)
for (k in 1:(nsteps+1)) {
	state.count[,k] <- tabulate(states[,k],nbins=2)
}
barplot(state.count,main='frequency of states vs. time',xlab='time', names.arg=1:(nsteps+1),ylab='quiescent or replicating',cex=1.5, cex.axis=1.5,cex.lab=1.5)
@
\caption{Counts of simulated cells in states Q and R out of a total count of 200 converge over time to the same distribution starting from: a) all in state Q ; b) all in state R.}
 \label{fig:comp12_2}
\end{figure} 

\begin{figure}[h!] 
<<comp12_4, fig.width=5, fig.height=5, out.width='0.49\\linewidth',fig.show='hold', echo=FALSE>>=
nsteps <- 40 # set number of time steps
ncells <- 1000 # set number of cells
states <- matrix(1,nrow=ncells,ncol=nsteps+1)# initialize states matrix 
for (j in 1:ncells) { # go through all the cells
      for (i in 1:nsteps) { # go through time steps
        decider <- runif(1) # generate a random number
        if (states[j,i]==1) { 
          if (decider < prob1) { # transition from 1 to 2
            states[j,i+1] <- 2 # new state is 2
          } else { 
            states[j,i+1] <- 1 # new state is still 1
          }
        } else {
          if (decider < prob2) { #  transition from 2 to 1
            states[j,i+1] <- 1 # new state is 1
          } else { 
            states[j,i+1] <- 2 # new state is still 2
          }
        }
      } 
    }
state.count <- matrix(0,nrow=2,ncol=nsteps+1)
 for (k in 1:(nsteps+1)) {
	state.count[,k] <- tabulate(states[,k],nbins=2)
}
barplot(state.count,main='frequency of states vs. time',xlab='time', names.arg=1:(nsteps+1),ylab='quiescent or replicating',cex=1.5, cex.axis=1.5,cex.lab=1.5)
states <- matrix(2,nrow=ncells,ncol=nsteps+1)# initialize states matrix 
for (j in 1:ncells) { # go through all the cells
      for (i in 1:nsteps) { # go through time steps
        decider <- runif(1) # generate a random number
        if (states[j,i]==1) { 
          if (decider < prob1) { # transition from 1 to 2
            states[j,i+1] <- 2 # new state is 2
          } else { 
            states[j,i+1] <- 1 # new state is still 1
          }
        } else {
          if (decider < prob2) { #  transition from 2 to 1
            states[j,i+1] <- 1 # new state is 1
          } else { 
            states[j,i+1] <- 2 # new state is still 2
          }
        }
      } 
    }
state.count <- matrix(0,nrow=2,ncol=nsteps+1)
for (k in 1:(nsteps+1)) {
	state.count[,k] <- tabulate(states[,k],nbins=2)
}
barplot(state.count,main='frequency of states vs. time',xlab='time', names.arg=1:(nsteps+1),ylab='quiescent or replicating',cex=1.5, cex.axis=1.5,cex.lab=1.5)
@
\caption{Counts of simulated cells in states Q and R out of a total count of 1000 converge over time to the same distribution starting from: a) all in state Q ; b) all in state R.}
\label{fig:comp12_3}
\end{figure} 

\section{Bioinformatics and Markov models}
\label{sec:bio12}
In section \ref{sec:model12} we saw a simple Markov model for a string of characters, which was used to model a poetic text in Russian. While it did provide some information about the distribution of the vowels and consonants in the text: for instance, that it is substantially more likely that a vowel is followed by a consonant than by another vowel, the usefulness of the model is limited. However, analysis of strings of characters is a crucial component of modern biology which is awash in sequence data: DNA, RNA, and protein sequences from different organisms are pouring into various data bases. Markov models have become indispensable for making sense of sequence data.

One of the major problems in \index{bioinformatics}\emph{bioinformatics} is identifying portions of the genome which code for proteins \citep{pevsner_bioinformatics_2009}. A \index{genome}genomic sequence consists of four letters, but their meaning and function depends on where they are and how they are used. Some parts of the genome (in humans, over 90\%) are not part of a gene, and the DNA sequence is never translated into an amino acid sequence. Others are genes, which are continuous chunks of DNA sequence that are flanked by a promotor sequence and regulatory region which controls when a gene is \emph{expressed}, followed by the gene proper which is transcribed into RNA and then translated into amino acids. Some parts used to be genes, but are no longer in use, those are called \index{genome!pseudogenes}\emph{pseudogenes}. These can be difficult to distinguish from actual, functional genes, because their sequences still have similar features, including the proximity of promoters, regulatory and coding regions. 

Within the borders of a gene there are other divisions. In eukaryotic genomes, after the gene sequence is transcribed into RNA, some portions called \index{genome!introns}  \emph{introns} are cut out, then the remaining pieces called \index{genome!exons}\emph{exons} are spliced together and only then translated into protein sequences. The role of introns in biology is a topic of ongoing research, since it seems rather wasteful to transcribe portions of genes, which are sometimes considerably longer than the protein-coding exons, only to discard them later. The problem of identifying introns within a gene is important.

Markov models are used to determine the structure behind the sequence of letters. Based on known sequences of exons and introns, one can generate a \index{Markov!HMM}\emph{Hidden Markov Model} (HMM) that connects the DNA sequence with its underlying meaning: whether it is part of an exon or an intron. These models are more complex than the plain Markov models that we have studied: they involve two sets of states: the hidden ones, like introns and exons, which are not observable, and the observations, such as the four nucleotides (A,T,G,C). There are also two sets of transition probabilities: the transition probabilities between hidden states, and the \emph{emission probabilities}, which are the probabilities that a hidden state produces a particular observation.

Figure \ref{fig:ch12_hmm_gene} shows an example of such a model for gene structure. The HMM has three hidden states: E (exon), 5 (the 5? boundary of an intron), and I (intron). Each of these states has its own probability distribution of nucleotides (letters in the sequence), with Exons containing equal proportions of all four letters, the 5? almost always being a G, and the Introns containing four times as many As and Ts as Gs and Cs.  The length of an intron is arbitrary, so the state has a probability of remaining in the same state. Each of the hidden states has its own probability of ``emitting'' a letter, so one can devise algorithms for finding the most probable string of hidden states based on an observed sequence of nucleotides. HMM enables intron-hunting to be done in a systematic manner, although, as with any random model, the results are never certain.

\begin{figure}[h!] %  figure placement: here, top, bottom, or pages
   \centering
 \includegraphics[width=4.5in]{ch12/hmm_model.png} 
   \caption{A diagram of a simple Hidden Markov Model for a eukaryotic gene, figure from \citep{eddy_what_2004}, used by permission.}
   \label{fig:ch12_hmm_gene}
\end{figure}

\discussion{What does the Markov property mean for Hidden Markov Models presented in this paper? How reasonable is it for an actual genetic sequence?}

\discussion{Hidden Markov models can predict the ``best state path'' or the sequence of hidden states with the highest probability. Why is a single state path often not sufficient to answer the questions?}

\discussion{What additional assumptions does the HMM in figure \ref{fig:ch12_hmm_gene} make about the distribution of letters in an exon or intron? Comment on the biological implications.}

\discussion{What bioinformatics problems are HMMs best suited for? What are some of their drawbacks?}

\discussionshere{The following questions refer to the paper ``What is a hidden Markov model?'' \citep{eddy_what_2004}.}


\section{Computational projects}
\label{sec:proj12}
In the projects below you will generate multiple simulations over a number of time steps. The states of $n$ simulations for $m$ days will be stored in a matrix with $n$ rows and $m+1$ columns, because we begin with the initial state on day 1 and then add $m$ more days. Suppose that this matrix is called \texttt{states} and we are simulating the two-state disease models, then the the state (1 or 2) of person number 3 after 10 days will be recorded in \texttt{states[3,11]}. To plot the histogram of the states at a particular time one can use the table() function together with barplot(), for instance to plot the distribution on day 10:
<<proj12-1, tidy=FALSE, tidy.opts=list(width.cutoff=50),eval=FALSE>>=
barplot(table(states[,10]),
	main='distribution at time 10')
@
To plot the distribution of states over all different times, using table() on the entire matrix doesn't work. Instead, one has to generate a vector of counts for each time step and then plot it using barplot(). The script below shows how to do this, given a matrix of states (states), the number of time steps (numsteps) and the number of states (numstates) which is 2 for the SI model.
<<proj12_2, tidy=FALSE, tidy.opts=list(width.cutoff=50),eval=FALSE>>=
state.count <- matrix(0,nrow=numstates,ncol=nsteps+1)
for (k in 1:(numsteps+1)) {
  state.count[,k] <- tabulate(states[,k],
  	nbins=numstates) 
  # count the states at timestep
}
barplot(state.count, names.arg=1:(numsteps+1),
	ylab='S or I')
@

\subsection{multiple simulations of a two-state model}
In this section you will generate multiple simulations \index{Markov!SI model}\index{model!epidemiology}of the two-state SI model with the same transition probabilities as in the last chapter's assignment: 0.04 (from S to I) and 0.12 (from I to S). Take your code for simulation of this model model from chapter 11 and add another for loop around your script to generate a two-dimensional matrix of states. 

\section*{Tasks}
\begin{enumerate}
\item  Simulate 100 individuals for 20 days (generating 100 separate state strings of length 21). Simulate the state histories of 100 individuals for 20 days (generating 100 separate state strings), starting with all 100 individuals in state S (number 1). Plot the histograms (frequencies of states 1 and 2) over time using the code provided above. Describe the behavior of the histograms over time. Does the distribution after 20 days remain the same if you run your script multiple times?

\item Repeat the last task, but set the initial states of all 100 individuals to I (number 2). Again, plot the histograms (frequencies of states 1 and 2) and describe the behavior of the histogram over time.  Does the distribution between 1 and 2 look the same as in the previous task after 20 days?

\item Change the infection rate (transition from S to I) to 0.2, and simulate a population of 100 individuals over 20 days, starting with everyone in the susceptible state, and plot the histograms over time. Describe the behavior of the histogram over time. Does the distribution after 20 days remain the same if you run your script multiple times? Do the histograms converge to the same distribution as in task 1?

\item Keep the same transition probability as in task 3 and simulate a population of 100 individuals over 20 days, starting with everyone in the infected state, and plot the histograms over time. Describe the behavior of the histogram over time. Does the distribution after 20 days remain the same if you run your script multiple times? Do the histograms converge to the same distribution as in task 2?
\end{enumerate}

\subsection{multiple simulations of a three-state model} 
In this project we return to the three-state model of nicotinic acetylcholine receptor  \index{Markov!ion channel model} \index{ion channel!nAChR} (nAChR) we saw in the previous chapters. As before, set the transition probabilities to the following: 0.04 (from R to C), 0.07 (from C to R), 0.12 (from C to O) and 0.02 (from O to C); the other transition probabilities are 0. Take your code for simulation of this model model from chapter 11 and add another for loop around your script to generate a two-dimensional matrix of states. 

\section*{Tasks}
\begin{enumerate}
\item  Simulate 300 ion channels for 100 microseconds, with all 300 channels initially in state C, and plot the histograms over time using the sample code above. Describe the behavior of the histogram over time. Does the distribution after 100 microseconds remain the same if you run your script multiple times?

\item Repeat the last task, but set the initial states of all ion channels to C. Again, plot the histograms and describe the behavior of the histogram over time.  Does the distribution look the same as in the previous task after 100 microseconds? Has the distribution converged?

\item Repeat the last task, but set the initial states of all ion channels to O. Again, plot the histograms and describe the behavior of the histogram over time.  Does the distribution look the same as in the previous task after 100 microseconds? Has the distribution converged?

\item Change the opening rate (transition from C to O) to 0.02 and repeat the computations in task 1 with initial states set to C. Plot the the resulting histogram over time and describe the behavior of the histogram over time. Does the distribution after 100 microseconds remain the same if you run your script multiple times? Compare the distribution you observe with the distribution in task 1.

\item Repeat the last task, but set the initial states of all ion channels to C. Again, plot the histograms and describe the behavior of the histogram over time.  Does the distribution between look the same as in the previous task after 100 microseconds? Has the distribution converged?

\item Repeat the last task, but set the initial states of all ion channels to O. Again, plot the histograms and describe the behavior of the histogram over time.  Does the distribution between look the same as in the previous task after 100 microseconds? Has the distribution converged?

\end{enumerate}
