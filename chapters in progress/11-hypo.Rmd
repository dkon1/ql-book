# Hypothesis testing

> Sometimes I'm right and I can be wrong  
My own beliefs are in my song.  
-- Sly and the Family Stone, *Everyday People*


Much of the scientific method is based on testing hypotheses and deciding whether to reject them. To do this, scientists formulate an idea (hypothesis), then accumulate data that can challenge it, and if the data contradict the hypothesis, they discard it (the hypothesis, not the data!) No hypothesis in science is ever proven in an absolute sense, which is why it is fundamentally different from mathematics. A hypothesis that has survived many tests and was found to be consistent with all available observations becomes a theory, like the theory of gravity or of evolution. But unlike a theorem, a scientific theory is not certain, and if solid evidence were to surface that contradicts Newton's gravitational theory, it would be falsified and thrown out (again, the theory, not the evidence.)

In this chapter we will describe the framework of hypothesis testing and apply it to the specific task of deciding whether two variables are independent. After reading it you will know:

* The difference between the truth of the hypothesis and a test result
* Describe four different outcomes of hypothesis testing
* Compute different hypothesis testing error rates
* Explain the meaning of p-value
* Use R to perform the chi-squared test

## Hypothesis testing

The hypothesis to be tested is usually called the *null hypothesis*, which helpfully rhymes with dull, because it represents the lack of anything interesting, essentially the default state of the system. In order to reject the null hypothesis, the data has to be substantially different from what is expected as default. For instance, medical tests have the null hypothesis that the patient is normal/healthy, and only if the results are substantially different from normal the patient is considered ill. Another common example is the criminal justice system: a defendant on trial undergoes a binary test where the null hypothesis is innocence. Only if the prosecutor's evidence is strong, that is, shows guilt beyond a reasonable doubt, that the null hypothesis is rejected and the defendant found guilty.

Tests are binary, in that there are only two possible decisions: to reject the hypothesis or to not reject it. We can never truly accept a hypothesis as true, due to the impossibility of perfect knowledge of the world. The decision to reject a hypothesis is called a *positive* test result, which seems backwards, but remember that the default or null hypothesis is a lack of anything unusual or interesting, so if the data are different from default, it is called a positive result. The decision to not reject the null hypothesis is called a *negative* test result. You are probably familiar with this in a medical context: if you've ever been tested for a disease, you know that a negative result is good news!

Hypothesis testing is also binary on the front end: if we had perfect knowledge, we could say whether the hypothesis is true or not. Ideally, we want the test to reject a false null hypothesis, and not reject a true null hypothesis. These results are called, respectively, a \emph{true positive} and a \emph{true negative}. In the language of probability, they are defined as follows:

 
\begin{mosdef}
For a hypothesis test, the probability of a \index{test!true positive} \emph{true positive} result is the probability of a positive result for a false hypothesis, and the probability of a \index{test!true negative} \emph{true negative} result is the probability of a negative result for a true hypothesis, or in mathematical notation:
$$ P(TP) = P(Pos \& F); \; P(TN) = P(Neg \& T)  $$
Conversely, the probability of a \index{test!false positive} \emph{false positive} result is the probability of a positive result for a true hypothesis, and the probability of a  \index{test!false negative}\emph{false negative} result is the probability of a negative result for a false hypothesis, or in mathematical notation:
$$ P(FP) = P(Pos \& T); \; P(FN) = P(Neg \& F)  $$
\end{mosdef}
\begin{table}[h!]
   \centering
   \begin{tabular}{ccc} % Column formatting, @{} suppresses leading/trailing space
        Test result & Hypothesis False  &  Hypothesis True \\
      Positive   & TP & FP \\
      Negative   & FN  & TN \\
   \end{tabular}
   \caption{Two-way table summarizing the four possible results of hypothesis testing. The values may be expressed either as counts, or as fractions (probabilities).}
   \label{tab:Hypothesis_Testing}
\end{table}
These definitions are summarized in the table \ref{tab:Hypothesis_Testing}. Based on these probabilities, one can compute the measures of quality of a given test, defined as follows:

\begin{mosdef}
The \index{test!sensitivity} \emph{sensitivity} of a test is the probability of obtaining the positive result, given a false hypothesis; and the  \index{test!specificity}  \emph{specificity} of a test is the probability of obtaining the negative result, given a true hypothesis. There are two kinds of \emph{error rates}:  \index{test!type I error} \emph{type I error rate} is the probability of obtaining the positive result, given a true hypothesis (complementary to specificity), and the  \index{test!type II error}  \emph{type II errors} is the probability of obtaining the negative result, given a false hypothesis (complementary to sensitivity). All four parameters (rates) of a binary test are summarized as follows:
$$ Sen = \frac{TP}{TP+FN};  \; Spec = \frac{TN}{TN+FP}$$
$$ FPR = \frac{FP}{TN+FP};  \; FNR = \frac{FN}{TP+FN}$$
\end{mosdef}
Notice that knowledge of sensitivity and specificity determine the type I and type II error rates of a test since they are complementary events.  Of course, it is desirable for a test to be both very sensitive (reject false null hypotheses, detect disease, convict guilty defendants) and very specific (not reject true null hypotheses, correctly identify healthy patients, acquit innocent defendants), but no test is perfect, and sometimes it makes the wrong decision. This is where statistical inference comes into play: given some information about these parameters,  a statistician can calculate the error rate in making different decisions.

\exercise{Calculate the marginal probabilities of the individual random variables, i.e. the probability of positive and negative X-ray test results, and of TB being present and absent.}

\exercise{Find the probability of positive result given that TB is absent (false positive rate) and the probability of a negative result given that TB is absent (specificity).}

\exercise{Find the probability of negative result given that TB is present (false negative rate) and the probability of a positive result given that TB is present (sensitivity).}

\exercise{Find the probability that a person who tests positive actually has TB (probability of TB present given a positive result).}

\exercise{Find the probability that a person who tests negative does not have TB (probability of no TB given a negative result).}

\exercise{Assuming the test result and the TB status are independent, calculate the expected probability of both TB being present and a positive X-ray test.}

\exercise{Under the same assumption, calculate the expected probability of both TB being absent and a positive X-ray test.}

\exerciseshere{Table \ref{tab:TB_test} shows the results of using X-ray imaging as a diagnostic test for tuberculosis in patients with known TB status. Use it to answer the questions below.}
\begin{table}[h!]
   \centering
   %\topcaption{Table captions are better up top} % requires the topcapt package
   \begin{tabular}{ lcr} % Column formatting, @{} suppresses leading/trailing space
      Test for TB &  TB absent & TB present \\
      Negative X-ray   &  1739  & 8    \\
      Positive X-ray    &  51 &  22  \\
   \end{tabular}
   \caption{Data for TB testing using X-ray imaging}
   \label{tab:TB_test}
\end{table} 

\subsection{rejecting the null hypothesis}
Hypothesis testing is one of the most important applications of statistics. A lot of people think of statistics as a collection of tests to be used for different hypotheses, which is too simplistic, but different tests do occupy a large fraction of statistics books. In this book we will only dip a toe into hypothesis testing, and will primarily approach it in a probabilistic (model-centered) way rather than from a  statistical (data-centered) viewpoint. Probability allows us to calculate the sensitivity and specificity of a test for a given null hypothesis, provided the hypothesis is simple enough and the data are sampled correctly. Here is a simple example:

\textbf{Example: testing whether a coin is fair.} Suppose we want to know whether a coin is fair (has equal probabilities of heads and tails) based on a data set of several coin tosses. How much evidence do we need in order to reject the hypothesis of a fair coin with a small chance of making a type I error? What is the corresponding chance of making a type II error, not detecting an unfair coin? 

Let us first consider a data set of two coin tosses. If one is heads and one is tails, it's obvious we have no evidence to reject the null hypothesis. But what if both times the coin landed heads? The probability of this happening for a fair coin is 1/4, which means that if you reject the null hypothesis based on the evidence, your probability of committing a type I error is 1/4. However, it is very difficult to answer the second question about making a type II error, because in order to do the calculation we need to know something about the probability of heads or tails. The hypothesis being false only means that the probability is not 1/2, but it could be anything between 0 and 1. 

Let us see how this test fares for a larger sample size. Suppose we toss a coin $n$ times, and if all $n$ come up heads, then we reject the hypothesis that the coin is fair. A fair coin will come up all heads with probability $1/2^n$, so that is the rate of false positives for this test. For example, if a coin came up heads ten times in a row, there is only a 1/1024 probability that this is the result of a fair coin, so the probability of making a type I error is less than 0.1\%. Is this careful enough? This question cannot be answered mathematically - it depends on your sense of acceptable risk of making a mistake. Notice that if you decide to use a very stringent criteria for rejecting a null hypothesis, you will necessarily end up not rejecting more false hypotheses. Such is the face of us mortals, dealing with imperfect information in an uncertain world.

This leads us to an important new idea: the probability that a given data set is produced from the model of the null hypothesis is called the \emph{p-value} of a test. In the example of coin tosses we just studied, the p-value was $p=1/2^n$. However, what if the data had 9 heads out of 10 tosses? The p-value then would be the probability of obtaining \textbf{9 or 10 heads out of 10}. This is because to compute the probability of making a false positive error, we consider all cases that could have produced the result that is as different from expectation, or even further from expectation (in this case, 5 heads out of 10) than the data. \citep{whitlock_analysis_2008}.

\begin{mosdef}
For a given data set $D$ and a null hypothesis $H_0$, the  \index{test!p-value}  \emph{p-value} is defined as the probability of obtaining a result \textbf{as far from expectation or farther than the data}, given the null hypothesis.
\end{mosdef}
The p-value is the most used, misused, and even abused quantity is statistics, so please think carefully about its definition. One reason this notion is frequently misused is because it is very tempting to conclude that the p-value is the probability of the null hypothesis being true, based on the data. That is not true! The definition has the opposite direction of conditionality - we assume that the null hypothesis is true, and based on that calculate the probability of obtaining the data. There is no way (according to classical ``frequentist'' statistics) of assigning a probability to the truth of a hypothesis, because it is not the result of an experiment. 

The simplest way to describe the p-value is that it is the likelihood of the hypothesis, based on the data set. This means that the smaller the p-value, the less likely the hypothesis, and one can be more certain about rejecting the hypothesis. Alternatively, the p-value represents the \textbf{probability of making a type 1 error, or rejecting the correct null hypothesis.} These two notions may seem to be in conflict, but they tell the same story: if the hypothesis is likely, the probability of making a type 1 error is high.

## Chi-squared test

Now we are ready to address the question of testing the independence hypothesis based on the table of observations and the calculated table of expected counts. In order to measure the difference between what is expected for a data table with two independent variables and the actual observations, we need to gather these differences into a single number. One can devise several ways of doing this, but the accepted measure is called the \index{chi-squared!statistic}\emph{chi-squared statistic} and it is defined as follows:
\begin{mosdef}
The chi-squared value for the independence test is calculated on the basis of a two-way table with $m$ rows and $n$ columns as the sum of the differences between the observed counts and the computed expected counts as follows:
$$  \chi^2= \sum_i \frac{(Observed(i)-Expected(i))^2}{Expected(i)} $$
The number of degrees of freedom of chi-squared is $d.f. = (m-1)(n-1)$.
\end{mosdef}

This number describes how far away the data is from what is expected for an independent data set. Therefore, the larger the chi squared statistic, the larger the differences between observed and expected frequency, and thus the null hypothesis  of independence is less likely. However, simply obtaining the $\chi^2$ is not enough to say whether the two variables are independent. We need to translate the chi-squared value into the language of probability, that is to ask, what is the probability of obtaining a data set with a particular $\chi^2$ value, if those two variables were independent.

This question is answered using the \index{distribution!chi-squared}\index{chi-squared!distribution}\emph{chi-squared probability distribution}, which describes the probability of the random variable $\chi^2$. Like the normal distribution we saw in section \ref{sec:math5} it is a continuos distribution, because $\chi^2$ can take any (positive) real value. In another similarity, the  $\chi^2$ distribution has an even more complicated functional form than the normal distribution, so I do not present it here, because it is not enlightening. I will also not share the derivation of the mathematical form of the distribution, as it is far outside the goals of this text. In practice, nobody computes either the chi-squared statistic or its probability distribution function by hand, instead computers handle these chores. The chi-squared distribution has one key parameter, called the number of degrees of freedom, which was defined above. Depending on d.f. the distribution changes, specifically for more degrees of freedom the distribution moves to the right, that is, the chi-squared values tend to be larger.

![The chi-squared distribution is used to compute the p-value as the total probability of obtaining a $\chi^2$ value at least as far from 0 as observed. (image by Inductiveload in public domain via Wikimedia Commons)](ch6/Chi-Squared_Distribution.png)

The chi-squared distribution is used to determine the probability of obtaining a chi-squared statistic as at least as large as observed, based on the null hypothesis of independence. Figure \ref{fig:ch6_chisq_dist} shows a plot of the chi-squared distribution, as well as the total probability to the right of an observed $\chi^2$. This allows one to use it for the *chi-squared test* for independence between random variables, by comparing the p-value obtained from the distribution (by a computer) against a number called the \index{statistics!significance}\emph{significance} level, which is decided by humans. The significance value $\alpha$ is a threshold that the test has to clear in order to reject the null hypothesis: if the p-value is less than $\alpha$, the independence hypothesis is rejected, otherwise it stands, although one can never say that the independence hypothesis is accepted.

There is no mathematical or statistical method for determining the appropriate significance level, it is entirely up to the users to decide how much risk of rejecting a true null hypothesis they are willing to tolerate. If you choose 0.01, that means you want the likelihood of the hypothesis to be less than 1\% percent in order to reject it. This is entirely arbitrary, and using a rigid significance level to decide whether a hypothesis is true can lead to major problems which we will discuss in the next chapter.

Like all mathematical models, the chi-squared distribution relies on a set of assumptions. If the assumptions are violated, then the probability distribution does not apply and the p-value does not reflect the actual likelihood of the hypothesis. Here are the \index{chi-squared!assumptions}assumptions:

\begin{itemize}

\item the data is from a simple random sample of the population, so that every subgroup of a certain size has an equal probability of being selected for the sample.


\item the sample size must be sufficiently large, the exact number depends on the degrees of freedom 


\item expected cell counts cannot be too small; a rule of thumb is that they must be at least 5 for a two by two table


\item the observations are always assumed to be independent of each other, that is one measurement has no effect on others; note that this does not mean the assumption that random variables are independent (that's what we are testing!)

\end{itemize}

\section{Hypothesis testing in R}
\label{sec:comp6}
R has many functions for different tests, including the chi-squared test\index{R programming!chi-squared test}. To use it, one first has to input a data set in the form of a two-way table, where each row represents the values of one random variable, and each column represents the values of the second random variable. The following script shows how to manually input a 2 by 2 contingency table into a matrix. In the matrix function, ncol stands for number of columns, and nrow for number of rows. Notice the order in which the numbers are put into the matrix: down the first column, then the second, etc. Type \texttt{help(matrix)} for more details. In order to access a specific element of the matrix, just like in vectors, R uses square brackets and two indices, first one for row, and second for column. Below are examples of accessing two elements of the matrix data defined above, and how to reference a particular element of the matrix.
<<comp6-1, tidy=TRUE,tidy.opts=list(width.cutoff=50)>>=
data <- matrix(c(442,514,38,6),ncol=2,nrow=2)
print(data)
print(data[1,2])
print(data[2,1])
@

Based on a given data set, how likely is the hypothesis that the two random variables are independent?  It is hard to do by hand (in the old days, you looked it up in a table of chi-squared values) but R will do it all for us: 1) calculate the expected counts, 2) compute the chi-squared value for the table, and 3) use the number of degrees of freedom and the chi-squared value to calculate the p-value of the independence hypothesis based on it. Use the \texttt{chisq.test()} function, and you will see output like this:
<<comp6-2, tidy=TRUE,tidy.opts=list(width.cutoff=50)>>=
test.output <- chisq.test(data)
print(test.output)
@
The results are the chi-squared values, the number of degrees of freedom (which depends on the number of rows and columns in the two-way table) and the p-value. The p-value is used to decide whether to reject the hypothesis, because it represents the likelihood of the hypothesis, given the data. In this case, the p-value is pretty small, so it seems relatively safe to reject the hypothesis of independence.  To see the results of the hypothesis test, type \texttt{print(test.output)}, and to access the p-value individually, use \texttt{test.output\$p.value}.

Finally, we need to specify the significance level $\alpha$ for the hypothesis test. This refers to the probability of rejecting a true null hypothesis, by random chance. For instance, if you reject the hypothesis at $\alpha=0.05$ significance, you're accepting a 5\% chance that you \emph{falsely rejected a correct hypothesis} (also called the rate of false positives). Note that it says nothing about failing to reject an incorrect hypothesis (also called the rate of false negatives.) 

\section{Independence in data sets}
\label{sec:bio6}
\subsection{maternal age and Down's syndrome}
Let us return to the data presented in section \index{medical!maternal age}\ref{sec:model6}. We noted that the fraction of women in different age categories carrying fetuses with DS are different, but how certain are we that is not a fluke? To test the hypothesis of independence, we input the data into R and then run the chi-squared test:
<<bio6-1, tidy=TRUE,tidy.opts=list(width.cutoff=50)>>=
data <- matrix(c(29806, 8135, 28, 64),ncol=2,nrow=2)
test.output <- chisq.test(data)
print(test.output)
@
Based on the calculations, we can answer the questions posed at the end of section \ref{sec:model6}:
\begin{enumerate}

\item Independence between the two variables means that the fraction of women with DS fetuses should be the same for all age groups (or vice versa).

\item To measure the extent of dependence of the two variables, calculate the chi-squared parameter, which is about 122.

\item This allows us to calculate the p-value and test the hypothesis. Here the p-value is very small (the number is actually caused by machine error). Therefore, the hypothesis can be 
rejected with a very small risk of making an error.
\end{enumerate}


\subsection{stop-and-frisk and race}
The practice of New York Police Department dubbed ``stop-and-frisk'' \index{policy!stop-and-frisk}gives police officers to power to stop, question, and search people on the street without a warrant. Since the practice commenced in the early 2000s, it has generated controversy for several reasons. First, the 4th amendment to the U.S. Constitution limits the power of the state to detain and search citizens, by mandating that officials first obtain a warrant based on ``probable cause,'' while based on the Supreme Court interpretation, police are allowed to stop someone without a warrant provided ``the officer has a reasonable suspicion supported by articulable facts'' that the person may be engaged in criminal activity. Exactly what these conditions mean and whether officers in NYPD always had reasonable suspicions before stopping is a legal matter, rather than a statistical one, and you can read what federal judge Scheindlin ruled on this matter here \citep{_stop-and-frisk_????}.

The second issue raised by stop-and-frisk is whether it violates the principle of equal protection under the law enshrined in the 14th amendment of the Constitution. The idea that the law and its agents should treat people of different backgrounds the same, that people can be punished for their actions, but not for who they are, is deeply rooted in American law and culture. Critics of stop-and-frisk charge that officers disproportionately stop and search people of African-American and Hispanic background and therefore violate their constitutional rights to equal protection. As part of the trial, statistical evidence was introduced about the number of stops of New Yorkers of different racial backgrounds, how many of those stops resulted in the use of force, and how many uncovered evidence of criminal activity leading to an arrest. Let us analyze the data using our tools to address whether race and somebody being ``stopped-and-frisked'' are related.

The data in the summary of judge Scheindlin's decision is as follows: between 2004 to 2012, out of 4.4 million stops, 52\% of the people stopped were black, 31\% of the people stopped were Hispanic, and 10\% of the people were white. The population of New York according to the 2010 census is approximately 23\% black, 29\% Hispanic, and 33\% white. You may notice that the fractions are suggestive of a higher probability of stops of African-Americans, and lower probability of stops of white individuals, but we cannot use fractions to perform a chi-squared test, because actual counts are necessary to quantify the uncertainty in the testing. 

Below I present data in the form of counts for only the calendar year 2011 \citep{_nypds_????}, in the form of a contingency table with two variables: race/ethnicity and being stopped by police without a warrant. I have used the census population of New York \url{http://factfinder2.census.gov} and its breakdown by race (white only, black only, Hispanic, other). The data are presented in table \ref{tab:stop_frisk_race}, and then are input in R and run through a chi-squared independence test.
\begin{table}[h!]
   \centering
   \begin{tabular}{lccccc} % Column formatting, @{} suppresses leading/trailing space
         & white & black & Hispanic & other & total  \\
      stopped & 61,805 & 350,743 & 223,740 & 49,436 & 685,724 \\
      not stopped  &  2,665,172 & 1,527,029  & 2,119,718 & 1,201,578 & 7,513,497\\
      Total & 2,726,977 & 1,877,772 & 2,343,458 & 1,251,014 & 8,199,221 \\
   \end{tabular}
   \caption{Number of New York residents subjected to stop-and-frisk and their racial identification.}
   \label{tab:stop_frisk_race}
\end{table}

<<bio6-2, tidy=TRUE>>=
data <- matrix(c(61805, 2665172, 350743, 1527029, 223740, 
2119718, 49436, 1201578),ncol=4,nrow=2)
test.output <- chisq.test(data)
print(test.output)
@
The results confirm what comparing the percentages suggested: the race of a person in NYC is not independent of whether or not they get stopped and frisked, with only a tiny probability that this disparity could have happened by chance. However, this is only the beginning of the analysis that experts performed for the court trial. Drawing conclusions about motives from the data is tricky, since two variables may be related without a causal connection. Defenders of the practice have argued that the racial disparities reflect differences in criminal activity. The data, however, show that only 6\% of the stops result in arrests, and 6\% more in court summons, so the vast majority of those stopped and frisked were not engaged in criminal activity. 

\section{Computational projects}
\label{sec:proj6}
In this section you will use two data sets: one that you collect from your class, and one from marine biology research. In both cases, you will input the data into a matrix in R and then use the chi-squared function to test for independence between two variables.

\subsection{thumb-on-top preference and sex}
Collect data in you group of students on the relationship between two random variables: sex (male or female) and thumb-on-top preference (left or right). Ask everyone to determine whether they prefer to have the right or left thumb on top when they clasp their hands with fingers interlaced (the preference is usually very strong, if you try to clasp your hands the other way, it feels uncomfortable). Those who are male and prefer to have their right thumb on top raise their hands to be counted, then female students who prefer their right thumb on top, and so on, until you have filled the 2 by 2 data table. (Students who don't identify as one of the traditional genders can pick either one.)

\section*{Tasks}
\begin{enumerate}
\item Use the data set to construct a two-way table with handedness and thumb-on-top preferences as the two variables and assign it to an R matrix.
\item  Use the R chi-squared function to calculate the p-value for the data set and decide whether to reject the hypothesis that sex and thumb on top preference are independent at significance levels $\alpha = 0.05, 0.01, 0.001$.
\end{enumerate}

\subsection{relationship between species and habitat}
\begin{table}[h!]
   \centering
   \begin{tabular}{ccc} % Column formatting, @{} suppresses leading/trailing space
      Habitat  & G. moringa &G. vicinus \\
      grass      & 127  & 116  \\
      sand      &  99   &  67  \\
      border    & 264 & 161   \\      
   \end{tabular}
   \caption{Number of individuals of each species sighted in each habitat}
   \label{tab:eels}
\end{table}
The  2-way table contains data for sighting of two moray eels species of genus \emph{Gymnothorax}, species \emph{moringa} and \emph{vicinus} in different habitats in a reef in Belize. We will test the hypothesis that the species of eel and the habitat are independent.
\section*{Tasks}
\begin{enumerate}
\item Put the data set in the two-way table \ref{tab:eels} above into an R matrix.

\item  Use the R chi-squared function to calculate the p-value for the data set and decide whether to reject the hypothesis that  species and habitat are independent at significance levels $\alpha = 0.05, 0.01, 0.001$.
\end{enumerate}

\subsection{independence testing of simulated data}
In this section you will  generate random numbers to produce a simulated 2 by 2 contingency table. Suppose that you have two groups of people: one with genotype A and the other with genotype B. The question is: does this genotype predispose people to a disease? In other words, are the variables of genotype and disease linked?

You will generate a data set in which the true answer is known, so you can investigate how frequently the test is right and wrong. To do this, you will use the binomial random generator we first used in section \ref{sec:comp4}. The code below generates two vectors of length sample.size containing 0s and 1s, in which 0 stands for normal and 1 indicates disease. The vector dis.genA contains the disease status of all people with genotype A, and dis.genB similarly contains the status for people with genotype B. The number of 0s and 1s in the two vectors are then tabulated using the \texttt{table()} function and are assigned to a data table.
<<proj6-1,eval=FALSE,tidy=TRUE,tidy.opts=list(width.cutoff=50)>>=  
sample.size <- 100 
probA <- 0.2 # probability of disease for genotype A
probB <- 0.3 # probability of disease for genotype B
dis.genA <- rbinom(sample.size,1,probA)
dis.genB <- rbinom(sample.size,1,probB)
data.mat <- matrix(c(table(dis.genA),table(dis.genB)),nrow=2,ncol=2)
@
You are now ready to generate 2 by 2 tables for different scenarios and see how well the chi-squared test works for different probabilities of disease. Before doing the following tasks, think carefully: what are the variables in this test for independence? what does it mean for them to be independent?

\section*{Tasks}
\begin{enumerate}

\item Generate data sets for genotype A  and genotype B of 50 patients each with 0.5 probability of disease for both data sets. Plot the histograms of both data sets and report whether they look similar. Place the counts into a data matrix and run a chi-squared test on it. Does test lead to rejection for the independence hypothesis at 0.1 significance level? How about at 0.05? Based on how you generated the data sets, is the hypothesis actually true? Did the chi-squared test get the result right?

\item Repeat the code in the first task 100 times by adding a for loop around it. In each loop iteration you generate the two datasets as before (both with probability of disease of 0.5), place the counts in a data matrix and perform a chi-squared test for independence of genotype and disease. Report how many of the 100 chi-squared tests result in rejection of the null hypothesis at the 0.1 and 0.05 significance level by using \texttt{test.output\$p.value} to put the p-values in a vector (e.g.  \texttt{p.vec}) and report how many of the p-values are less than the significance level $a$ (the command \texttt{sum(p.vec<a)} will do this for you.) Since you know exactly whether the hypothesis is true, how many of the test conclusions are \textbf{wrong} for each significance level? Explain whether this agrees with the notion of p-value.

\item Now let us change the two data sets so they are different, and thus the data are not independent. For the first data set (genotype A), let the probability of disease be 0.4 and for the second data set (genotype B) let disease occur with probability 0.6. Count how many of the 100 chi-squared tests result in rejecting the null hypothesis at the 0.1 and 0.05 significance level. Since you know exactly whether the hypothesis is true, how many of the test conclusions are \textbf{wrong} for each significance level? Explain whether this agrees with the notion of p-value.

\item Finally, let us generate data from very different probabilities of disease. For the first data set (genotype A), let the probability of disease be 0.2 and for the second data set (genotype B) let disease occur with probability 0.8. Count how many of the 100 chi-squared tests result in rejecting the null hypothesis at the 0.1 and 0.05 significance level. Since you know exactly whether the hypothesis is true, how many of the test conclusions are \textbf{wrong} for each significance level? Explain whether this agrees with the notion of p-value.

\end{enumerate}