[["index.html", "Quantifying Life Purpose and purview 0.1 A brief motivation of mathematical modeling 0.2 Purpose of this book 0.3 Organization of the book", " Quantifying Life Dmitry Kondrashov 2021-02-22 Purpose and purview What is a man, said Athos, who has no landscape? Nothing but mirrors and tides. – Anne Michaels, Fugitive Pieces This is an online book to help biologists and biology-adjacent folks learn quantitative skills through the practice of programming in R. These skills can be roughly sorted into four types: Building models and understanding assumptions Writing code to perform computational tasks Performing mathematical analysis of models Working with data and using statistical tools These skills interface, intertwine, and reinforce each other in the practice of biological research and are thus presented concurrently in this book, instead of being corralled into separate courses taught by different departments, like mathematics, statistics, and computer science. Here I combine ideas and skills from all of these disciplines into an educational narrative organized by increasing exposure to programming concepts. 0.1 A brief motivation of mathematical modeling A mathematical model is a representation of some real object or phenomenon in terms of quantities (numbers). The goal of modeling is to create a description of the object in question that may be used to pose and answer questions about it, without doing hard experimental work. A good analogy for a mathematical model is a map of a geographic area: a map cannot record all of the complexity of the actual piece of land, because then the map would need to be size of the piece of land, and then it wouldn’t be very useful! Maps, and mathematical models, need to sacrifice the details and provide a birds-eye view of reality in order to guide the traveler or the scientist. The representation of reality in the model must be simple enough to be useful, yet complex enough to capture the essential features of what it is trying to represent. Mathematical modeling has long been essential in physics: for instance, it is well known that distance traveled by an object traveling at constant speed \\(v\\) is proportional to the time traveled (called \\(t\\)). This mathematical model can be expressed as an equation: %\\[d = vt\\] Since the time of Newton, physicists have been very successful at using mathematics to describe the behavior of matter of all sizes, ranging from subatomic particles to galaxies. However, mathematical modeling is a new arrow in a biologist’s quiver. Many biologists would argue that living systems are much more complex than either atoms or galaxies, since even a single cell is made up of a mind-boggling number of highly dynamic, interacting entities. That is true, but new advances in experimental biology are producing data that make quantitative methods indispensable for biology. The advent of genetic sequencing in the 1970s and 80s has allowed us to determine the genomes of different species, and in the last few years next-generation sequencing has reduced sequencing costs for an individual human genome to a few thousand dollars. The resulting deluge of quantitative data has answered many outstanding questions, and also led to entirely new ones. We now understand that knowledge of genomic sequences is not enough for understanding how living things work, so the burgeoning field of systems biology investigates the interactions between genes, proteins, or other entities. The central question is to understand how a network of interactions between individual molecules can lead to large-scale results, such as the development of a fertilized egg into a complex organism. The human mind is not suited for making correct intuitive judgements about networks comprised of thousands of actors. Addressing questions of this complexity requires quantitative modeling. 0.2 Purpose of this book This textbook is intended for a college-level course for biology and pre-medicine majors, or more established scientists interested in learning the applications of mathematical methods to biology. The book brings together concepts found in mathematics, computer science, and statistics courses to provide the student a collection of skills that are commonly used in biological research. The book has two overarching goals: one is to explain the quantitative language that often is a formidable barrier to understanding and critically evaluating research results in biological and medical sciences. The second is to teach students computational skills that they can use in their future research endeavors. The main premise of this approach is that computation is critical for understanding abstract mathematical ideas. These goals are distinct from those of traditional mathematics courses that emphasize rigor and abstraction. I strongly believe that understanding of mathematical concepts is not contingent on being able to prove all of the underlying theorems. Instead, premature focus on abstraction obscures the ideas for most students; it is putting the theoretical cart before the experiential horse. I find that students can grasp deep concepts when they are allowed to experience them tangibly as numbers or pictures, and those with an abstract mindset can generalize and add rigor later. As I demonstrate in part 3 of the book, Markov chains can be explained without relying on the machinery of measure theory and stochastic processes, which require graduate level mathematical skills. The idea of a system randomly hopping between a few discrete states is far more accessible than sigma algebras and martingales. Of course, some abstraction is necessary when presenting mathematical ideas, and I provide correct definitions of terms and supply derivations when I find them to be illuminating. But I avoid rigorous proofs, and always favor understanding over mathematical precision. The book is structured to facilitate learning computational skills. Over the course of the text students accumulate programming experience, progressing from assigning values to variables in the first chapter to solving nonlinear ODEs numerically by the end of the book. Learning to program for the first time is a challenging task, and I facilitate it by providing sample scripts for students to copy and modify to perform the requisite calculations. Programming requires careful, methodical thinking, which facilitates deeper understanding of the models being simulated. In my experience of teaching this course, students consistently report that learning basic scientific programming is a rewarding experience, which opens doors for them in future research and learning. It is of course impossible to span the breadth of mathematics and computation used for modeling biological scenarios. This did not stop me from trying. The book is broad but selective, sticking to a few key concepts and examples which should provide enough of a basis for a student to go and explore a topic in more depth. For instance, I do not go through the usual menagerie of probability distributions in chapter 4, but only analyze the uniform and the binomial distributions. If one understands the concepts of distributions and their means and variances, it is not difficult to read up on the geometric or gamma distribution if one encounters it. Still, I omitted numerous topics and entire fields, some because they require greater mathematical sophistication, and others because they are too difficult for beginning programmers, e.g. sequence alignment and optimization algorithms. I hope that you do not end your quantitative journey with this book! I take an even more selective approach to the biological topics that I present in every chapter. The book is not intended to teach biology, but I do introduce biological questions I find interesting, refer the reader to current research papers, and provide discussion questions for you to wrestle with. This requires a basic explanation of terms and ideas, so most chapters contain a broad brushstrokes summary of a biological field, e.g. measuring mutation rates, epidemiology modeling, hidden Markov models for gene structure, and limitations of medical testing. I hope the experts in these fields forgive my omitting the interesting details that they spend their lives investigating, and trust that I managed to get the basic ideas across without gross distortion. 0.3 Organization of the book A course based on this textbook can be tailored to fit the quantitative needs of a biological sciences curriculum. At the University of Chicago the course I teach has replaced the last quarter of calculus as a first-year requirement for biology majors. This material could be used for a course without a calculus pre-requisite that a student takes before more rigorous statistics, mathematics, or computer science courses. It may also be taught as an upper-level elective course for students with greater maturity who may be ready to tackle the eigenvalues and differential equations chapters. My hope is that it may also prove useful for graduate students or established scientists who need an elementary but comprehensive introduction to the concepts they encounter in the literature or that they can use in their own research. Whatever path you traveled to get here, I wish you a fruitful journey through biomathematics and computation! "],["arithmetic-and-variables.html", "1 Arithmetic and variables 1.1 Blood circulation and mathematical modeling 1.2 Parameters and variables in models 1.3 First steps in R 1.4 R Assignment", " 1 Arithmetic and variables You can add up the parts, but you won’t have the sum; You can strike up the march, there is no drum. Every heart, every heart to love will come But like a refugee. – Leonard Cohen, Anthem Mathematical modeling begins with a set of assumptions. In fact, one may say that a mathematical model is a bunch of assumptions translated into mathematics. These assumptions may be more or less reasonable, and they may come from different sources. For instance, many physical models are so well-established that we refer to them as laws; we are pretty sure they apply to molecules, cells, and organisms as well as to inanimate objects. Thus we may use physical laws as the foundation on which to build models of biological entities; these are often known as first-principles (theory-based) models. Other times we have experimental evidence which suggests a certain kind of relationship between quantities, perhaps we find that the amount of administered drug and the time until the drug is completely removed from the bloodstream are proportional to each other. This observation can be turned into an empirical (experiment-based) model. Yet another type of model assumption is not based on either theory or experiment, but simply on convenience: e.g. let us assume that the mutation rates in two different loci are independent, and see what the implications are. These are sometimes called toy or cartoon models. This leads to the question: how do you decide whether a model is good? It is surprisingly difficult to give a straightforward answer to this question. Of course, one major goal of a model is to capture some essential features of reality, so in most biological modeling studies you will see a comparison between experimental results and predictions of the model. But it is not enough for a model to be faithful to experimental data! Think of a simple example: suppose your experiment produced 5 data points as a function of time; it is possible to find a polynomial (of fourth degree) that passes exactly through all 5 points, by specifying the coefficients of its 5 terms. This is called data fitting and it has a large role to play in mathematical modeling of biology. However, I think you will agree that in this case we have learned very little: we just substituted 5 values in the data set with 5 values of the coefficients of the mathematical model. To heighten the absurdity, imagine a data set of 1001 points that you have modeled using a 1000-degree polynomial. This is an example of overfitting, or making the model agree with the data by making it overly complex. Substituting a complicated model for a complicated real situation does not help understand it. One necessary ingredient of a useful model is simplicity of assumptions. Simplicity in modeling has at least two virtues: simple models can be grasped by our limited minds, and simple assumptions can be tested against evidence. A simple model that fails to reproduce experimental data can be more informative than a complex model that fits the data perfectly. If a simple model fails, you have learned that you are missing something in your assumptions; but a complex model can be right for the wrong reasons, like erroneous assumptions canceling each other, or it may contain needless assumptions. This is why good modeling is a difficult skill that balances simplicity of assumptions against fidelity to empirical data . In this chapter you will learn how to do the following: distinguish variables and parameters in models describe the state space of a model perform arithmetic operations in R assign variables in R 1.1 Blood circulation and mathematical modeling Galen was one of the great physicians of antiquity. He studied how the body works by performing experiments on humans and animals. Among other things, he was famous for a careful study of the heart and how blood traveled through the body. Galen observed that there were different types of blood: arterial blood that flowed out of the heart, which was bright red, and venous blood that flowed in the opposite direction, which was a darker color. This naturally led to questions: what is the difference between venous and arterial blood? where does each one come from and where does it go? You, a reader of the 21st century, likely already know the answer: blood circulates through the body, bringing oxygen and nutrients to the tissues through the arteries, and returns back through the veins carrying carbon dioxide and waste products, as shown in figure . Arterial blood contains a lot of oxygen while venous blood carries more carbon dioxide, but otherwise they are the same fluid. The heart does the physical work of pushing arterial blood out of the heart, to the tissues and organs, as well as pushing venous blood through the second circulatory loop that goes through the lungs, where it picks up oxygen and releases carbon dioxide, becoming arterial blood again. This may seem like a very natural picture to you, but it is far from easy to deduce by simple observation. Human blood circulates throughout the body and returns to the heart, veins shown in blue and arteries in red. Circulatory System en by LadyofHats in public domain via Wikimedia Commons. 1.1.1 Galen’s theory of blood Galen came up with a different explanation based on the notion of humors, or fluids, that was fundamental to the Greek conception of the body. He proposed that the venous and arterial blood were different humors: venous blood, or natural spirits, was produced by the liver, while arterial blood, or vital spirits, was produced by the heart and carried by the arteries, as shown in figure . The heart consisted of two halves, and it warmed the blood and pushed both the natural and vital spirits out to the organs; the two spirits could mix through pores in the septum separating its right and left halves. The vital and natural spirits were both consumed by the organs, and regenerated by the liver and the heart. The purpose of the lungs was to serve as bellows, cooling the blood after it was heated by the heart. Illustration of Galen’s conception of the blood system, showing different spirits traveling in one direction, but not circulating. Reproduced by permission of Barbara Becker. Is this a good theory of how the heart, lungs, and blood work? Doctors in Europe thought so for over one thousand years! Galen’s textbook on physiology was the standard for medical students through the 17th century. The theory seemed to make sense, and explain what was observable. Many great scientists and physicians, including Leonardo DaVinci and Avicenna, did not challenge the inaccuracies such as the porous septum in the heart, even though they could not see the pores themselves. It took both better observations and a quantitative testing of the hypothesis to challenge the orthodoxy. 1.1.2 Mathematical testing of the theory William Harvey was born in England and studied medicine in Padua under the great physician Hieronymus Fabricius. He became famous, and would perform public demonstrations of physiology, using live animals for experiments that would not be approved today. He also studied the heart and the blood vessels, and measured the volume of the blood that can be contained in the human heart. He was quite accurate in estimating the correct volume, which we now know to be about 70 ml (1.5 oz). What is even more impressive is that he used this quantitative information to test Galen’s theory. Let us assume that all of the blood that is pumped out by the heart is consumed by the tissues, as Galen proposed; let us further assume that the heart beats at constant rate of 60 beats per minute, with a constant ejection volume of 70 ml. Then over the course of a day, the human body would consume about \\[\\mathrm{Volume} = 70 \\ \\mathrm {mL} \\times 60 \\ \\mathrm {(beats \\ per \\ minute)} \\times 60 \\ \\mathrm {(minutes \\ per \\ hour)} \\times 24 \\ \\mathrm {(hours \\ per \\ day)}\\] or over 6,000 liters of blood! You may quibble over the exact numbers (some hearts beat faster or slower, some hearts may be larger or smaller) but the impact of the calculation remains the same: it is an absurd conclusion. Galen’s theory would require the human being to consume and produce a quantity of fluid many times the volume of the human body (about 100 liters) in a day! This is a physical impossibility, so the only possible conclusion in that Galen’s model is wrong. This led Harvey to propose the model that we know today: that blood is not consumed by the tissues, but instead returns to the heart and is re-used again . This is why we call the heart and blood vessels part of the circulatory system of the body. This model was controversial at the time - some people proclaimed they would ``rather be wrong with Galen, than right with Harvey’’ - but eventually became accepted as the standard model. What is remarkable is that Harvey’s argument, despite being grounded in empirical data, was strictly mathematical. He adopted the assumptions of Galen, made the calculations, and got a result which was inconsistent with reality. This is an excellent example of how mathematical modeling can be useful, because it can provide clear evidence against a wrong hypothesis. 1.2 Parameters and variables in models Many biologists remain skeptical of mathematical modeling. The criticism can be summarized like this: a theoretical model either agrees with experiment, or it does not. In the former case, it is useless, because the data are already known; in the latter case, it is wrong! As I indicated above, the goal of mathematical modeling is not to reproduce experimental data; otherwise, indeed, it would only be of interest to theoreticians. The correct question to ask is, does a theoretical model help us understand the real thing? There are at least three ways in which a model can be useful: A model can help a scientist make sense of complex data, by testing whether a particular mechanism explains the observations. Thus, a model can help clarify our understanding by throwing away the non-essential features and focusing on the most important ones. A mathematical model makes predictions for situations that have not been observed. It is easy to change parameters in a mathematical model and calculate the effects. This can lead to new hypotheses that can be tested by experiments. Model predictions can lead to better experimental design. Instead of trying a whole bunch of conditions, the theoretical model can suggest which ones will produce big effects, and thus can save a lot of work for the lab scientist. In order to make a useful model of a complex living system, you have to simplify it. Even if you are only interested in a part of it, for instance a cell or a single molecule, you have to make simplifying choices. A small protein has thousands of atoms, a cell consists of millions of molecules, which all interact with each other; keeping track mathematically of every single component is daunting if not impossible. To build a useful mathematical model one must choose a few quantities which describe the system sufficiently to answer the questions of interest. For instance, if the positions of a couple of atoms in the protein you are studying determine its activity, those positions would make natural quantities to include in your model. You will find more specific examples of models later in this chapter. Once you have decided on the essential quantities to be included in the model, these are divided into variables and parameters. As suggested by the name, a variable typically varies over time and the model tracks the changes in its value, while parameters usually stay constant, or change more slowly. However, that is not always the case. The most important difference is that variables describe quantities within the system being modeled, while parameters usually refer to quantities which are controlled by something outside the system. As you can see from this definition, the same quantity can be a variable or a parameter depending on the scope of the model. Let’s go back to our example of modeling a protein: usually the activity (and the structure) of a protein is influenced by external conditions such as pH and temperature; these would be natural parameters for a model of the molecule. However, if we model an entire organism, the pH (e.g. of the blood plasma) and temperature are controlled by physiological processes within the organism, and thus these quantities will now be considered variables. Perhaps the clearest way to differentiate between variables and parameters is to think about how you would present a data set visually. We will discuss plotting graphs of functions in chapter 2, and plotting data sets in chapter 3, but the reader has likely seen many such plots before. Consider which of the quantities you would to plot to describe the system you are modeling. If the quantity belongs on either axis, it is a variable, since it is important to describe how it changes. The rest of the quantities can be called parameters. Of course, depending on the question you ask, the same quantity may be plotted on an axis or not, which is why this classification is not absolute. After we have specified the essential variables for your model, we can describe a complex and evolving biological system in terms of its state. This is a very general term, but it usually means the values of all the variables that you have chosen for the model, which are often called state variables. For instance, an ion channel can be described with the state variable of conformation, which may be in a open state or in a closed state. The range, or collection of all different states of the system is called the state space of the model. Below you will find examples of models of biological systems with diverse state spaces. 1.2.1 discrete state variables: genetics There are genes which are present in a population as two different versions, called *alleles} - let us use letters \\(A\\) and \\(B\\) to label them. One may describe the genetic state of an individual based on which allele it carries. If this individual is haploid, e.g. a bacterium, then it only carries a single copy of the genome, and its state can be described by a single variable with the state space of \\(A\\) or \\(B\\). A diploid organism, like a human, possesses two copies of each gene (unless it is on one of the sex chromosomes, X or Y); each copy may be in either state \\(A\\) or \\(B\\). This may seem to suggest that there are four different values in the genetic state space, but if the order of the copies does not matter (which is usually the case), then \\(AB\\) and \\(BA\\) are effectively the same, so the state space consists of three values: \\(AA\\), \\(BB\\), and \\(AB\\). 1.2.2 discrete state variables: population Consider the model of a population of individuals, with the variable of number of individuals (populations size) and parameters being the birth and death rates. The state space of this model is all integers between 0 and infinity. Consider the model of a population of individuals who may get infected. Assume that the total number of individuals does not change (that is, there are no births and deaths) and that these individuals can be in one of two states: healthy or sick (in epidemiology these are called susceptible or infectious). There are typically two parameters in such models: the probability of infection and the probability of recovery. Since the total population is fixed at some number \\(N\\), the space space of the model is all pairs of integers between 0 and \\(N\\) that add up to \\(N\\). 1.2.3 continuous state variables: concentration Suppose that a biological molecule is produced with a certain rate and degraded with a different rate, and we would like to describe the quantity of the molecule, usually expressed as concentration. The relevant variables here are concentration and time, and you will see those variables on the axes of many plots in biochemistry. Concentration is a ratio of the number of molecules and the volume, so the state space can be any positive real number (although practically there is a limit as to how many molecules can fit inside a given volume, but for simplicity we can ignore this). Going even further, let us consider an entire cell, which contains a large number of different molecules. We can describe the state of a cell as the collection of all the molecular concentrations, with the parameters being the rates of all the reactions going on between those molecules. The state space for this model with \\(N\\) different molecules is \\(N\\) positive real numbers. 1.2.4 multiple variables in medicine Doctors take medical history from patients and measure vital signs to get a picture of a patient’s health. These can be all be thought of as variables in a model of a person that physicians construct. Some of these variables are discrete, for instance whether there is family history of hypertension, which has only two values: yes or no. Other variables are numbers with a range, such as weight and blood pressure. The state space of this model is a combination of categorical values (such as yes/no) and numerical values (within a reasonable range). 1.2.5 Discussion questions Several biological models are indicated below. Based on what you know, divide the quantities into variables and parameters and describe the state space of the model. Note that there may be more than one correct interpretation The volume of blood pumped by the heart over a certain amount of time, depending on the heart rate and the ejection volume. The number of wolves in a national forest depending on the number of wolves in the previous year, the birth rate, the death rate, and the migration rate. The fraction of hemes in hemoglobin (a transport protein in red blood cells) which are bound to oxygen depending on the partial pressure of oxygen and the binding cooperativity of hemoglobin. The number of mutations that occur in a genome, depending on the mutation rate, the amount of time, and the length of the genome. The concentration of a drug in the blood stream depending on the dose, time after administration, and the rate of metabolism (processing) of the drug. Describing an outbreak of an infectious disease in a city in terms of the fractions of infected, healthy, and recovered people, depending on the rate of infection, rate of recovery, and the mortality rate of the disease. 1.3 First steps in R A central goal of this book is to help you, the reader, gain experience with computation, which requires learning some programming (cool kids call it “coding”). Programming is a way of interacting with computers through a symbolic language, unlike the graphic user interfaces that we’re all familiar with. Basically, programming allows you to make a computer do exactly what you want it to do. There is a vast number of computer languages with distinct functionalities and personalities. Some are made to talk directly to the computer’s “brain” (CPU and memory), e.g. Assembly, while others are better suited for human comprehension, e.g. python or Java. Programming in any language involves two parts: 1) writing a program (code) using the commands and the syntax for the language; 2) running the code by using a compiler or interpreter to translate the commands into machine language and then making the computer execute the actions. If your code has a mistake in it, the compiler or interpreter should catch it, and return an error message to you instead of executing the code. Sometimes, though, the code may pass muster with the interpreter/compiler, but it may still have a mistake (bug). This can be manifested in two different ways: either the code execution does not produce the result that you intended, or it hangs up or crashes the computer (the latter is hard to do with the kind of programming we will be doing). We will discuss errors and how to prevent and catch these bugs as you develop your programming skills. In this course, our goal is to compute mathematical models and to analyze data, so we choose a language that is designed specially for these tasks, which is called R. To proceed, you’ll need to download and install R, which is freely available here. In addition to downloading the language (which includes the interpreter that allows you to run R code on your computer) you will need to download a graphic interface for writing, editing, and running R code, called R Studio (coders call this an IDE, or an Integrated Developer Environment), which is also free and available here. 1.3.1 R Markdown and R Studio In this course you will use R using R Studio and R Markdown documents, which are text files with the extension .Rmd. Markdown is a simple formatting syntax for creating reports in HTML, PDF, or Word format by incorporating text with code and its output. More details on using R Markdown are here. In fact, this whole book is written in R Markdown files and then compiled to produce the beautiful (I hope you agree) web book that you are reading. If you open an Rmd file in R Studio, you will see a Knit button on top of the Editor window. Clicking it initiates the processing of the file into an output document (in HTML, PDF, or Word format) that includes the text as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: print(&quot;Hello there!&quot;) ## [1] &quot;Hello there!&quot; To run the code inside a single R code chunk, click the green arrow in the top right of the chunk. This will produce an output, in this case the text “Hello there!”. Inside the generated output file, for example the web book you may be reading, the output of code chunks is shown below the box with the R code and indicated by two hashtags. You can make text bold or italic like so. You can also use mathematical notation called LaTeX, which you’ll see used below to generate nice-looking equations. LaTeX commands are surrounded by dollar signs, for example $e^x$ generates \\(e^x\\). Mathematical types love LaTeX, but you can use R Markdown without it. 1.3.2 numbers and arithmetic operations When you get down to the brass tacks, all computation rests on performing arithmetic operations: addition, subtraction, multiplication, division, exponentiation, etc. The symbols used for arithmetic operations are what you’d expect: +, -, *, / are the four standard operations, and ^ is the symbol for exponentiation. For example, type 2^3 in any R code chunk and execute it: 2^3 ## [1] 8 You see that R returns the result by printing it out on the screen. The number in square brackets [1] is not important for now; it is useful when the answer contains many numbers and has to be printed out on many rows. The second number is the result of the calculation. For numbers that are either very large or very small, it’s too cumbersome to write out all the digits, so R, like most computational platforms, uses the scientific notation. For instance, if you want to represent 1.4 billion, you type in the following command; note that 10 to the ninth power is represented as e+09 and the prefix 1.4 is written without any multiplication sign: 1.4*10^9 ## [1] 1.4e+09 There are also certain numbers built into the R language, most notably \\(\\pi\\) and \\(e\\), which can be accessed as follows: pi ## [1] 3.141593 exp(1) ## [1] 2.718282 The expression exp() is an example of a function, which we will discuss in section ; it returns the value of \\(e\\) raised to the power of the number in parenthesis, hence exp(1) returns \\(e\\). Notice that although both numbers are irrational, and thus have infinitely many decimal digits, R only prints out a few of them. This doesn’t mean that it doesn’t have more digits in memory, but it only displays a limited number to avoid clutter. The number of digits to be displayed can be changed, for example to display 10 digits, type in options(digits=10). Computers are very good at computation, as their name suggests, but they have limitations. In order to manipulate numbers, they must be stored in computer memory, but computer memory is finite. There is a limit to the length of the number that is feasible to store on a computer. This has implications for both very large numbers and to very small numbers, which are close to zero, because both require many digits for storage. All programming languages have an upper limit on the biggest number it will store and work with. If an arithmetic operation results in a number larger than that limit, the computer will call it an overflow error. Depending on the language, this may stop the execution of the program, or else produce a non-numerical value, such as NaN (not a number) or Inf (infinite). Do exercise to investigate the limitations of R for large numbers. On the other hand, very small numbers present their own challenges. As with very large numbers, a computer cannot store an arbitrary number of digits after the decimal (or binary) point. Therefore, there is also the smallest number that a programming language will accept and use, and storing a smaller number produces an underflow error. This will either cause the program execution to stop, or to return the value 0 instead of the correct answer. Do exercise to investigate the limitations of R for small numbers. This last fact demonstrates that all computer operations are imprecise, as they are limited by what’s called the machine precision, which is illustrated in exercise . For instance, two similar numbers, if they are within the machine precision of one another, will be considered the same by the computer. Modern computers have large memories, and their machine precision is very good, but sometimes this error presents a problem, e.g. when subtracting two numbers. A detailed discussion of machine error is beyond the scope of this text, but anyone performing computations must be aware of its inherent limitations. 1.3.3 R Coding Exercises Calculate the value of \\(\\pi\\) raised to the 10th power. Use the scientific notation to multiply four billion by \\(\\pi\\). Use the scientific notation with large exponents (e.g. 1e+100, 1e+500, etc.) to find out what happens when you give R a number that is too large for it to handle. Approximately at what order of magnitude does R produce an overflow error? In the same fashion, find out what happens when you give R a number that is too small for it to handle. Approximately at what order of magnitude does R produce an underflow error? How close can two numbers be before R thinks they are the same? Subtract two numbers which are close to each other, like 24 and 24.001, and keep making them closer to each other, until R returns a difference of zero. Report at what value of the actual difference this happens. 1.3.4 variable assignment Variables in programming languages are used to store and access numerical or other information. After assigning} it a value for the first time (initializing), a variable name can be used to represent the value we assigned to it. Invoking the name of variable recalls the stored value from computer’s memory. There are a few rules about naming variables: a name cannot be a number or an arithmetic operator like +, in fact it cannot contain symbols for operators or spaces inside the name, or else confusion would reign. Variable names may contain numbers, but not as the first character. When writing code it is good practice to give variables informative names, like height or city_pop. The symbol ‘=’ is used to assign a value to a variable in most programming languages, and can be used in R too. However, it is customary for R to use the symbols &lt;- together to indicate assignment, like this: var1 &lt;- 5 After this command the variable var1 has the value 5, which you can see in the upper right frame in R Studio called Environment. In order to display the value of the variable as an output on the screen, use the special command print() (it’s actually a function, which we will discuss in the next chapter). The following two commands show that the value of a variable can be changed after it has been initialized: var1 &lt;- 5 var1 &lt;- 6 print(var1) ## [1] 6 While seemingly contradictory, the commands are perfectly clear to the computer: first var1 is assigned the value 5 and then it is assigned 6. After the second command, the first value is forgotten, so any operations that use the variable var1 will be using the value of 6. Entire expressions can be placed on the right hand side of an assignment command: they could be arithmetic or logical operations as well as functions, which we will discuss later on. For example, the following commands result in the value 6 being assigned to the variable var2: var1 &lt;- 5 var2 &lt;- var1+1 print(var2) ## [1] 6 Even more mind-blowing is that the same variable can be used on both sides of an assignment operator! The R interpreter first looks on the right hand side to evaluate the expression and then assigns the result to the variable name on the left hand side. So for instance, the following commands increase the value of var1 by 1, and then assign the product of var1 and var2 to the variable var2: var1 &lt;- var1 + 1 print(var1) ## [1] 6 var2 &lt;- var1-1 print(var2) ## [1] 5 var2 &lt;- var1*var2 print(var2) ## [1] 30 We have seen example of how to assign values to variables, so here is an example of how NOT to assign values, with the resulting error message: var1 + 1 &lt;- var1 The left-hand side of an assignment command should contain only the variable to which you are assigning a value, not an arithmetic expression to be performed. 1.3.5 R Coding Exercises The following commands or scripts do not work as intended. Find the errors and correct them, then run them to make sure they do what they are intended to do: 1.3.6 Exercises: The following R commands or short scripts contain errors; your job is to fix them so they runs as described. (Remove the # at the start of each line to “uncomment” the code first.) Assign the value -10 to a variable neg -&gt; -10 Assign a variable the value 5 and then increase its value by 3: 2pac &lt;- 5 2pac &lt;- 2pac + 3 Assign the values 4 and 7 to two variables, then add them together and assign the sum to a new variable: total &lt;- part1 + part2 part1 &lt;- 4 part2 &lt;- 7 Add 5 and 3 and save it into variable my.number 5 + 3 &lt;- my.number Print the value of my.number on the screen: print[my.number] Replace the value of my.number with 5 times its current value my.number &lt;- 5my.number Assign the values of 7 and 8 to variables a and b, respectively, multiply them and save the results in variable x a&lt;-7 b&lt;-8 x&lt;-ab print(x) Assign the value 42 to a variable, then increase it by 1 age &lt;- 42 age + 1 &lt;- age Assign the value 10 to variable radius, then calculate the area of the circle with that radius using the formula \\(A = \\pi r^2\\): r &lt;- 10 area &lt;- pir^2 1.4 R Assignment In the following tasks you will translate mathematical models into R commands and make the computer perform calculations. The first step is to give descriptive names to these variables (instead of single letters!), and then use these variables to calculate the predictions of the models. Take the model of a heart pumping with a constant rate, where V is the total volume of blood pumped by the heart over time, S is the stroke volume, \\(R\\) is the heart rate and \\(t\\) is time: \\[ V = SRt \\] For a heart beating at 60 beats per minute with stroke volume of 75 ml over 80 minutes, create variables for the stroke volume, heart rate, and time (come up with your own descriptive names). Calculate the total volume pumped by the heart by turning the equation into a line of R code, assigning the result to a variable, and print out the result. ANSWER: 360000 ml # THIS IS A COMMENT. YOUR CODE GOES HERE Consider an infectious disease that spreads through a new population at an exponential rate r (per day). If there are initially I infected individuals, then after t days the model predicts the total number of infected to be T: \\[ T = I(1+r)^t \\] If the initial number of infected is 20, and the rate of spread is 8% per day, calculate the total number of infected after 30 days by assigning these values to informatively named variables, turning the equation above into a line of R code, assigning the result to a variable, and print out the result. ANSWER: 201.2531 infected # THIS IS A COMMENT. YOUR CODE GOES HERE The logistic model for a population P that is limited to a certain number, called carrying capacity K (population size), can be written as follows, with t time (years), r the intrinsic growth rate (per year) and A a dimensionless constant. \\[ P= \\frac{K}{1 + A e^{-rt}} \\] Let K be 30000, r be 0.08, and A be 29, assign these values to informatively named variables. Calculate the population size after 25 years by turning the equation above into a line of R code, assigning the result to a variable, and print out the result. ANSWER: 6091.713 individuals # THIS IS A COMMENT. YOUR CODE GOES HERE "],["functions-and-their-graphs.html", "2 Functions and their graphs 2.1 Dimensions of quantities 2.2 Functions and their graphs 2.3 Vectors and plotting in R 2.4 Rates of biochemical reactions 2.5 R Assignment", " 2 Functions and their graphs Some fathers, if you ask them for the time of day, spit silver dollars. Donald Barthelme, The Dead Father Mathematical models describe how various quantities affect each other. In the last chapter we learned that these descriptions can be written down, often in the form of an equation. For instance, we can describe the total volume of blood pumped over a period of time as the product of stroke volume, the heart rate and the number of minutes, which can be written as an equation. The different quantities have their own meaning and roles, depending on what they stand for. To better describe how these quantities are related we use the deep idea of mathematical functions. In this chapter you will learn to do the following: use dimensional analysis to deduce the meaning of quantities in a model understand the concept of function, dependent and independent variables recognize basic functional forms and the shape of their graphs use R to plot functions understand basic models of reaction rates 2.1 Dimensions of quantities What distinguishes a mathematical model from a mathematical equation is that the quantities involved have a real-world meaning. Each quantity represents a measurement, and associated with each one are the units of measurement. The number 173 is not enough to describe the height of a person - you are left to wonder 173 what? meters, centimeters, nanometers, light-years? Obviously, only centimeters make sense as a unit of measurement for human height; but if we were measuring the distance between two animals in a habitat, meters would be a reasonable unit, and it were the distance between molecules in a cell, we would use nanometers. Thus, any quantity in a mathematical model must have associated units, and any graphs of these quantities must be labeled accordingly. In addition to units, each variable and parameter has a meaning, which is called the dimension of the quantity. For example, any measurement of length or distance has the same dimension, although the units may vary. The value of a quantity depends on the units of measurement, but its essential dimensionality does not. One can convert a measurement in meters to that in light-years or cubits, but one cannot convert a measurement in number of sheep to seconds - that conversion has no meaning. Thus leads us to the fundamental rule of mathematical modeling: terms that are added or subtracted must have the same dimension. This gives mathematical modelers a useful tool called dimensional analysis, which involves replacing the quantities in an equation with their dimensions. This serves as a check that all dimensions match, as well as allowing to deduce the dimensions of any parameters for which the dimension was not specified. Example. As we saw in chapter 1, the relationship between the amount blood pumped by a heart in a certain amount of time is expressed in the following equation, where \\(V_{tot}\\) and \\(V_s\\) are the total volume and stroke volume, respectively, \\(R\\) is the heart rate, and \\(t\\) is the time: \\[ V_{tot} = V_sRt \\] The dimension of a quantity \\(X\\) is denoted by \\([X]\\); for example, if \\(t\\) has the dimension of time, we write \\([t] = time\\). The dimension of volume is \\([V_{tot}] = length^3\\), the dimension of stroke volume is \\([V_s] = volume/beat\\) and the dimension of time \\(t\\) is time, so we can re-write the equation above in dimensional form: \\[length^3 = length^3/ beat \\times R \\times time\\] Solving this equation for R, we find that it must have the dimensions of \\([R] = beats/time\\). It can be measured in beats per minute (typical for heart rate), or beats per second, beats per hour, etc. but the dimensionality of the quantity cannot be changed without making the model meaningless. There are also dimensionless quantities, or pure numbers, which are not tied to a physical meaning at all. Fundamental mathematical constants, like \\(\\pi\\) or \\(e\\), are classic examples, as are some important quantities in physics, like the Reynolds number in fluid mechanics. Quantities with a dimension can be made dimensionless by dividing them by another quantity with the same dimension and “canceling” the dimensions. For instance, we can express the height of a person as a fraction of the mean height of the population; then the height of a tall person will become a number greater than 1, and the height of a short one will become less than 1. This new dimensionless height does not have units of length - they have been divided out by the mean height. This is known as rescaling the quantity, by dividing it by a preferred scale. There is a fundamental difference between rescaling and changing the units of a quantity: when changing the units, e.g. from inches to centimeters, the dimension remains the same, but if one divides the quantity by a scale, it loses its dimension. Example. The model for a population of bacteria that doubles every hour is described by the equation, where \\(P_0\\) is initial number of bacteria and \\(P\\) is the population after \\(t\\) hours: \\[ P = P_0 2^t \\] Let us define the quantity \\(R=P/P_0\\), so we can say that population increased by a factor of \\(R\\) after \\(t\\) hours. This ratio is a dimensionless quantity because \\(P\\) and \\(P_0\\) have the same dimension of bacterial population, which cancel out. The equation for \\(R\\) can be written as follows: \\[ R= 2^t \\] According to dimensional analysis, both sides of the equation have to be dimensionless, so \\(t\\) must also be a dimensionless variable. This is surprising, because \\(t\\) indicates the number of hours the bacterial colony has been growing. This reveals the subtle fact that \\(t\\) is a rescaled variable obtained by dividing the elapsed time by the length of the reproductive cycle. Because of the assumption that the bacteria divide exactly once an hour, \\(t\\) counts the number of hours, but if they divided once a day, \\(t\\) would denote the number of days. So \\(t\\) doesn’t have units or dimensions, but instead denotes the dimensionless number of cell divisions. 2.1.1 Exercises For each biological model below determine the dimensions of the parameters, based on the given dimensions of the variables. Model of number of mutations \\(M\\) as a function of time \\(t\\): \\[ M(t) = M_0 + \\mu t\\] Model of molecular concentration \\(C\\) as a function of time \\(t\\): \\[ C(t) = C_0 e^{-kt} \\] Model of tree height \\(H\\) (length) as a function of age \\(a\\) (time): \\[ H(a) = \\frac{b a}{c + a}\\] Model of cooperative binding of ligands, with fraction of bound receptors \\(\\theta\\) as a function of ligand concentration \\(L\\): \\[ \\theta (L) = \\frac{L^n}{L^n + K_d}\\] Model of concentration of a gene product \\(G\\) (concentration) as a function of time \\(t\\): \\[ G(t) = G_m (1 - e^{-\\alpha t})\\] Michaelis-Menten model of enzyme kinetics, \\(v\\) is reaction rate (1/time) and \\(S\\) is substrate concentration: \\[ v(S) = \\frac{v_{max} S}{K_m + S}\\] Logistic model of population growth, \\(P\\) is population size and time \\(t\\): \\[ P(t) = \\frac{A e^{kt}}{1 + B(e^{kt} -1)} \\] 2.2 Functions and their graphs A relationship between two variables addresses the basic question: when one variable changes, how does this affect the other? An equation, like the examples in the last section, allows one to calculate the value of one variable based on the other variable and parameter values. In this section we seek to describe more broadly how two variables are related by using the mathematical concept of functions. Definition 2.1 A function is a mathematical rule which has an input and an output. A function returns a well-defined output for every input, that is, for a given input value the function returns a unique output value. In this abstract definition of a function it doesn’t have to be written as an algebraic equation, it only has to return a unique output for any given input value. In mathematics we usually write them down in terms of algebraic expressions. As in mathematical models, you will see two different kinds of quantities in equations that define functions: variables and parameters. The input and the output of a function are usually variables, with the input called the independent variable and the output called the dependent variable. The relationship between the input and the output can be graphically illustrated in a graph, which is a collection of paired values of the independent and dependent variable drawn as a curve in the plane. Although it shows how the two variables change relative to each other, parameters may change too, which results in a different graph of the function. While graphing calculators and computers can draw graphs for you, it is very helpful to have an intuitive understanding about how a function behaves, and how the behavior depends on the parameters. Here are the three questions to help picture the relationship (assume \\(x\\) is the independent variable and it is a nonnegative real number): what is the value of the function at \\(x=0\\)? what does the function do when \\(x\\) becomes large (\\(x \\to \\infty\\))? what does the function do between the two extremes? Below you will find examples of fundamental functions used in biological models with descriptions of how their parameters influence their graphs. 2.2.1 linear and exponential functions The reader is probably familiar with linear and exponential functions from algebra courses. However, they are so commonly used that it is worth going over them to refresh your memory and perhaps to see them from another perspective. Definition 2.2 A linear function \\(f(x)\\) is one for which the difference in two function values is the same for a specific difference in the independent variable. In mathematical terms, this can be written an equation for any two values of the independent variable \\(x_1\\) and \\(x_2\\) and a difference \\(\\Delta x\\): \\[ f(x_1 + \\Delta x) - f(x_1) = f(x_2 + \\Delta x) - f(x_2) \\] The general form of the linear function is written as follows: \\[\\begin{equation} f(x) = ax + b \\label{eq:linear_funk} \\end{equation}\\] The function contains two parameters: the slope \\(a\\) and the y-intercept \\(b\\). The graph of the linear function is a line (hence the name) and the slope \\(a\\) determines its steepness. A positive slope corresponds to the graph that increases as \\(x\\) increases, and a negative slope corresponds to a declining function. At \\(x=0\\), the function equals \\(b\\), and as \\(x \\to \\infty\\), the function approaches positive infinity if \\(a&gt;0\\), and approaches negative infinity if \\(a&lt;0\\). Definition 2.3 An exponential function \\(f(x)\\) is one for which the ratio of two function values is the same for a specific difference in the independent variable. Mathematically speaking, this can be written as follows for any two values of the independent variable \\(x_1\\) and \\(x_2\\) and a difference \\(\\Delta x\\): \\[ \\frac{f(x_1 + \\Delta x)}{f(x_1)} = \\frac{f(x_2 + \\Delta x)}{f(x_2)}\\] Exponential functions can be written using different symbolic forms, but they all have a constant base with the variable \\(x\\) in the exponent. I prefer to use the constant \\(e\\) (base of the natural logarithm) as the base of all the exponential functions, for reasons that will become apparent in chapter 15. This does not restrict the range of possible functions, because any exponential function can be expressed using base \\(e\\), using a transformation: \\(a^x = e^{x \\ln(a)}\\). So let us agree to write exponential functions in the following form: \\[\\begin{equation} f(x) = a e^{rx} \\label{eq:exp_funk} \\end{equation}\\] The function contains two parameters: the \\(r\\) and the multiplicative constant \\(a\\). The graph of the exponential function is a curve which crosses the y-axis at \\(y=a\\) (plug in \\(x=0\\) to see that this is the case). As \\(x\\) increases, the behavior of the graph depends on the sign of the rate constant \\(r\\). If \\(r&gt;0\\), the function approaches infinity (positive if \\(a&gt;0\\), negative if \\(a&lt;0\\)) as \\(x \\to \\infty\\). If \\(r&lt;0\\), the function decays at an ever-decreasing pace and asymptotically approaches zero as \\(x \\to \\infty\\). Thus the graph of \\(f(x)\\) is a curve either going to infinity or a curve asymptotically approaching 0, and the steepness of the growth or decay is determined by \\(r\\). Figure 2.1: Plots of two linear functions (left) and two exponential functions (right). Can you identify which linear function has the positive slope and which one negative? Which exponential function has a positive rate constant and which one negative? 2.2.2 Exercises Answer the questions below, some of which refer to the function graphs in figure ??. Which of the linear graphs in the first figure corresponds to \\(f(x) = 5x\\) and which corresponds to \\(f(x) = 10-x\\)? State which parameter allows you to connect the function with its graph and explain why. Which of the exponential graphs in the second figure corresponds to \\(f(x) = 0.1e^{0.5x}\\) and which corresponds to \\(f(x) = 12e^{-0.2x}\\)? State which parameter allows you to connect the function with its graph and explain why. Demonstrate algebraically that a linear function of the form given in equation satisfies the property of linear functions from definition . Demonstrate algebraically that an exponential function of the form given in equation satisfies the property of exponential functions from definition . Modify the exponential function by adding a constant term to it \\(f(x) = a e^{rx} + b\\). What is is the value of this function at \\(x=0\\)? How does the function defined in the previous exercise, \\(f(x) = a e^{rx} + b\\), how does it behave as \\(x \\to \\infty\\) if \\(r&gt;0\\)? How does the function \\(f(x) = a e^{rx} + b\\) behave as \\(x \\to \\infty\\) if \\(r&lt;0\\)? 2.2.3 rational and logistic functions Let us now turn to more complex functions, made up of simpler components that we understand. Consider a ratio of two polynomials, called a rational function. The general form of such functions can be written down as follows, where ellipsis stands for terms with powers lower than \\(n\\) or \\(m\\): \\[\\begin{equation} f(x) = \\frac{a_0 + ... + a_n x^n}{b_0 + ... + b_m x^m} \\label{eq:rational_funk} \\end{equation}\\] The two polynomials may have different degrees (highest power of the terms, \\(n\\) and \\(m\\)), but they are usually the same in most biological examples. The reason is that if the numerator and the denominator are ``unbalanced’’, one will inevitably overpower the other for large values of \\(x\\), which would lead to the function either increasing without bound to infinity (if \\(n&gt;m\\)) or decaying to zero (if \\(m&gt;n\\)). There’s nothing wrong with that, mathematically, but rational functions are most frequently used to model quantities that approach a nonzero asymptote for large values of the independent variable. For this reason, let us assume \\(m=n\\) and consider what happens as \\(x \\to \\infty\\). All terms other than the highest-order terms become very small in comparison to \\(x^n\\) (this is something you can demonstrate to yourself using R), and thus both the numerator and the denominator approach the terms with power \\(n\\). This can be written using the mathematical limit notation \\(\\lim_{x \\to \\infty}\\) which describes the value that a function approaches when the independent variable increases without bound: \\[ \\lim_{x \\to \\infty} \\frac{a_0 + ... + a_n x^n}{b_0 + ... + b_n x^n} = \\frac{ a_n x^n}{ b_n x^n} = \\frac{ a_n}{ b_n} \\] Therefore, the function approaches the value of \\(a_n /b_n\\) as \\(x\\) grows. Similarly, let us consider what happens when \\(x=0\\). Plugging this into the function results in all of the terms vanishing except for the constant terms, so \\[ f(0) = \\frac{ a_0}{ b_0} \\] Between 0 and infinity, the function either increases or decreases monotonically, depending on which value (\\(a_n /b_n\\) or \\(a_0/b_0\\)) is greater. Two examples of plots of rational functions are shown in figure , which shows graphs increasing from 0 to 1. Depending on the degree of the polynomials in a rational function, it may increase more gradually (solid line) or more step-like (dashed line). The following model, called the Hill equation , describes the fraction of receptor molecules which are bound to a ligand, which is a chemical term for a free molecule that binds to another, typically larger, receptor molecule. \\(\\theta\\) is the fraction of receptors bound to a ligand, \\(L\\) denotes the ligand concentration, \\(K_d\\) is the dissociation constant, and \\(n\\) called the binding cooperativity or Hill coefficient: \\[ \\theta = \\frac{L^n}{ L^n +K_d}\\] The Hill equation is a rational function, and Figure shows plots of the graphs of two such function in the right panel. This model is further explored in exercise 2.2.10. Example. A common model of population over time is the logistic function. There are variations on how it is written down, but here is one general form: \\[\\begin{equation} f(x) = \\frac{a e^{rx} }{b+e^{rx}} \\label{eq:logistic_funk} \\end{equation}\\] The numerator and denominator both contain exponential functions with the same power. If \\(r&gt;0\\) when \\(x \\to \\infty\\), the denominator approaches \\(e^{rx}\\), since it becomes much greater than \\(b\\), and we can calculate: \\[ \\lim_{x \\to \\infty} = \\frac{a e^{rx} }{e^{rx}} = a; \\; \\mathrm{if} \\; r&gt;0 \\] On the other hand, if \\(r&lt;0\\), then the numerator approaches zero as \\(x \\to \\infty\\), and so does the function \\[ \\lim_{x \\to \\infty} = \\frac{0}{b} = 0; \\; \\mathrm{if} \\; r&lt;0 \\] Notice that switching the sign of \\(r\\) has the same effect as switching the sign of \\(x\\), since they are multiplied. Which means that for positive \\(r\\), if \\(x\\) is extended to negative infinity, the function approaches 0. This is illustrated in the second plot in Figure , which shows two logistic functions increasing from 0 to a positive level, one with \\(a=20\\) (solid line) and the second with \\(a=10\\) (dashed line). The graph of logistic functions has a characteristic sigmoidal (S-shaped) shape, and its steepness is determined by the rate \\(r\\): if \\(r\\) is small, the curve is soft, if \\(r\\) is large, the graph resembles a step function. Figure 2.2: Examples of two graphs of logistic functions (left) and two Hill functions (right). 2.2.4 Exercises: For each biological model below answer the following questions in terms of the parameters in the models, assuming all are nonnegative real numbers. 1) what is the value of the function when the independent variable is 0? 2) what value does the function approach when the independent variable goes to infinity? 3) verbally describe the behavior of the functions between 0 and infinity (e.g., function increases, decreases). Model of number of mutations \\(M\\) as a function of time \\(t\\): \\[ M(t) = M_0 + \\mu t\\] Model of molecular concentration \\(C\\) as a function of time \\(t\\): \\[ C(t) = C_0 e^{-kt} \\] Model of cooperative binding of ligands, with fraction of bound receptors \\(\\theta\\) as a function of ligand concentration \\(L\\): \\[ \\theta = \\frac{L^n}{L^n + K_d}\\] Model of tree height \\(H\\) (length) as a function of age \\(a\\) (time): \\[ H(a) = \\frac{b a }{c + a}\\] Model of concentration of a gene product \\(G\\) (concentration) as a function of time \\(t\\): \\[ G(t) = G_m (1 - e^{-\\alpha t})\\] The simplified Goldman-Hodgkin-Katz model of ionic current \\(I\\) (current) as a function of membrane potential \\(V\\) (voltage): \\[ I(V) = - b V \\frac{1- c e^{-\\alpha V}} {1- e^{-\\alpha V}} \\] Michaelis-Menten model of enzyme kinetics, \\(v\\) is reaction rate (1/time) and \\(S\\) is substrate concentration: \\[ v(S) = \\frac{v_{max} S}{K_m + S}\\] Logistic model of population growth, \\(P\\) is population size and time \\(t\\): \\[ P(t) = \\frac{A e^{kt}}{1 + B(e^{kt} -1)} \\] 2.3 Vectors and plotting in R 2.3.1 writing scripts and calling functions Programming means arranging a number of commands in a particular order to perform a task. Typing them one at a time into the command line is inefficient and error-prone. Instead, the commands are written into a file called a program or script (the name depends on the type of language; since R is a scripting language you will be writing scripts), which can be edited, saved, copied, etc. To open a new script file, in R Studio, go to File menu, and choose New R Script. This will open an editor window where you can type your commands. To save the script file (do this often!!), click the Save button (with the little floppy disk icon) or select Save from the File menu. You will also see small buttons at the top of the window that say Run, Re-run, and Source. The first two will run either the current line or a selected region of the script, while the Source button will run the entire file. Now that you know how to create a script, you should never type your R code into the command line, unless you’re testing a single command to see what it does, or looking up help. R comes equipped with many functions that correspond to standard mathematical functions. As we saw in section , exp() is the exponential function that returns \\(e\\) raised to the power of the input value. Other common ones are: sqrt() returns the square root of the input value; sin() and cos() return the sine and the cosine of the input value, respectively. Note that all of these function names are followed by parentheses, which is a hallmark of a function (in R as well as in mathematics). This indicates that the input value has to go there, for example exp(5). To compute the value of \\(e^5\\), save it into a variable called var1 and then print out the value on the screen, you can create the following script: var1 &lt;- exp(5) print(var1) ## [1] 148.4132 If you run the above code chunk in R Studio you will see two things happen: a variable named var1 appears in the Environment window (top right) with the value 148.41… and the same value is printed out in the command line window (bottom left). The most important principle of the procedural brand of programming (which includes R) is this: the computer (that is, the compiler or interpreter) evaluates the commands from top to bottom, one at a time. The variables are used with the values that they are currently assigned. If one variable (var1) was assigned in terms of another (var2), and then var2 is changed later, this does not change the value of var2. Here is an illustration of how this works: var2 &lt;- 20 var1 &lt;- var2/20 print(var2) ## [1] 20 var2 &lt;- 10 print(var1) ## [1] 1 Notice that var1 doesn’t change, because the R interpreter reads the commands one by one, and does not go back to re-evaluate the assignment for var1 after var2 is changed. Learning to think in this methodical, literal manner is crucial for developing programming skills. 2.3.2 vector variables Variables may contain more than a single number, they can also store a bunch of numbers, which is then called an array. When numbers in an array are organized as a single ordered list, this is called a vector. There are several ways of producing a vector of numbers in R. 2.3.2.1 c() function The most direct method of making a vector is to put together several values by listing them inside the function c() and assigning the output to a variable, e.g. my.vec: my.vec&lt;-c(pi,45,912.8, 0) print(my.vec) ## [1] 3.141593 45.000000 912.800000 0.000000 This variable my.vec is now a vector variable that contains four different numbers. Each of those numbers can be accessed individually by referencing its position in the vector, called the index. In the R language the the index for the first number in a vector is 1, the index for the second number is 2, etc. The index is placed in square brackets after the vector name, as follows: print(my.vec[1]) ## [1] 3.141593 print(my.vec[2]) ## [1] 45 print(my.vec[3]) ## [1] 912.8 print(my.vec[4]) ## [1] 0 2.3.2.2 the colon operator Another way to generate a sequence of numbers in a particular order is to use the colon operator, which produces a vector of integers from the first number to the last, inclusive. Here are two examples: my.vec1&lt;-1:20 print(my.vec1) ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 my.vec2&lt;-0:-20 print(my.vec2) ## [1] 0 -1 -2 -3 -4 -5 -6 -7 -8 -9 -10 -11 -12 -13 -14 -15 -16 -17 -18 ## [20] -19 -20 You can also access some but not all of the values stored in a vector simultaneously. To do this, enter a vector of positive integers inside the square brackets, either using the colon operator or using the c() function. Here are two examples, the first prints out the 4th through the 10th element of the vector my.vec1, while the second prints out the 1st, 5th, and 11th elements of the vector my.vec2: print(my.vec1[4:10]) ## [1] 4 5 6 7 8 9 10 print(my.vec2[c(1,5,11)]) ## [1] 0 -4 -10 2.3.2.3 seq() function If you want to generate a sequence of numbers with a constant difference other than 1, you’re in luck: R provides a function called seq(). It takes three inputs: the starting value, the ending value, and the step (difference between successive elements). For example, to generate a list of numbers starting at 20 up to 50, with a step size of 3, type the first command; to obtain the same sequence in reverse, use the second command: my.vec1&lt;-seq(20,50,3) print(my.vec1) ## [1] 20 23 26 29 32 35 38 41 44 47 50 my.vec2&lt;-seq(50,20,-3) print(my.vec2) ## [1] 50 47 44 41 38 35 32 29 26 23 20 2.3.2.4 rep() function Sometimes you want to create a vector of repeated values. For example, you can create a variable with 20 zeros, you can use rep() like this: zeros &lt;- rep(0,20) print(zeros) ## [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 You can repeat any value, say create a vector by repeating the number pi: pies &lt;- rep(pi,7) print(pies) ## [1] 3.141593 3.141593 3.141593 3.141593 3.141593 3.141593 3.141593 You can even repeat another vector, like the vector my.vec that was assigned above: my.vecs &lt;- rep(my.vec, 5) print(my.vecs) ## [1] 3.141593 45.000000 912.800000 0.000000 3.141593 45.000000 ## [7] 912.800000 0.000000 3.141593 45.000000 912.800000 0.000000 ## [13] 3.141593 45.000000 912.800000 0.000000 3.141593 45.000000 ## [19] 912.800000 0.000000 2.3.3 calculations with vector variables NewVec &lt;- 2*my.vec print(NewVec) ## [1] 6.283185 90.000000 1825.600000 0.000000 You can also perform calculations with multiple vector variables, but this requires extra care. R can perform any arithmetic operation with two vector variables, for instance adding two vectors results in a vector containing the sum of corresponding elements of the two vectors: my.vec1&lt;-1:5 my.vec2&lt;-0:4 print(my.vec1) ## [1] 1 2 3 4 5 print(my.vec2) ## [1] 0 1 2 3 4 sum.vec&lt;-my.vec1+my.vec2 print(sum.vec) ## [1] 1 3 5 7 9 One needs to take care that the two vectors have the same number of elements (length). If you try to operate on (e.g. add) two vectors of different lengths, R will return a warning and the result will not be what you expect: my.vec1&lt;-1:2 my.vec2&lt;-0:4 print(my.vec1) ## [1] 1 2 print(my.vec2) ## [1] 0 1 2 3 4 sum.vec&lt;-my.vec1+my.vec2 ## Warning in my.vec1 + my.vec2: longer object length is not a multiple of shorter ## object length print(sum.vec) ## [1] 1 3 3 5 5 2.3.4 Exercises The following R commands or short scripts contain errors; your job is to fix them so they runs as described. Assign a vector of three numbers to a variable: date_num &lt;- (3,8,16) Assign a range of values to a vector variable and print out the third one: the.vals &lt;- 0:10 print[the.vals(3)] Assign a range of values to a vector variable and print out the fourtieth and sixty-first values: all.the.vals &lt;- 0:100 print(all.the.vals[40,61]) Take the two vectors assigned above and assign their product to another vector: product &lt;- the.vals*all.the.vals Create a vector vec1 of ten integers and print the second and the eighth elements: vec1 &lt;- 11:20 print(vec1[2:8]) Create a vector vec1 and then multiply all of its elements by 20 and assign it to another vector: vec1&lt;-seq(-3,2,0.1) vec2 &lt;- 20vec1 Create a vector vec1, a vector vec2 and print out all the elements of the first divided by the second: vec1 &lt;- 0:5 vec2 &lt;- 3:8 print[vec1/vec2] 2.3.5 Plotting with vectors curve(x^2, 0, 10, lwd = 3, xlab = &quot;x&quot;, ylab = &quot;quadratic&quot;, cex.axis = 1.5, cex.lab = 1.5) curve(20 * exp(-0.5 * x), 0, 5, lwd = 3, xlab = &quot;x&quot;, ylab = &quot;exponential&quot;, cex.axis = 1.5, cex.lab = 1.5) Figure 2.3: Two examples of plots using curve: quadratic (\\(y=x^2\\)) and exponential (\\(y=20*e^{-0.5x}\\)) There are several ways of creating plots of mathematical functions or data R. If you want to plot a mathematical function, the simplest function is curve(). You can tell that this is a function, because it uses parentheses; the first input is an expression for the function, and the next two define the range of the independent variable over which to plot the graph. Two examples of plotting a quadratic function over the range from 0 and 5, and an exponential variation over the range of 0 to 10 are shown in figure . One can change the default look of the plot produced by curve by setting different options, which are optional inputs into the curve function, One is the line width lwd which can be increased from the default value of 1 to produce thicker curves, as demonstrated in the example above. One can add labels on the x and y axes with xlab and ylab options, respectively; note that these are strings of characters, and thus must be put in quotes to differentiate them from a variable name. There is one very important option not shown above: that of overlaying a curve on top of an existing plot, which is done by typing add=TRUE. This option takes logical (Boolean) values TRUE and FALSE, which must be typed in all caps and without quotes. 2.3.5.1 plot() function In addition to curve, one can use the function plot() in R to create two dimensional graphs from two vector-valued variables of the same length, e.g. plot(x,y). The first input variable corresponds to the independent variable (e.g. x), which is plotted on the x-axis, and the second variable corresponds to the dependent variable (e.g. y) which is plotted on the y-axis. In figure you see graphs of exponential and logistic function plotted using plot(). The following chunk creates a vector variable time, then calculates a new variable quad using time in a single operation: time &lt;- 0:10 quad &lt;- (time - 5)^2 print(time) ## [1] 0 1 2 3 4 5 6 7 8 9 10 print(quad) ## [1] 25 16 9 4 1 0 1 4 9 16 25 This chunk plots the two vector variables quad as a function of time, and adds a title to the plot plot(time, quad, main = &quot;Quadratic function of time&quot;) The default plot style in R uses circles to indicate each plotted point. To change it, you need to set the option t (type), for example, setting t='l' (the lowercase letter L) produces a continuous line connecting the individual data points. plot(time, quad, main = &quot;Quadratic function of time&quot;, type = &quot;l&quot;, xlab = &quot;time&quot;, ylab = &quot;y = f(t)&quot;) plot() is a versatile function that has many options function has many options which can be changed to determine the color, the style, and other attributes of the plot. For a full list type help(plot) in the console or type plot in the search bar of the Help pane in the bottom right window. 2.3.5.2 using lines() or points() You may also want to plot multiple graphs on the same figure. The plot() function creates a new plot window, so if you want to add another plot on top of the first one, you have to use another function. There are two ones available: lines() which produces continuous curves connecting the points, and points() which plots individual symbols at every point. Let us illustrate this by plotting two different exponential functions on one plot, and two different logistic functions on the second one, which were discussed in section . When you’ve got multiple plots on the same figure, they need to be distinct and labeled. To distinguish them, below I use the option col to specify the color of the plot, and I add a legend describing the parameters of each plot to the figure . The function has a lot of options, so if you want to understand the details, type help(legend) in the prompt or go to Help tab in the lower right frame of R Studio and type legend. x &lt;- seq(0, 10, 0.5) y &lt;- 10 + 20 * exp(-0.5 * x) plot(x, y, xlab = &quot;x&quot;, ylab = &quot;exponential&quot;, col = 1, lwd = 3) y &lt;- 10 + 20 * exp(-2 * x) lines(x, y, col = 2, lwd = 3) leg.txt = c(&quot;b=10,a=20,r=-0.5&quot;, &quot;b=10,a=20,r=-2&quot;) legend(&quot;topright&quot;, leg.txt, col = 1:2, pch = c(1, NA), lty = c(0, 1), lwd = 3) x &lt;- seq(-10, 10, 1) y &lt;- 20 * exp(0.5 * x)/(1 + exp(0.5 * x)) plot(x, y, xlab = &quot;x&quot;, ylab = &quot;logistic&quot;, col = 4, lwd = 3) y &lt;- 20 * exp(1.5 * x)/(1 + exp(1.5 * x)) lines(x, y, col = 2, lwd = 3) leg.txt = c(&quot;a=20,b=1,r=0.5&quot;, &quot;a=20,b=1,r=1.5&quot;) legend(&quot;topleft&quot;, leg.txt, col = c(4, 2), pch = c(1, NA), lty = c(0, 1), lwd = 3) Figure 2.4: Overlaying multiple plots in R: two exponential functions of the form \\(y=b+ae^{rx}\\) on the left, two logistic functions of the form \\(y= ae^{rx}/(b+e^{rx})\\) on the right. 2.3.6 Exercises The following R commands or short scripts contain errors; your job is to fix them so they runs as described. Multiply a vector by a constant and add another constant and assign the result to a vector: new.vals &lt;- 5 + 8the.vals Assign range to be a sequence of values from 0 to 100 with step of 0.1, and calculate the vector variable result as the square of the vector variable range: range &lt;- seq(0,0.1,100) result &lt;- square(range) Plot result as a function of range: plot(result, range) Plot the graph of the function \\(f(x) = (45-x)/(4x+3)\\) over the range of 0 to 100: curve((45-x)/(4x+3), 0, 100) Plot a quadratic function with specified coefficients \\(a\\), \\(b\\), \\(c\\) over a given range of independent variable \\(x\\): a&lt;-10 b&lt;- -15 c&lt;- 5 y&lt;-a*x^2+b*x+c x&lt;-seq(-0.5,2,0.01) plot(x,y,type=&#39;l&#39;) Overlay two different plots of the logistic function with different values of the parameter \\(r\\): time&lt;-0:100 a&lt;-1000 b&lt;-50 r&lt;-0.1 Population&lt;-a*exp(r*time)/(b+exp(r*time)) plot(time,Population,type=&#39;l&#39;) r&lt;-10 lines(time,Population,col=2) 2.4 Rates of biochemical reactions Living things are dynamic, they change with time, and much of mathematical modeling in biology is interested in describing these changes. Some quantities change fast and others slowly, and every dynamic quantity has a rate of change, or rate for short. Usually, the quantity that we want to track over time is the variable, and in order to describe how it changes we introduce a rate parameter. If we are describing changes over time, all rate parameters have dimensions with time in the denominator. As a simple example, the velocity of a physical object describes the change in distance over time, so its dimension is \\([v] = length/time\\). On the most fundamental level, the work of life is performed by molecules. The protein hemoglobin transports oxygen in the red blood cells, while neurotransmitter molecules like serotonin carry signals between neurons. Enzymes catalyze reactions, like those involved in oxidizing sugar and making ATP, the energy currency of life. Various molecules bind to DNA to turn genes on and off, while myosin proteins walk along actin fibers to create muscle contractions. In order to describe the activity of biological molecules, we must measure and quantify them. However, they are so small and so numerous that it is not usually practical to count individual molecules (although with modern experimental techniques it is sometimes possible). Instead, biologists describe their numbers using concentrations. Concentration has dimensions of number of molecules per volume, and the units are typically molarity, or moles (\\(\\approx 6.022*10^{23}\\) molecules) per liter. Using concentrations to describe molecule rests on the assumption that there are many molecules and they are well-mixed, or homogeneously distributed throughout the volume of interest. Molecular reactions are essential for biology, whether they happen inside a bacterial cell or in the bloodstream of a human. Reaction kinetics refers to the description of the rates, or the speed, of chemical reactions. Different reactions occur with different rates, which may be dependent on the concentration of the reactant molecule. Consider a simple reaction of molecule \\(A\\) (called the substrate) turning into molecule \\(B\\) (called the product), which is usually written by chemists with an arrow: \\[ A \\xrightarrow{k} B \\] But how fast does the reaction take place? To write down a mathematical model, we need to define the quantities involved. First, we have the concentration of the molecule \\(A\\), with dimensions of concentration. Second, we have the rate of reaction, let us call it \\(v\\), which has dimension of concentration per time (just like velocity is length per time). How are the two quantities related? 2.4.1 Constant (zeroth-order) kinetics In some circumstances, the reaction rate \\(v\\) does not depend on the concentration of the reactant molecule \\(A\\). In that case, the relationship between the rate constant \\(k\\) and the actual rate \\(v\\) is: \\[\\begin{equation} v = k \\label{eq:kinetics_0th_order} \\end{equation}\\] Dimensional analysis insists that the dimension of \\(k\\) must be the dimension of \\(v\\), or concentration/time. This is known as constant, or zero-order kinetics, and it is observed at concentrations of \\(A\\) when the reaction is at its maximum velocity: for example, ethanol metabolism by ethanol dehydrogenase in human liver cannot proceed any faster than about 1 drink per hour. 2.4.2 First-order kinetics . In other conditions, it is easy to imagine that increasing the concentration of the reactant \\(A\\) will speed up the rate of the reaction. A simple relationship of this type is linear: \\[\\begin{equation} v = kA \\label{eq:kinetics_1st_order} \\end{equation}\\] In this case, the dimension of the rate constant \\(k\\) is 1/time. This is called first-order kinetics, and it usually describes reactions when the concentration of \\(A\\) is small, and there are plenty of free enzymes to catalyze more reactions. 2.4.3 Michaelis-Menten model of enzyme kinetics However, if the concentration of the substrate molecule \\(A\\) is neither small nor large, we need to consider a more sophisticated model. An enzyme is a protein which catalyzes a biochemical reaction, and it works in two steps: first it binds the substrate, at which point it can still dissociate and float away, and then it actually catalyzes the reaction, which is usually practically irreversible (at least by this enzyme) and releases the product. The enzyme itself is not affected or spent, so it is free to catalyze more reactions. Let denote the substrate (reactant) molecule by \\(A\\), the product molecule by \\(B\\), the enzyme by \\(E\\), and the complex of substrate and enzyme \\(AE\\). The classic chemical scheme that describes these reactions is this: \\[ A + E \\underset{k_{-1}}{\\overset{k_1}{\\rightleftharpoons}} AE \\xrightarrow{k_2} E + B \\] You could write three different kinetic equations for the three different arrows in that scheme. Michaelis and Menten used the simplifying assumptions that the binding and dissociation happens much faster than the catalytic reaction, and based on this they were able to write down an approximate, but extremely useful Michaelis-Menten model of an enzymatic reaction: \\[\\begin{equation} v = \\frac{v_{max} A}{K_M+A} \\label{eq:kinetics_MM_kinetics} \\end{equation}\\] Here \\(v\\) refers to the rate of the entire catalytic process, that is, the rate of production of \\(B\\), rather than any intermediate step. Here the reaction rate depends both on the concentration of the substrate \\(A\\) and on the two constants \\(v_{max}\\), called the maximum reaction rate, and the constant \\(K_M\\), called the Michaelis constant. They both depend on the rate constants of the reaction, and \\(v_{max}\\) also depends on the concentration of the enzyme. The details of the derivation are beyond us for now, but you will see in the following exercises how this model behaves for different values of \\(A\\). %Make a box? %Chemical kinetics models are important in a variety of biological fields. The concentration of a drug in the bloodstream is described by an equation that involves the rate of application of the drug and the rate of its removal, known as drug metabolism. The concentration of a sugar, such as fructose, in a cell an be modeled as a system of differential equations that describe all the reactions in the process of glycolysis. Many important molecules exist primarily to regulate each other, for instance, the product of a gene (the protein which is produced when the gene is expressed) can be used to turn on another gene, whose product may turn off the first gene. Such networks of interactions can also be described by a bunch of kinetics equations; the growing field of systems biology studies their complex behaviors. 2.5 R Assignment Copy your script from the R assignment in chapter 1, question 1 and modify the line that assigns the time variable to instead assign it a vector from 0 to 1440 minutes (one for each minute); keep the same values for stroke volume and heart rate. Then your calculation for total volume should produce a vector of the same length as time. Print out the value of the vector after 3 hours (72 minutes), keeping in mind that it is the 73rd element of the vector. ANSWER: 324000 ml # THIS IS A COMMENT. YOUR CODE GOES HERE Plot the volume vs time using the type line with black line color and label your axes. Then change the heart rate to 120 beats per minute and re-calculate the total blood volume with the new parmamer. Overlay that plot on top of the previous one using the lines() funcion with red line color. Based on the plots, describe what kind of function is this model, and what is the graphical difference between the two graphs. # THIS IS A COMMENT. YOUR CODE GOES HERE TYPE YOUR ANSWERS HERE Copy your script from the R assignment in chapter 1, question 2 and modify the line that assigns the time variable to instead assign it a vector from 0 to 60 days (one for each day); keep the same values for all the other variables. Then your calculation for total number of infected should produce a vector of the same length as time. Print out the value of the vector after 10 days, keeping in mind that it is the 11th element of the vector. ANSWER: 43.1785 infected # THIS IS A COMMENT. YOUR CODE GOES HERE Plot the total infected population vs time using the type line with black line color and label your axes. Then change the infection rate to 10% and recalculate the infection numbers with the new parameter. Overlay that plot on top of the previous one using the lines() function with red line color. Based on the plots, report what are the approximate doubling times for the number of infectious for the two different infection rates. # THIS IS A COMMENT. YOUR CODE GOES HERE TYPE YOUR ANSWERS HERE Copy your script from the R assignment in chapter 1, question 3 and modify the line that assigns the time variable to instead assign it a vector from 0 to 100 years (one for each year); keep the same values for all the other variables. Then your calculation for the population should produce a vector of the same length as time. Print out the value of the vector after 15 years, keeping in mind that it is the 16th element of the vector. ANSWER: 3081.781 individuals # THIS IS A COMMENT. YOUR CODE GOES HERE Plot the population vs time using the type line with black line color and label your axes. Then change the parameter A to 2 and overlay that plot on top of the previous one using the lines() function with red line color. Report the difference between the two plots in terms of their initial values and their final values. # THIS IS A COMMENT. YOUR CODE GOES HERE TYPE YOUR ANSWERS HERE "],["describing-data-sets.html", "3 Describing data sets 3.1 Mutations and their rates 3.2 Describing data sets 3.3 Working with data in R 3.4 R Assignment", " 3 Describing data sets Get your facts first, and then you can distort them as much as you please. – Rudyard Kipling, An Interview with Mark Twain Science begins with experimental measurements, which are then verified by reproducing the results. But no experimental result is perfectly reproducible because all are subject to random noise, whether it is caused by unpredictable processes or is due to measurement error. Describing collections of numbers with noise is the first step to understanding the biological systems that are being measured. In this chapter you will learn to do the following: calculate means and medians of a data set calculate variances and standard deviations produce histograms and interpret them use R to plot and analyze data sets 3.1 Mutations and their rates All Earth-based lifeforms receive an inheritance from their parent(s): a string of deoxyribonucleic acids ( DNA) called the genetic sequence, or genome of an individual. The information to produce all the necessary components to build and run the organism is encoded in the sequence of the four different nucleotides: adenine, thymine, guanine, and cytosine (abbreviated as A, T, G, C). Different parts of the genome play different roles; some discrete chunks called genes contain the instructions to build proteins, the workhorses of biology. To make a protein from a gene, the information is transcribed from DNA into messenger ribonucleic acid ( mRNA), which is then translated into a string of amino acids which constitute the protein. The genetic code determines the translation, using three nucleic acids in DNA and RNA to represent a single amino acid in a protein. Thus, a sequence of DNA results in a specific sequence of amino acids, which determine the structure and function of the protein. Different types of substitution point mutations are distinguished by their effects on the gene products; image by Jonsta247 in public domain via Wikimedia Commons. The above processes involve copying and transferring information. As we know from experience, copying information inevitably means introducing errors. This is particularly important when passing information from parent to offspring, because then an entire organism has to develop and live based on a faulty blueprint. Changes introduced in the genome of an organism are called mutations, and they can be caused either by errors in copying DNA when making a new cell (replication) or through damage to DNA through physical means (e.g. ionizing radiation) or chemical mechanisms (e.g. exogenous molecules that react with DNA). The simplest mutation involve a single nucleotide and are called point mutations. A nucleotide may be deleted, an extra nucleotide inserted, or a new one substituted instead: the three different types of substitution mutations are shown in figure . Large-scale mutations may involve whole chunks of the genome that are cut out and pasted in a different location, or copied and inserted in another position, but they are typically much more rare than point mutations. Mutations can have different effects on the mutant organism, although acquisition of super-powers has not been observed. Usually, point mutations have either little observable effect or a negative effect on the health of the mutant. A classic example is sickle-cell disease, in which the molecules of the protein hemoglobin, responsible for carrying oxygen in the blood from the lungs to the tissues, tends to stick together and clump, resulting in sickle-shaped red blood cells. The disease is caused by a single substitution mutation in the gene that codes for one of the two components of hemoglobin, called \\(\\beta\\)-globin. The substitution of a single nucleotide in the DNA sequence changes one amino acid in the protein from glutamate to valine, which causes the proteins to aggregate. This missense}* mutation (see figure ) is carried by a fraction of the human population, and those who inherit the allele allele from both parents develop the painful and sometimes deadly disease. Such mutations that are present in some but not all of a population are called polymorphisms, to distinguish them from mutations that occurred in evolutionary lineages and differentiate species from each other. One of the central questions of evolutionary biology is how frequently do mutations occur? Since mutations are generally undesirable, most living things have developed ways to minimize the frequency of errors in copying DNA, and to repair DNA damage. But although mutations are rare, they occur spontaneously in all organisms because molecular processes such as copying a DNA molecule are subject to random noise arising from thermal motion. So mutations are fundamentally a random process and we need to use descriptive statistics to analyze data with inherent randomness. 3.2 Describing data sets 3.2.1 central value of a data set A data set is a collection of measurements. These measurements can come from many kinds of sources, and can represent all sorts of quantities. One big distinction is between numerical and categorical data sets. Numerical data sets contain numbers, either integers or real numbers. Some examples: number of individuals in a population, length, blood pressure, concentration. Categorical data sets may contain numbers, symbols, or words, limited to a discrete, usually small, number of values. The word categorical is used because this kind of data corresponds to categories or states of the subject of the experiment. Some examples: genomic classification of an individual on the basis of one locus (e.g. wild type or mutant), the state of an ion channel (open or closed), the stage of a cell in the cell cycle. A data set contains more than one measurement, the number of them is called the size of the data set and is usually denoted by the letter \\(n\\). To describe a data set numerically, one can use numbers called statistics (not to be confused with the branch of science of the same name). The most common statistics aim to describe the central value of the data set to represent a typical measurement. If you order all of the measurements from highest to lowest and then take the the middle value, you have found the median (if there is an even number of values, take the average between the middle two). Precisely half of the data values are less than the median and the other half are greater, so it represents the true ``middle’’ value of the measurement. Note that the median can be calculated either for numerical or categorical data, as long as the categories can be ordered in some fashion. The value that occurs most frequently in the data set is called its . For some data sets, particularly those which are symmetric, the mode coincides with the mean (see next paragraph) and the median, but for many others it is distinct. The mode is the most visual of the three statistics, as it can be picked out from the histogram plot of a data set (which is described in subsection 3.2.3) as the value corresponding to the maximum frequency. The mode can also be used for both categorical and numerical data. The average or mean of a data set is the sum of all the values divided by the number of values. It is also called the expected value (particularly in the context of probability, which we will discuss later) because it allows to simply predict the sum of a large number of measurements with a given mean, by multiplying the mean by the number. The mean can be calculated only for a numerical data set, since we cannot add non-numerical values. Definition 3.1 The mean of a data set \\(X\\), also known as the average or the arithmetic mean is usually indicated with a bar over the variable symbol, and defined as the sum of the values divided by the number of values: \\[\\begin{equation} \\bar X = \\frac{1}{n} \\sum_{i=1}^n x_i \\label{eq:ch3_mean_def} \\end{equation}\\] The mean, unlike the median, is not the middle value of the data set, instead it represents the center of mass of the measured values . Another way of thinking of the mean is as a weighted sum of the values in the data set. The weights represent the frequency of occurrence of each numeric value in the data set, which we will further discuss in subsection 3.2.3. The mean is the most frequently used statistic, but it is not always interpreted correctly. Very commonly the mean is reported as the most representative value of a data set, but that is often misleading. Here are at least two situations in which the mean can be tricky: 1) data sets with a small number of discrete values; 2) data sets with outliers, or isolated numbers very far from the mean. Examples of misleading means. Mean quantities for data sets with a few quantities are not the typical value, such as in the number of children born in a year per individual, also known as the birth rate. The birth rate per year in 2013 for both the United States and Russia is 1.3% per person, but you will have to look for a long time to find any individual who gave birth to 1.3% of a child. While this point may be obvious, it is often overlooked when interpreting mean values. Outliers are another source of trouble for means. For example, a single individual (let’s call him or her B.G.) with a wealth of $50 billion moves into a town of 1000 households with average wealth of $100,000. Although none of the original residents’ assets have changed, the mean wealth of the town improves dramatically, as you can calculate in one of the exercises at the end of the chapter. One can site the improved per capita (per individual) in the town as evidence of economic growth, but that is obviously misleading. In cases with such dramatic outliers, the median is more informative as representation of a typical value of the data set. 3.2.2 Exercises For the (small) data sets given below, calculate the mean and the median (by hand or using a calculator) and compare the two measures of the center. Data set of the population of the city of Chicago (in millions) in the last 4 census years (2010, 2000, 1990, 1980): {2.7, 2.9, 2.8, 3.0}. Data set of the numbers of the fish blacknose dace (Rhinichthys atratulus) collected in 6 different streams in the Rock Creek watershed in Maryland: {76, 102, 12, 55, 93, 98}. Data set of tuberculosis incidence rates (per 100,000 people) in the 5 largest metropolitan areas in the US in 2012: {5.2, 6.6, 3.2, 5.5, 4.5}. Data set of ages of mothers at birth for five individuals: {19, 20, 22, 32, 39}. Data set of ages of fathers at birth for five individuals: {22, 23, 25, 36, 40}. Data set of the number of new mutations found on maternal chromosomes for five individuals: {9, 10, 11, 26, 15}. Data set of the number of new mutations found on paternal chromosomes for five individuals: {39, 43, 51, 53, 91}. Consider the hypothetical town with 1000 households with mean and median wealth of $100,000 and one person with assets for $50 billion. Calculate the mean value of the combined data set, and compare it to the new median value. Suppose you’d like to add a new observation to a data set; e.g. the 6-th largest metropolitan area (Philadelphia) to the tuberculosis incidence data set, which is 3.0. Calculate the mean of the 6-values data set, without using the 5 values in the original data set, but only using the mean of the 5-value data set and the new value. Generalize this to calculating the sample mean for any \\(n\\)-value data set, given the mean of the \\(n-1\\) values, plus one new value. 3.2.3 spread of a data set The center of a data set is obviously important, but so is the spread around the center. Sometimes the spread is caused by noise or error, for example in a data set of repeated measurements of the same variable under the same conditions. Other times the variance is due to real changes in the system, or due to inherent randomness of the system, and the size of the spread, as well as the shape of the histogram are important for understanding the mechanism. The simplest way to describe the spread of a numerical data set is to look at the difference between the maximum and minimum values, called the range. However, it is obviously influenced by outliers, since the extreme values are used. To describe the typical spread, we need to use all the values in the data set, and see how far each one is from the center, measured by the mean. There is a problem with the naive approach: if we just add up all the differences of data values from the mean, the positives will cancel the negatives, and we’ll get an artificially low spread. One way to correct this is to take the absolute value of the differences before adding them up. However, for somewhat deep mathematical reasons, the standard measure of spread uses not absolute values, but squares of the differences, and then divides that sum not by the number of data points \\(n\\) but by \\(n-1\\). Definition 3.2 The variance of a data set \\(X\\) with \\(n\\) values is the sum of the squared differences of each value of the variable from the mean, divided by \\(n-1\\): \\[\\begin{equation} Var(X) = \\frac{1}{n-1} \\sum_{i=1}^n (\\bar X - x_i)^2 \\end{equation}\\] The variance is a sum of square differences, so its dimension is the square of the dimensions of the measurements in \\(X\\). In order to obtain a measure of the spread comparable to the values of \\(X\\), we take the square root of variance and call it the standard deviation of the data set \\(X\\): \\[\\begin{equation} \\sigma(X) = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (\\bar X - x_i)^2} \\end{equation}\\] Just as the mean is a weighted average of all of the values in the data set, the variance is a weighted average of all the squared deviations of the data from the mean. 3.2.4 Exercises: For the (small) data sets below, calculate the range, variance, and standard deviation (by hand or using a calculator). Compare the range and the standard deviation for each case: which one is larger? by how much? Data set of the population of the city of Chicago (in millions) in the last 4 census years (2010, 2000, 1990, 1980): {2.7, 2.9, 2.8, 3.0}. Data set of the numbers of the fish blacknose dace (Rhinichthys atratulus) collected in 6 different streams in the Rock Creek watershed in Maryland: {76, 102, 12, 55, 93, 98}. Data set of tuberculosis incidence rates (per 100,000 people) in the 5 largest metropolitan areas in the US in 2012: {5.2, 6.6, 3.2, 5.5, 4.5}. Data set of ages of mothers at birth for five individuals: {19, 20, 22, 32, 39}. Data set of ages of fathers at birth for five individuals: {22, 23, 25, 36, 40}. Data set of the number of new mutations found on maternal chromosomes for five individuals: {9, 10, 11, 26, 15}. Data set of the number of new mutations found on paternal chromosomes for five individuals: {39, 43, 51, 53, 91}. Consider the hypothetical town with 1000 households with mean and median wealth of $100,000 and one person with assets for $50 billion. Calculate the mean value of the combined data set, and compare it to the new median value. (harder) Suppose that a data set has a fixed range (e.g. all values have to lie between 0 and 1). What is the greatest possible standard deviation for any data set within the range? Hint: think about how to place the points as far from the mean as possible. How do the data sets above relate to your prediction?} 3.2.5 describing data sets in graphs Data sets can be presented visually to indicate the frequency of different values. This can be done in a number of ways, depending on the kind of data set. For a data set with only a few values, e.g. a categorical data set, a good way to represent it is with a pie chart. Each category is represented by a slice of the pie with the area of the same share of the pie as the fraction of the data set in the category. There is some evidence, however, that pie charts can be misleading to the eye, so R does not recommend using them. For a numerical data set it is useful to plot the frequencies of a range of values, which is called a histogram. Its independent axis has the values of the data variable, and the dependent axis has the frequency of those values. If the data set consists of real numbers that range across an interval, that interval is divided into subintervals (usually of equal size), called bins, and the number of measurements in each bin is indicated on the y-axis. In order to be visually informative, there should be a reasonable number (usually no more than a few dozen, although it varies) of bins. The most frequent measurements are represented as the highest bars or points on the histogram. Histograms can denote either the counts of measurements in each bin, or to show the fraction of the total number of measurements in each bin. The only difference between those two kinds of histogram is the scale of the y-axis, and, confusingly, both can be called frequencies. Figure 3.1: Length of bacteria Bacillus subtilis measured under the microscope as discrete values with step of 0.5; data from citep{watkins-intro-stats} A histogram of the measured lengths of the bacterium Bacillus subtilis is shown in figure . The data set was measured in increments in half a micron, with numbers varying between 1.5 and 4.5 microns. The histogram shows that the most common measurement (the mode) is 2 \\(\\mu m\\). Adding up all of the frequencies in the histogram tells us that there are approximately 200 total values in the data set. This allows us to find the median value by counting the frequencies of the first few bins until we get to 100 (the median point), which resides in the bin for 2.5 \\(\\mu m\\). It is a little bit more difficult to estimate the mean, but it should be clear that the center of mass of the histogram is also near 2.5 (it is actually 2.49). Finally, the hardest task is estimating the spread of the data set, such as the the standard deviation, based on the histogram. The range of the data set is \\(4.5-1.5 = 3\\), so we know for sure that it is less than 1.5. The histogram shows that the deviations from the mean value of 2.5 range from 2 (rarely) to 0.5 (most prevalent). This should give you an idea that the weighted average of the deviations is less than 1. Indeed, the correct standard deviation is about 0.67. There are different ways of plotting data sets that have more than one variable. For instance, a data set measured over time is called a time series. If the values are plotted with the corresponding times on the x-axis, then it is called a time plot. This is useful to show the changes of the values of your variable over time. If the data set doesn’t undergo any significant changes over time, it makes more sense to represent it as a pie chart or histogram. More generally, one may plot two variables measured together on a single plot, which is called a scatterplot. We will explore such plots and the relationships between two measured variables in chapter 4. 3.2.6 Exercises Answer the following questions, based on the histograms in figure (mutation data) and in figure (heart rate data). How many people in the mutation data have fathers either younger than 20 or older than 40? How many have more than 80 new mutations? Estimate the median and mean of the two variables in the mutation data set. State the range of each data set, and estimate the standard deviation of the two variables in the mutation data set. How many people in the heart rate data have heart rates greater than 80 bpm? How many have body temperature less that 97 F? Estimate the median and mean of the two variables in the heart rate data set. State the range of each data set, and estimate the standard deviation of the two variables in the heart rate data set. my_data &lt;- read.table(&quot;data/HR_temp.txt&quot;, header = TRUE) hist(my_data$HR, col = &quot;gray&quot;, main = &quot;Heart rate data&quot;, xlab = &quot;heart rate (bpm)&quot;) hist(my_data$Temp, col = &quot;gray&quot;, main = &quot;Body temperature data&quot;, xlab = &quot;temperature (F)&quot;) Figure 3.2: Histograms of heart rates and body temperatures Figure 3.3: Histograms of paternal ages and the number of new mutations from 73 families; data from citep{kong_rate_2012} 3.3 Working with data in R 3.3.1 reading in data into data frames One way to input data into R is to read in a text file, where several variables are stored in columns. For instance, the file HR_temp.txt contains three variables: body temperature (in Fahrenheit), sex (1 for male, 2 for female), and heart rate (in beats per minute). The values for the variables are arranged in columns, while first row of the file contains the names of the variables (Temp, Sex, and HR, respectively). Note that the data file has to be saved into the same folder as the .Rmd file week1.Rmd for this to work. vitals &lt;- read.table(file = &quot;data/HR_temp.txt&quot;, header = TRUE) plot(vitals$HR, vitals$Temp, main = &quot;Body temp as function of heart rate&quot;, xlab = &quot;heart rate (bpm)&quot;, ylab = &quot;body temperature (F)&quot;) mTemp &lt;- mean(vitals$Temp) sdTemp &lt;- sd(vitals$Temp) abline(mTemp, 0) mean(vitals$HR) ## [1] 73.76154 sd(vitals$HR) ## [1] 7.062077 The R command read.table() reads this file and and puts it into a data frame called data. The three variables are stored inside the data frame, and can be accessed by appending the dollar sign and variable name to the data frame, so data$HR refers to only the heart rates, and data$Temp refers to the body temperatures. The plot shows the relationship of the two data variables, and the function abline(98.6,0) plots a line with the intercept 98.6a and slope 0 on top of the scatterplot. You can also load data from a package, e.g. HistData, which contains many classic data sets. Got to the Packages tab in the lower right window in R Studio, click Install and type HistData. We will use the data set called Galton that contains the heights of parents (the mean of mother’s and father’s) and their children, in variables parent and child. The script below plots the two variables, with parent as the independent (explanatory) variable and child as the dependent (response) variable. library(HistData) plot(Galton$parent, Galton$child, main = &quot;Height of child vs average height of parents&quot;, xlab = &quot;parent height (inches)&quot;, ylab = &quot;child height (inches)&quot;) summary(Galton$parent) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 64.00 67.50 68.50 68.31 69.50 73.00 summary(Galton$child) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 61.70 66.20 68.20 68.09 70.20 73.70 3.3.2 descriptive statistics One can also calculate basic descriptive statistics as follows: paste(&quot;The mean parental height is:&quot;, mean(Galton$parent)) ## [1] &quot;The mean parental height is: 68.3081896551724&quot; paste(&quot;The mean child height is:&quot;, mean(Galton$child)) ## [1] &quot;The mean child height is: 68.0884698275862&quot; paste(&quot;The standard deviation of parental height is:&quot;, sd(Galton$parent)) ## [1] &quot;The standard deviation of parental height is: 1.78733340172202&quot; paste(&quot;The standard deviation of child height is:&quot;, sd(Galton$child)) ## [1] &quot;The standard deviation of child height is: 2.51794136627677&quot; Why do you think the standard deviation of parental height is much smaller? R has histogram function hist(), which does a passable job of representing the distribution of a variable such as child height or parent height. Compare the width of the two distributions and consider why they are different. hist(Galton$child) hist(Galton$parent) 3.3.3 Exercises: The following code chunks contain errors. Find and fix them so they work as intended. Calculate the mean and standard deviation of the heart rates of the first 30 individuals in the data frame vitals: mean(vitals$HR[30]) ## [1] 64 sd(vitals$HR[30]) ## [1] NA Calculate the mean and standard deviation of the ratio of heart rates to body temperatures for the data set vitals: mean(vitals$HR/Temp) sd(vitals$HR/Temp) Plot a scatterplot of the child heights as the response variable and the parent heights and the explanatory variable, and overlay the line y=x on top. plot(parent, child, main = &#39;Height of child vs average height of parents&#39;, xlab= &#39;child height (inches)&#39;, ylab= &#39;parent height (inches)&#39;) abline(1,0) Calculate the median of both parent and child heights: median(Galton) Plot the histogram of parent heights of the first half of the group: hist(Galton$parent/2) Plot the histogram for the ratio of parent and child heights for the entire data set and calculate its mean and variance: hist(Galton$parent/child) mean(Galton$parent/child) sd(Galton$parent/child) 3.4 R Assignment In this section you will read in three different data files and assign each data frame a descriptive name, e.g. Alcohol, and then plot and fit relationship between the variables within each data frame. Your task will be to choose the parameters slope and intercept that produce a plot that does a decent job of resembling the data, meaning that the line should pass through the scatterplot and capture its trend (rising or falling) fairly well. Read in the mutations data set found in the file kong_mutation_data.txt, using the option header=TRUE. It contains three variables: PatAge (paternal age in years), MatAge (maternal age in years), and Mutations (number of de novo mutations). Print the mean and standard deviation of each variable, and generate histograms of all three variables. Plot Mutations as a function PatAge using circles (the default type). Assign values to slope and intercept variables, calculate a vector of predicted values for Mutation, and use lines() to add a plot of the predicted values to the data plot. Try different values of the slope and intercept until you find ones that fit the data reasonably well. Calculate the sum (use the sum() function) of squared differences between the vector Mutation and the vector of predicted values and print it out. # THIS IS A COMMENT. YOUR CODE GOES HERE Make a plot of Mutations vs MatAge using circles (the default type). Assign values to slope and intercept variables, calculate a vector of predicted values for Mutation, and use lines() to add a plot of the predicted values to the data plot. Try different values of the slope and intercept until you find ones that fit the data reasonably well. Calculate the sum (use the sum() function) of squared differences between the vector Mutation and the vector of predicted values and print it out. # THIS IS A COMMENT. YOUR CODE GOES HERE Read in the data set of heart rates before and after exercis found in the file HR_class.txt with the option header=TRUE. It contains variables Rest1 (heart rate at rest in bpm), Rest2 (heart rate at rest in bpm), Ex1 (heart rate after exercise in bpm), Ex2 (heart rate after exercise in bpm). Print the mean and standard deviation of each variable, and generate histograms of all four variables. # THIS IS A COMMENT. YOUR CODE GOES HERE Plot Ex1 as a function of Rest1 using circles (the default type). Assign values to slope and intercept variables, calculate a vector of predicted values for Ex1, and use lines() to add a plot of the predicted values to the data plot. Try different values of the slope and intercept until you find ones that fit the data reasonably well. Calculate the sum (use the sum() function) of squared differences between the vector Ex1 and the vector of predicted values and print it out. # THIS IS A COMMENT. YOUR CODE GOES HERE Plot Rest2 as a function of Rest1 using circles (the default type). Assign values to slope and intercept variables, calculate a vector of predicted values for Rest2, and use lines() to add a plot of the predicted values to the data plot. Try different values of the slope and intercept until you find ones that fit the data reasonably well. Calculate the sum (use the sum() function) of squared differences between the vector Rest2 and the vector of predicted values and print it out. # THIS IS A COMMENT. YOUR CODE GOES HERE Read in the data for the concentration of alcohol in blood plasma after administration of alcohol in the file Alcohol_data.txt with the option header=TRUE. It contains variables Conc (blood alcohol concentration in micrograms/mL) and the variable Time (in hours after consumption). Plot Conc as a function of Time using circles (the default type). Assign values to slope and intercept variables, calculate a vector of predicted values for Conc, and use lines() to add a plot of the predicted values to the data plot. Try different values of the slope and intercept until you find ones that fit the data reasonably well. Calculate the sum (use the sum() function) of squared differences between the vector Conc and the vector of predicted values and print it out. # THIS IS A COMMENT. YOUR CODE GOES HERE The variable Conc rises and falls, so it would be better to fit it using two separate linear functions. Assign two different variables for slopes and two different for intercepts of two linear functions, and calculate two vectors of predicted values, by splitting up the vector Time into the rising and falling subsets. Reminder: use a range of indices (e.g. 1:5) to create a subset of consecutive elements of a vector. Overlay a plot of two linear functions over the data plot, one to match the rise and the second the decay (they should intercept roughly at the peak of the concentration.) Calculate the sums of squared differences between the two subsets of vector Conc and the two vectors of predicted values and print the sum of the two sums. # THIS IS A COMMENT. YOUR CODE GOES HERE YOUR ANSWERS GO HERE "],["linear-regression.html", "4 Linear regression 4.1 Linear relationship between two variables 4.2 Linear least-squares fitting 4.3 Linear regression using R 4.4 Regression to the mean 4.5 R Assignment", " 4 Linear regression The place in which I’ll fit will not exist until I make it. James Baldwin In the last two chapters we learned to use data sets which fall into a few categories. We now turn to data which can be measured as a range of numerical values. We can ask a similar question of numerical data that we asked of categorical: how can we tell whether two variables are related? And if they are, what kind of relationship is it? This takes us into the realm of data fitting, raising two related questions: what is the best mathematical relationship to describe a data set? and what is the quality of the fit? You will learn to do the following in this chapter: define the quality of the fit between a line and a two-variable data set calculate the parameters for the best-fit line based on statistics of the data set use R to calculate and plot best-fit line for a data set understand the meaning of correlation and covariance understand the phenomenon of regression to the mean 4.1 Linear relationship between two variables Although there is always error in any real data, there may be a relationship between the two variables that is not random: for example, when one goes up, the other one tends to go up as well. These relationships may be complicated, but in this chapter we will focus on the the simplest and most common type of relationship: linear, where a change in one variable is associated with a proportional change in the other, plus an added constant. This is expressed mathematically using the familiar equation for a linear function, with parameters slope (\\(m\\)) and intercept (\\(b\\)): \\[ y = mx + b\\] Let us say you have measured some data for two variables, which we will call, unimaginatively, \\(x\\) and \\(y\\). This data set consists of pairs of numbers: one for \\(x\\), one for \\(y\\), for example, the heart rate and body temperature of a person go together. They cannot be mixed up between different people, as the data will lose all meaning. We can denote this a list of \\(n\\) pairs of numbers: \\((x_i, y_i)\\) (where \\(i\\) is an integer between 1 and \\(n\\)). Since this is a list of pairs of numbers, we can plot them as separate points in the plane using each \\(x_i\\) as the x-coordinate and each \\(y_i\\) as the y-coordinate. This is called a scatterplot of a two-variable data set. For example, two scatterplots of a data set of heart rate and body temperature are shown in figure . In the first one, the body temperature is on the x-axis, which makes it the explanatory variable; in the second one, the body temperature is on the y-axis, which makes it the response variable. data &lt;- read.table(&quot;data/HR_temp.txt&quot;, header = TRUE) plot(data$Temp, data$HR, main = &quot;heart rates vs. body temps&quot;, cex = 1.5, cex.axis = 1.5, cex.lab = 1.5) plot(data$HR, data$Temp, main = &quot;body temps vs. heart rates&quot;, cex = 1.5, cex.axis = 1.5, cex.lab = 1.5) Figure 4.1: Scatterplot of heart rates and body temperatures: a) with heart rate as the explanatory variable; b) with body temperature as the explanatory variable. 4.2 Linear least-squares fitting 4.2.1 sum of squared errors It is easy to find the best-fit line for a data set with only two points: its slope and intercept can be found by solving the two simultaneous linear equations, e.g. if the data set consists of \\((3,2.3), (6, 1.7)\\), then finding the best fit values of \\(m\\) and \\(b\\) means solving the following two equations: \\[\\begin{eqnarray*} 3m + b &amp;=&amp; 2.3 \\\\ 6m + b &amp;=&amp; 1.7 \\end{eqnarray*}\\] These equations have a unique solution for each unknown: \\(m=-0.2\\) and \\(b=2.9\\) (you can solve it using basic algebra). However, a data set with two points is very small and cannot serve as a reasonable guide for finding a relationship between two variables. Let us add one more data point, to increase our sample size to three: \\((3,2.3), (6, 1.7), (9, 1.3)\\). How do you find the best fit slope and intercept? take two points and find a line, that is the slope and the intercept, that passes through the two. It should be clear why this is a bad idea: we are arbitrarily ignoring some of the data, while perfectly fitting two points. So how do we use all the data? Let us write down the equations that a line with slope \\(m\\) and intercept \\(b\\) have to satisfy in order to fit our data points: \\[\\begin{eqnarray*} 3m + b &amp;=&amp; 2.3 \\\\ 6m + b &amp;=&amp; 1.7 \\\\ 9m + b &amp;=&amp; 1.3 \\end{eqnarray*}\\] This system has no exact solution, since there are three equations and only two unknowns. We need to find \\(m\\) and \\(b\\) such that they are a best fit to the data, not the perfect solution. To do that, we need to define what we mean by the goodness of fit. One simple way to asses how close the fit is to the data is to subtract the predicted values of \\(y\\) from the data, as follows: \\(e_i = y_i - (mx_i + b)\\). The values \\(e_i\\) are called the errors or residuals of the linear fit. If the values predicted by the linear model (\\(mx_i+b\\)) are close to the actual data \\(y_i\\), then the error will be small. However, if we add it all up, the errors with opposite signs will cancel each other, giving the impression of a good fit simply if the deviations are symmetric. A more reasonable approach is to take absolute values of the deviations before adding them up. This is called the total deviation, for \\(n\\) data points with a line fit: \\[ TD = \\sum_{i=1}^n | y_i - mx_i - b | \\] Mathematically, a better measure of total error is a sum of squared errors, which also has the advantage of adding up non-negative values, but is known as a better measure of the distance between the fit and the data (think of Euclidean distance, which is also a sum of squares) : \\[ SSE = \\sum_{i=1}^n ( y_i - mx_i - b )^2 \\] Thus we have formulated the goal of fitting the best line to a two-variable data set, also known as linear regression: find the values of slope and intercept that result in the lowest possible sum of squared errors. There is a mathematical recipe which produces these values, which will be described in the next section. Any model begins with assumptions and in order for linear regression to be a faithful representation of a data set, the following must be true: the variables have a linear relationship all of the measurements are independent of each other there is no noise in the measurements of the explanatory variable the noise in the measurements of the response variable is normally distributed with mean 0 and identical standard deviation The reasons why these assumptions are necessary for linear regression to work are beyond the scope of the text, and they are elucidated very well in the book Numerical Recipes . However, it is important to be aware of them because if they are violated, the resulting linear fit may be meaningless. It’s fairly clear that if the first assumption is violated, you are trying to impose a linear relationship on something that is actually curvy. The second assumption of independence is very important and often overlooked. The mathematical reasons for it have to do with properly measuring the goodness of fit, but intuitively it is because measurements that are linked can introduce a new relationship that has to do with the measurements, rather than the relationship between the variables. Violation of this assumption can seriously damage the reliability of the linear regression. The third assumption is often ignored, since usually the explanatory variable is also measured and thus has some noise. The reason for it is that the measure of goodness of fit is based only on the response variable, and there is no consideration of the noise in the explanatory variable. However, a reasonable amount of noise in the explanatory variable is not catastrophic for linear regression. Finally, the last assumption is due to the statistics of maximum-likelihood estimation of the slope and intercept, but again some deviation from perfect normality (bell-shaped distribution) of the noise, or slightly different variation in the noise is to be expected. 4.2.2 best-fit slope and intercept Definition 4.1 The covariance of a data set of pairs of values \\((X,Y)\\) is the sum of the products of the corresponding deviations from their respective means: \\[ Cov(X,Y) = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar X) (y_i - \\bar Y) \\] Intuitively, this means that if two variable tend to deviate in the same direction from their respective means, they have a positive covariance, and if they tend to deviate in opposite directions from their means, they have a negative covariance. In the intermediate case, if sometimes they deviate together and other times they deviate in opposition, the covariance is small or zero. For instance, the covariance between two independent random variables is zero, as we saw in section . It should come as no surprise that the slope of the linear regression depends on the covariance, that is, the degree to which the two variables deviate together from their means. If the covariance is positive, then for larger values of \\(x\\) the corresponding \\(y\\) values tend to be larger, which means the slope of the line is positive. Conversely, if the covariance is negative, so is the slope of the line. And if the two variables are independent, the slope has to be close to zero. The actual formula for the slope of the linear regression is : \\[\\begin{equation} m = \\frac{Cov(X,Y)}{Var(X)} \\end{equation}\\] I will not provide a proof that this slope generates the minimal sum of squared errors, but that is indeed the case. To find the intercept of the linear regression, we make use of one other property of the best fit line: in order for it to minimize the SSE, it must pass through the point \\((\\bar X, \\bar Y)\\). Again, I will not prove this, but note that the point of the two mean values is the central point of the ``cloud’’ of points in the scatterplot, and if the line missed that central point, the deviations will be larger. Assuming that is the case, we have the following equation for the line: \\(\\bar Y = m\\bar X + b\\), which we can solve for the intercept \\(b\\): \\[\\begin{equation} b = \\bar Y - \\frac{Cov(X,Y) \\bar X}{Var(X)} \\end{equation}\\] 4.2.3 Execises Body leanness (B) and heat loss rate (H) in boys; partial data set from B(\\(m^2/kg\\)) H(\\(^\\circ C /min)\\) 7.0 0.103 5.0 0.091 3.6 0.014 3.3 0.024 2.4 0.031 2.1 0.006 Use the data set in table to answer the following questions: Compute the means and standard deviations of each variable. Compute the covariance between the two variables. Calculate the slope and intercept of the linear regression for the data with \\(B\\) as the explanatory variable. Make a scatterplot of the data set with \\(B\\) as the explanatory variable and sketch the linear regression line with the parameters you computed. Calculate the slope and intercept of the linear regression the data with \\(H\\) as the explanatory variable. Make a scatterplot of the data set, with \\(H\\) as the explanatory variable and sketch the linear regression line with the parameters you computed. 4.2.4 correlation and goodness of fit} The correlation between two random variables is a measure of how much variation in one corresponds to variation in the other. If this sounds very similar to the description of covariance, it’s because they are closely related. Essentially, correlation is a normalized covariance, restricted to lie between -1 and 1. Here is the definition: Definition 4.2 The (linear) correlation of a dataset of pairs of data values (X,Y) is: \\[ r = \\frac{Cov(X,Y)}{\\sqrt{{Var(X)}{Var(Y)}}} = \\frac{Cov(X,Y)}{\\sigma_X \\sigma_Y} \\] If the two variables are identical, \\(X=Y\\), then the covariance becomes its variance \\(Cov(X,Y) = Var(X)\\) and the denominator also becomes the variance, and the correlation is 1. This is also true if \\(X\\) and \\(Y\\) are scalar multiples of each other, as you can see by plugging in \\(X= cY\\) into the covariance formula. The opposite case if \\(X\\) and \\(Y\\) are diametrically opposite, \\(X = -cY\\), which has the correlation coefficient of -1. All other cases fall in the middle, neither perfect correlation nor perfect anti-correlation. The special case if the two variables are independent, and thus their covariance is zero, has the correlation coefficient of 0. This gives a connection between correlation and slope of linear regression: \\[\\begin{equation} m = r \\frac{\\sigma_Y}{\\sigma_X} \\label{eq:slope_corr} \\end{equation}\\] Whenever linear regression is reported, one always sees the values of correlation \\(r\\) and squared correlation \\(r^2\\) displayed. The reason for this is that \\(r^2\\) has a very clear meaning of the the fraction of the variance of the dependent variable \\(Y\\) explained by the linear regression \\(Y=mX+b\\). Let us unpack what this means. According to the stated assumptions of linear regression, the response variable \\(Y\\) is assumed to be linear relationship with the explanatory variable \\(X\\), but with independent additive noise (also normally distributed, but it doesn’t play a role for this argument). Linear regression captures the linear relationship, and the remaining error (residuals) represent the noise. Thus, each value of \\(Y\\) can be written as \\(Y = R + \\hat Y\\) where \\(R\\) is the residual (noise) and the value predicted by the linear regression is \\(\\hat Y =mX+b\\). The assumption that \\(R\\) is independent of \\(Y\\) means that \\(Var(Y) = Var (\\hat Y) + Var (R)\\) because variance is additive for independent random variables, as we discussed in section . By the same reasoning \\(Cov(X,\\hat Y + R) = Cov(X,\\hat Y) + Cov(X,R)\\). These two covariances can be simplified further: \\(Cov(X,R) = 0\\) because \\(R\\) is independent random noise. \\(X\\) and the predicted \\(\\hat Y\\) are perfectly correlated, so \\(Cov(X,\\hat Y) = Cov(X,mX+b) = Var(X) = Var(\\hat Y)\\). This leads to the derivation of the meaning of \\(r^2\\): \\[\\begin{equation} \\begin{aligned} r^2 = \\frac{Cov(X,Y)^2}{Var(X) Var(Y)} &amp;= \\frac{(Cov(X,\\hat Y) + Cov(X,R) )^2}{Var(X) Var(Y)} = \\\\ =\\frac{Var(X)Var(\\hat Y)}{Var(X) Var(Y)} &amp;= \\frac{Var(\\hat Y)}{Var(Y)} \\end{aligned} \\label{eq:ch8_frac_var} \\end{equation}\\] One should be cautious when interpreting results of a linear regression. First, just because there is no linear relationship does not mean that there is no other relationship. Figure shows some examples of scatterplots and their corresponding correlation coefficients. What it shows is that while a formless blob of a scatterplot will certainly have zero correlation, so will other scatterplots in which there is a definite relationship (e.g. a circle, or a X-shape). The point is that correlation is always a measure of the linear relationship between variables. The second caution is well known, as that is the danger of equating correlation with a causal relationship. There are numerous examples of scientists misinterpreting a coincidental correlation as meaningful, or deeming two variables that have a common source as causing one another. For example, one can look at the increase in automobile ownership in the last century and the concurrent improvement in longevity and conclude that automobiles are good for human health. It is well-documented, however, that a sedentary lifestyle and automobile exhaust do not make a person healthy. Instead, increased prosperity has increased both the purchasing power of individuals and enabled advances in medicine that have increase our lifespans. To summarize, one must be careful when interpreting correlation: a weak one does not mean there is no relationship, and a strong one does not mean that one variable causes the changes in the other. There is another important measure of the quality of linear regression: the residual plot. The residuals are the differences between the predicted values of the response variable and the actual value from the data. As stated above, linear regression assumes that there is a linear relationship between the two variables, plus some uncorrelated noise added to the values of the response variable. If that were true, then the plot of the residuals would look like a vaguely spherical blob, with a mean value of 0 and no discernible trend (e.g. no increase of residual for larger \\(x\\) values). Visually assessing residual plots is an essential check on whether linear regression is a reasonable fit to the data in addition to the \\(r^2\\) value. 4.2.5 Exercises Figure shows scatterplots of the rate of oxygen consumption (VO) and heart rate (HR) measured in two macaroni penguins running on a treadmill (really). The authors performed linear regression on the data and found the following parameters: \\(VO =0.23HR - 11.62\\) (penguin A) and \\(VO =0.25HR - 20.93\\) (penguin B). The datasets have the standard deviations: \\(\\sigma_{VO} = 6.77\\) and \\(\\sigma_{HR} = 28.8\\) (penguin A) and \\(\\sigma_{VO} = 8.49\\) and \\(\\sigma_{HR} = 30.6\\) (penguin B). Find the dimensions and units of the slope and the intercept of the linear regression for this data (the units of HR and VO are on the plot). Data set B has a larger slope than data set A. Does this mean the correlation is higher in data set B than in A? Explain. Calculate the correlation coefficients for the linear regressions of the two penguins; explain how much variance is explained in each case. Re-calculate the slopes of the two linear regressions if the explanatory and response variables were reversed. Does changing the order of variable affect the correlation? 4.3 Linear regression using R We now have the tools to compute the parameters of the best-fit line, provided we can calculate the means, variances, and covariance of the two variable data set. Of course, the best way to do all this is to let a computer handle it. The function for calculating linear regression in R is lm(), which outputs a bunch of information to a variable called myfit in the script below. The slope, intercept, and other parameters can be printed out using the summary() function. In the script below you see a bunch of information, but we are concerned with the ones in the first column correspond to the best fit intercept (-166.2847) and the slope (2.4432). You can check that they correspond to our formulas by computing the covariance, the variances, and the means of the two variables: my_data &lt;- read.table(&quot;data/HR_temp.txt&quot;, header = TRUE) myfit &lt;- lm(HR ~ Temp, my_data) summary(myfit) ## ## Call: ## lm(formula = HR ~ Temp, data = my_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -16.6413 -4.6356 0.3247 4.8304 15.8474 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -166.2847 80.9123 -2.055 0.04190 * ## Temp 2.4432 0.8235 2.967 0.00359 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.858 on 128 degrees of freedom ## Multiple R-squared: 0.06434, Adjusted R-squared: 0.05703 ## F-statistic: 8.802 on 1 and 128 DF, p-value: 0.003591 m &lt;- cov(my_data$HR, my_data$Temp)/var(my_data$Temp) print(m) ## [1] 2.443238 b &lt;- mean(my_data$HR) - m * mean(my_data$Temp) print(b) ## [1] -166.2847 Here Temp and HR are the explanatory and response variables, respectively, and my_data is the name of the data frame they are stored in. The best fit parameters are stored in myfit, and the line can be plotted using abline(myfit). The script below shows how to calculate a linear regression line and then plot it over a scatterplot in R, and the result is shown in figure a. plot(my_data$Temp, my_data$HR, main = &quot;scatterplot and linear regression line&quot;, cex = 1.5, cex.axis = 1.5, cex.lab = 1.5) abline(myfit) HRresiduals &lt;- resid(myfit) plot(data$Temp, HRresiduals, main = &quot;residuals plot&quot;, cex = 1.5, cex.axis = 1.5, cex.lab = 1.5) abline(0, 0) Figure 4.2: Linear regression for a data set of heart rates and body temperatures (a); and the residuals (b). However, what does this mean about the quality of the fit? Just because we found a line to draw through a scatterplot does not mean that this line is meaningful. In fact, looking at the plot, there does not seem to be much of a relationship between the two variables. There are various statistical measures for the significance of linear regression, the most important one relies on the correlation between the two data sets. Look again at the summary statistics for the data set of heart rates and temperatures. There are several different statistics here, and the one that we care about is the \\(r^2\\), which is reported here as ‘Multiple R-squared’. This number tells us that the linear regression accounts for only about 6% of the total variance of the heart rate. In other words, there is no significant linear relationship in this data set. As mentioned in section , the other important check is plotting the residuals of the data set, after the linear fit is subtracted. You see the result in figure b, showing that the residuals do not have any pronounced pattern. So it is reasonable to conclude that linear regression was a reasonable model to which to fit the data. The low correlation is because data seem to have little to no relationship, not because there is some complicated nonlinear relationship. Here is an example of a linear regression performed and the line plotted over the basic R plot. Note that lm() uses the following syntax to indicate which variable is which: lm(Y ~ X) (where Y is the response variable and X is the explanatory variable.) myfit &lt;- lm(child ~ parent, Galton) summary(myfit) ## ## Call: ## lm(formula = child ~ parent, data = Galton) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.8050 -1.3661 0.0487 1.6339 5.9264 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 23.94153 2.81088 8.517 &lt;2e-16 *** ## parent 0.64629 0.04114 15.711 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.239 on 926 degrees of freedom ## Multiple R-squared: 0.2105, Adjusted R-squared: 0.2096 ## F-statistic: 246.8 on 1 and 926 DF, p-value: &lt; 2.2e-16 print(paste(&quot;The best-fit slope is: &quot;, myfit$coefficients[2])) ## [1] &quot;The best-fit slope is: 0.646290581993716&quot; print(paste(&quot;The best-fit intercept is: &quot;, myfit$coefficients[1])) ## [1] &quot;The best-fit intercept is: 23.9415301804085&quot; The summary outputs a whole bunch of information that is returned by the lm() function, as the object myfit. The most important are the intercept and slope, which may be printed out as shown above, and the R-squared parameter, also called the coefficient of determination. The value of R-squared is not accessible directly in myfit, but it is printed out in the summary (use multiple R-squared for our assignments.) The actual best-fit line can be plotted as follows over a scatterplot of the data; notice that abline can take myfit as an input and use the slope and intercept: #Overlay the best-fit line on the base R plot plot(Galton$parent, Galton$child, xlab=&#39;mid-parent height (inches)&#39;, ylab=&#39;child height (inches)&#39;) abline(myfit) After performing linear regression it is essential to check that the residuals obey the assumptions of linear regression. The residuals are the difference between the predicted response variable values and the actual values of the response variable, in this case the child height. The residuals are contained in the object myfit as variable residuals: plot(Galton$parent, myfit$residuals, xlab=&#39;mid-parent height (inches)&#39;, ylab=&#39;residuals (inches)&#39;) abline(0,0) It appears that the residuals meet the assumptions of being independent of measurement (shapeless scatterplot), are centered at zero, and look roughly normally distributed, although that can be checked more carefully using other tools. 4.3.1 Exercises: Calculate descriptive statistics (mean and standard deviation) of the residuals from the linear regression above. What do you expect them to be, and how do they differ from the expectation? Using this calculation, check that the coefficient of determination really captures the fraction of total variance explained by linear regression Perform linear regression on the Galton data set with the response and explanatory variables switched, and report which parameters changed and how. Plot the residuals from your new linear regression and calculate and report their descriptive statistics. What do you expect them to be, and how do they differ from the expectation? Using this calculation, check that the coefficient of determination really captures the fraction of total variance explained by linear regression. 4.4 Regression to the mean The phenomenon called regression to the mean is initially surprising. Francis Galton first discovered this by comparing the heights of parents and their offspring. Galton took a subset of parents who are taller than average and observed that their children were, on average, shorter than their parents. He also compared the heights of parents who are shorter than average, and found that their children were on average taller than their parents. This suggests the conclusion that in long run everyone will converge closer to the average height - hence “regression to mediocrity”, as Galton called it . library(&quot;HistData&quot;) myfit &lt;- lm(child ~ parent, Galton) plot(Galton$parent, Galton$child, xlab = &quot;mid-parent height (inches)&quot;, ylab = &quot;child height (inches)&quot;, cex = 1.5, cex.axis = 1.5, cex.lab = 1.5) abline(myfit, lwd = 3, lty = 1) abline(0, 1, lwd = 3, lty = 2, col = &quot;red&quot;) myfit &lt;- lm(parent ~ child, Galton) plot(Galton$child, Galton$parent, xlab = &quot;child height (inches)&quot;, ylab = &quot;mid-parent height (inches)&quot;, cex = 1.5, cex.axis = 1.5, cex.lab = 1.5) abline(myfit, lwd = 3) abline(0, 1, lwd = 3, lty = 2, col = &quot;red&quot;) Figure 4.3: Galton data on heights of parents and of children as scatterplots (two versions with explanatory and response variables switched). The dotted red lines show the identity line y=x and the solid black line is the linear regression. But that is not the case! The parents and children in Galton’s experiment had a very similar mean and standard deviation. This appears to be a paradox, but it is easily explained using linear regression. Consider two identically distributed random variables \\((X,Y)\\) with a positive correlation \\(r\\). The slope of the linear regression is \\(m = r \\sigma_Y/\\sigma_X\\) and since \\(\\sigma_Y=\\sigma_X\\), the slope is simply \\(r\\). Select a subset with values of \\(X\\) higher than \\(\\bar X\\), and consider the mean value of \\(Y\\) for that subset. If the slope \\(m&lt;1\\) (the correlation is not perfect), then the mean value of \\(Y\\) for that subset is less than the mean value of \\(X\\). Similarly, for a subset with values of \\(X\\) lower than \\(\\bar X\\), the mean value of \\(Y\\) for that subset is greater than the mean value of \\(X\\), again as long as the slope is less than 1. Figure shows Galton’s data set (available in R by installing the package ‘HistData’) along with the linear regression line and the identity like (\\(y=x\\)). If each child had exactly the same height as the parents, the scatterplot would lie on the identity line. Instead, the linear regression lines have slope less than 1 for both the plot with the parental heights as the explanatory variable and for the plot with the variables reversed. The correlation coefficient \\(r\\) does not depend on the order of the variables; so using the equation we can see the difference in slopes is explained by the two data sets having different standard deviations, and reversing the explanatory and response variables results in reciprocation of the ratio of standard deviations. The children’s heights have a higher standard deviation, which is likely an artifact of the experiment. In the data set the heights of the two parents were averaged to take them both into account, which substantially reduces the spread between male and female heights. To summarize, although the children of taller parents are shorter on average than their parents, and the children of shorter parents are taller than their parents, the overall standard deviation does not decrease from generation to generation. 4.4.1 Discussion questions Please read the paper on measuring the rate of de novo mutations in humans and its relationship to paternal age . What types of mutations were observed in the data set? What were the most and the least common? The paper shows that both maternal and paternal age are positively correlated with offspring inheriting new mutations. What biological mechanism explains why paternal age is the dominant factor? What could explain the substantial correlation with maternal age? Is linear regression the best representation of the relationship between paternal age and number of mutations? What other model did the authors use to fit the data, and how did it perform? What do you make of the historical data of paternal ages the authors present at the end of the paper? Can you postulate a testable hypothesis based on this observation? 4.5 R Assignment In each of the following questions, do these tasks: calculate the linear regression on the indicated variables in the data set plot the scatterplot and overlay the linear regression line plot the residuals as a function of the explanatory variable answer questions about interpretation of linear regression Use the data from the file kong_mutation_data.txt and perform the tasks stated above for Mutations as a function of PatAge (remember to label your plots and axes!) # YOUR CODE HERE How does the slope compare to the fit your performed by eye in question 1.1a? What does the slope of that line mean? What fraction of the variance is explained by the linear relationship? Does the plot of residuals show any significant deviations from the assumptions? YOUR ANSWERS GO HERE Perform the same tasks as above for Mutations as a function of MatAge (remember to label your plots and axes!) # YOUR CODE HERE How does the slope compare to the fit your performed by eye in question 1.1c? What does the slope of that line tell you? What fraction of the variance is explained by the linear relationship? Is the number of de novo mutations explained better by maternal age or paternal age? Does the plot of residuals show any significant deviations from the assumptions? YOUR ANSWERS GO HERE Use the data from the file HR_class.txtand perform the tasks stated above for variables Ex1 as a function of Rest1 (remember to label your plots and axes!) # YOUR CODE HERE How does the slope compare to the fit your performed by eye in question 1.2b? What does the slope of that line tell you? What fraction of the variance is explained by the linear relationship? Does the plot of residuals show any significant deviations from the assumptions? YOUR ANSWERS GO HERE Perform the tasks stated above for variables Rest2 as a function of Rest1 (remember to label your plots and axes!) # YOUR CODE HERE How does the slope compare to the fit your performed by eye in question 1.2c? What does the slope of that line tell you? What fraction of the variance is explained by the linear relationship? Does the plot of residuals show any significant deviations from the assumptions? YOUR ANSWERS GO HERE Use the data from the file Alcohol_data.txt and perform the tasks stated above for variables Conc as a function of Time (remember to label your plots and axes!) # YOUR CODE HERE How does the slope compare to the fit your performed by eye in question 1.3a? What does the slope of that line tell you? What fraction of the variance is explained by the linear relationship? Does the plot of residuals show any significant deviations from the assumptions? YOUR ANSWERS GO HERE As you did in question 1.3b, divide the variables Time and Conc into two subsets, first for the rising portion and second for the falling portion. Make a new plot of the data, compute two linear regressions for the two subsets of the variables (Conc and Time) and plot the two best-fit lines on the same plot. You don’t need to plot the residuals, but it’s a fun challenge if you want to do it! # YOUR CODE HERE "],["linear-difference-equations.html", "5 Linear difference equations 5.1 Discrete time population models 5.2 Solutions of linear difference models 5.3 Numerical solutions in R 5.4 R Assignment", " 5 Linear difference equations We’re captive on the carousel of time We can’t return we can only look behind From where we came And go round and round and round In the circle game – Joni Mitchell, The Circle Game All living things change over time, and this evolution can be quantitatively measured and analyzed. Mathematics makes use of equations to define models that change with time, known as dynamical systems. In this unit we will learn how to construct models that describe the time-dependent behavior of some measurable quantity in life sciences. Numerous fields of biology use such models, and in particular we will consider changes in population size, the progress of biochemical reactions, the spread of infectious disease, and the spikes of membrane potentials in neurons, as some of the main examples of biological dynamical systems. Many processes in living things happen regularly, repeating with a fairly constant time period. One common example is the reproductive cycle in species that reproduce periodically, whether once a year, or once an hour, like certain bacteria that divide at a relatively constant rate under favorable conditions. Other periodic phenomena include circadian (daily) cycles in physiology, contractions of the heart muscle, and waves of neural activity. For these processes, theoretical biologists use models with discrete time, in which the time variable is restricted to the integers. For instance, it is natural to count the generations in whole numbers when modeling population growth. This chapter will be devoted to analyzing dynamical systems in which time is measured in discrete steps. In this chapter you will learn to do the following: write down discrete-time (difference) equations based on stated assumptions find analytic solutions of linear difference equations use for loops in R compute numeric solutions of difference equations 5.1 Discrete time population models Let us construct our first models of biological systems. We will start by considering a population of some species, with the goal of tracking its growth or decay over time. The variable of interest is the number of individuals in the population, which we will call \\(N\\). This is called the dependent variable, since its value changes depending on time; it would make no sense to say that time changes depending on the population size. Throughout the study of dynamical systems, we will denote the independent variable of time by \\(t\\). To denote the population size at time \\(t\\), we can write \\(N(t)\\) but sometimes use \\(N_t\\). 5.1.1 static population In order to describe the dynamics, we need to write down a rule for how the population changes. Consider the simplest case, in which the population stays the same for all time. (Maybe it is a pile of rocks?) Then the following equation describes this situation: \\[N(t+1) = N(t) \\] This equation mandates that the population at the next time step be the same as at the present time \\(t\\). This type of equation is generally called a difference equation, because it can be written as a difference between the values at the two different times: \\[N(t+1) - N(t) = 0\\] This version of the model illustrates that a difference equation at its core describes the increments of \\(N\\) from one time step to the next. In this case, the increments are always 0, which makes it plain that the population does not change from one time step to the next. 5.1.2 exponential population growth Let us consider a more interesting situation: as a colony of dividing bacteria, such as E. coli, shown in figure . We will assume that each bacterial cell divides and produces two daughter cells at fixed intervals of time, and let us further suppose that bacteria never die. Essentially, we are assuming a population of immortal bacteria with clocks. This means that after each cell division the population size doubles. As before, we denote the number of cells in each generation by \\(N(t)\\), and obtain the equation describing each successive generation: \\[ N(t+1) = 2N(t)\\] It can also be written in the difference form, as above: \\[ N(t+1) - N(t) = N(t) \\] The increment in population size is determined by the current population size, so the population in this model is forever growing. This type of behavior is termed exponential growth, which we will investigate further in section . Scanning electron micrograph of a dividing Escherichia coli bacteria; image by Evangeline Sowers, Janice Haney Carr (CDC) in public domain via Wikimedia Commons. 5.1.3 population with births and deaths Suppose that a type of fish lives to reproduce only once after a period of maturation, after which the adults die. In this simple scenario, half of the population is female, a female always lays 1000 eggs, and of those, 1% survive to maturity and reproduce. Let us set up the model for the population growth of this idealized fish population. The general idea, as before, is to relate the population size at the next time step \\(N(t+1)\\) to the population at the present time \\(N(t)\\). Let us tabulate both the increases and the decreases in the population size. We have \\(N(t)\\) fish at the present time, but we know they all die after reproducing, so there is a decrease of \\(N(t)\\) in the population. Since half of the population is female, the number of new offspring produced by \\(N(t)\\) fish is \\(500N(t)\\). Of those, only 1% survive to maturity (the next time step), and the other 99% (\\(495N(t)\\)) die. We can add all the terms together to obtain the following difference equation: \\[ N(t+1) = N(t) - N(t) + 500N(t) - 495 N(t) = 5N(t) \\] The number 500 in the expression is the birth rate of the population per individual, and the negative terms add up to the death rate of 496 per individual. We can re-write the equation in difference form: \\[ N(t+1) - N(t) = 4N(t) \\] This expression again generates growth in the population, because the birth rate outweighs the death rate. 5.1.4 dimensions of birth and death rates As we discussed in section the dimensions of quantities in a model have to satisfy the rules of dimensional analysis we discussed in chapter 2. In the case of population models, the birth and death rates measure the number of individuals that are born (or die) within a reproductive cycle for every individual at the present time. Their dimensions must be such that the terms in the equation all match: \\[ [N(t+1) - N(t)] = [population] = [r] [N(t)] = [r] \\times [population] \\] This implies that \\(r\\) is algebraically dimensionless. However, the meaning of \\(r\\) is the rate of change of population over one (generation) time step. \\(r\\) is the birth or death rate of the population per generation, and therefore, when such rates are measured, they are reported with units of inverse time (e.g. number of offspring per year). 5.1.5 linear demographic models We will now write a general difference equation for any population with constant birth and death rates. This will allow us to substitute arbitrary values of the birth and death rates to model different biological situations. Suppose that a population has the birth rate of \\(b\\) per individual, and the death rate \\(d\\) per individual. Then the general model of the population size is: \\[\\begin{equation} N(t+1) = (1 + b - d)N(t) \\label{linear_pop} \\end{equation}\\] The general equation also allows us to check the dimensions of birth and death rates, especially as written in the incremental form: $ N(t+1) - N(t) = (b - d)N(t)$. The change in population rate over one reproductive cycle is given by the current population size multiplied by the difference of birth and death rates, which as we saw are algebraically dimensionless. The right hand side of the equation has the dimensions of population size, matching the difference on the left hand side. 5.2 Solutions of linear difference models 5.2.1 simple linear models Having set up the difference equation models, we would naturally like to solve them to find out how the dependent variable, such as population size, varies over time. A solution may be analytic, meaning that it can be written as a formula, or numerical, in which case it is generated by a computer in the form of a sequence of values of the dependent variable over a period of time. In this section, we will find some simple analytic solutions and learn to analyze the behavior of difference equations which we cannot solve exactly. (#def:diff_eqn) A function \\(N(t)\\) is a (over some time period \\(a &lt; t &lt; b\\)) of a difference equation \\(N(t+1) = f(N(t))\\) if it satisfies that equation (over some time period \\(a &lt; t &lt; b\\)). For instance, let us take our first model of the static population, \\(N(t+1) = N(t)\\). Any constant function is a solution, for example, \\(N(t) = 0\\), or \\(N(t) = 10\\). There are actually as many solutions as there are numbers, that is, infinitely many! In order to specify exactly what happens in the model, we need to specify the size of the population at some point, usually, at the “beginning of time”, \\(t = 0\\). This is called the initial condition for the model, and for a well-behaved difference equation it is enough to determine a unique solution. For the static model, specifying the initial condition is the same as specifying the population size for all time. Now let us look at the general model of population growth with constant birth and death rates. We saw in equation above that these can be written in the form \\(N(t+1) = (1 + b - d) N(t)\\). To simplify, let us combine the numbers into one growth parameter \\(r = 1 + b - d\\), and write down the general equation for population growth with constant growth rate: \\[N(t+1) = rN(t)\\] To find the solution, consider a specific example, where we start with the initial population size \\(N_0 = 1\\), and the growth rate \\(r=2\\). The sequence of population sizes is: 1, 2, 4, 8, 16, etc. This is described by the formula \\(N(t) = 2^t\\). In the general case, each time step the solution is multiplied by \\(r\\), so the solution has the same exponential form. The initial condition \\(N_0\\) is a multiplicative constant in the solution, and one can verify that when \\(t=0\\), the solution matches the initial value: \\[\\begin{equation} N(t) = r^t N_0 \\label{eq:lin_discrete_sol} \\end{equation}\\] I would like the reader to pause and consider this remarkable formula. No matter what the birth and death parameters are selected, this solution predicts the population size at any point in time \\(t\\). In order to verify that the formula for \\(N(t)\\) is actually a solution in the meaning of definition , we need to check that it actually satisfies the difference equation for all \\(t\\), not just a few time steps. This can be done algebraically by plugging in \\(N(t+1)\\) into the left side of the dynamic model and \\(N(t)\\) into the right side and checking whether they match. For \\(N(t)\\) given by equation , \\(N(t+1) = r^{t+1} N_0\\), and thus the dynamic model becomes: \\[r^{t+1} N_0 = r \\times r^t N_0\\] Since the two sides match, this means the solution is correct. 5.2.2 models with a constant term Now let us consider a dynamic model that combines two different rates: a proportional rate (\\(rN\\)) and a constant rate which does not depend on the value of the variable \\(N\\). We can write such a generic model as follows: \\[ N(t+1) = rN(t) + a \\] The right-hand-side of this equation is a linear function of \\(N\\), so this is a linear difference equation with a constant term. What function \\(N(t)\\) satisfies it? One can quickly check that that the same solution \\(N(t) = r^t N_0\\) does not work because of the pesky constant term \\(a\\): \\[ r^{t+1} N_0 \\neq r \\times r^t N_0 + a\\] To solve it, we need to try a different form: specifically, an exponential with an added constant. The exponential can be reasonably surmised to have base \\(r\\) as before, and then leave the two constants as unknown: \\(N(t) = c_1 r^t + c_2\\). To figure out whether this is a solution, plug it into the linear difference equation above and check whether a choice of constants can make the two sides agree: \\[ N(t+1) = c_1 r^{t +1} + c_2 = rN(t) + a = rc_1 r^t + rc_2+ a\\] This equation has the same term \\(c_1 r^{t +1}\\) on both sides, so they can be subtracted out. The remaining equation involves only \\(c_2\\), and its solution is \\(c_2 = a/(1-r)\\). Therefore, the general solution of this linear difference equation is the following expression, which is determined from the initial value by plugging \\(t=0\\) and solving for \\(c\\). \\[\\begin{equation} N(t) = c r^t + \\frac{a}{1-r} \\label{eq:ch14_sol_wconst} \\end{equation}\\] Example. Take the difference equation \\(N(t+1) = 0.5 N(t) + 40\\) with initial value \\(N(0)= 100\\). The solution, according to our formula is \\(N(t) = c 0.5^t + 80\\). At \\(N(0) = 100 = c+80\\), so \\(c=20\\). Then the compete solution is \\(N(t) = 20 \\times 0.5^t + 80\\). To check that this actually works, plug this solution back into the difference equation: \\[ N(t+1) = 20 \\times 0.5^{t+1} + 80 = 0.5 \\times (20 \\times 0.5^t + 80) + 40 = 20 \\times 0.5^{t+1} + 80\\] The equation is satisfied and therefore the solution is correct. 5.2.3 population growth and decline The parameter \\(r\\) can assume different values, depending on the birth and death rates. If the birth rate is greater than the death rate, \\(r &gt; 1\\), and if it is the other way around, \\(r &lt; 1\\). Note that for a realistic biological population, the death rate is limited by the number of individuals present in the population. The maximum number of individuals at any time is \\(N(t)+bN(t)\\), so this means that \\(d \\leq b +1\\). Therefore, for a biological population, \\(r \\geq 0\\). x &lt;- 0:6 y0 &lt;- 5 plot(x, y0 * 2^x, t = &quot;b&quot;, lwd = 3, cex.axis = 1.5, cex.lab = 1.5, xlab = &quot;time&quot;, ylab = &quot;population&quot;) y0 &lt;- 1 lines(x, y0 * 2^x, t = &quot;b&quot;, lwd = 3, col = 2) leg.txt &lt;- c(&quot;N(0)=5&quot;, &quot;N(0)=1&quot;) legend(&quot;topleft&quot;, leg.txt, cex = 1.5, col = c(1, 2), pch = 1, lty = 1, lwd = 3) y0 &lt;- 1000 plot(x, y0 * 0.5^x, t = &quot;b&quot;, lwd = 3, cex.axis = 1.5, cex.lab = 1.5, xlab = &quot;time&quot;, ylab = &quot;population&quot;) y0 &lt;- 500 lines(x, y0 * 0.5^x, t = &quot;b&quot;, lwd = 3, col = 2) leg.txt &lt;- c(&quot;N(0)=1000&quot;, &quot;N(0)=500&quot;) legend(&quot;topright&quot;, leg.txt, cex = 1.5, col = c(1, 2), pch = 1, lty = 1, lwd = 3) Figure 5.1: Plots of solutions of linear difference equations: a) \\(N(t+1) = 2N(t)\\) with different initial values; b) \\(N(t+1) = 0.5N(t)\\) with different initial values The solutions in formula and are exponential functions, which as we saw in section have a limited menu of behaviors, depending on the value of \\(r\\). If \\(r &gt; 1\\), multiplication by \\(r\\) increases the size of the population, so the solution \\(N(t)\\) will grow. If \\(r &lt; 1\\), multiplication by \\(r\\) decreases the size of the population, so the solution \\(N(t)\\) will decay (see figure ). Finally, if \\(r=1\\), multiplication by \\(r\\) leaves the population size unchanged, like in the pile of rocks model. Here is the complete classification of the behavior of population models with constant birth and death rates (assuming \\(r&gt;0\\)): \\(r &gt; 1\\): \\(N(t)\\) grows without bound \\(r &lt; 1\\): \\(N(t)\\) decays to the constant \\(a/(1-r)\\) \\(r = 1\\): \\(N(t)\\) remains constant As we see, there are only two options for solution of linear difference equations: ever-faster growth or decay to zero or another constant value. The exponential growth of populations is also known as after the early population modeler Thomas Malthus. He used a simple population model with constant growth rate to predict demographic disaster due to the exponentially increasing population outstripping the growth in food production. In fact, human population has not been growing with a constant birth rate, and food production has (so far) kept up pace with population size, illustrating yet again that mathematical models are only as good as the assumptions that underlie them. 5.2.4 Exercises For the following scenarios for a population: 1) Construct a dynamic model by writing down a difference equation both in the updating function form (\\(N(t+1) = f(N(t))\\)) and the increment form (\\(N(t+1) - N(t) = g(N(t))\\)) and specify the time step; 2) find the solution of the linear difference equation with a generic initial value and check that it satisfies the difference equation by plugging the solution into your equation (in either form); 3) plug in the given initial value and predict the future. Zombies have appeared in Chicago. Every day, each zombie produces 3 new zombies. Suppose that initially there is only one zombie, how many zombies will there be in 7 days? Suppose hunters kill 50 deer in a national forest every hunting season, while the deer by themselves have equal birth and death rates. If there are initially 500 deer in the forest, predict how many there will be in 5 years. The number of infected people in a population grows by 8% per day and those who become infected remain infected. If initially there are 8 infected, how many individuals will be infected in 45 days? Suppose bacteria in a population divide in 2 every hour and 90% of the current population dies after reproduction, not including the new offspring. If the population initially has 10 million bacteria, predict how many there will be in 12 hours. In a rabbit population, each pair produces 2.2 offspring every year (i.e. 1.1 per capita) and the adults have a 0.9 annual death rate after reproduction (and assume the whole population is paired up into mating pairs.) If initially there are 55 rabbits, predict how many there will be in 5 years. Consider the same rabbit population, but now a python that lives nearby eats exactly 1 rabbit a month. If initially there are 55 rabbits, how many do you predict will be in 10 years? 10 fish are added to an aquarium every month, while 80% of those present survive every month and there is no reproduction. If initially there are 10 fish in the aquarium, predict how many there will be in 2 years. A Mardi Gras parade marches down the street, and every block 50 new revelers join the parade, while 20% of parade leaves for various reasons. Suppose the parade starts with 12 people, predict how many there will be after 30 blocks. Twenty flies fly into a house every day, and half of the flies in the house at beginning of each day find their way out (no births or deaths happen in the house.) If there are 10 files in the house initially, how many will there be in 12 days? Every day someone eats 10 pieces of candy from a bowl, while someone else adds half as many pieces as are currently in the bowl (before 10 pieces are removed.) If there are currently 20 pieces of candy in the bowl, how many will there be in 7 days? 5.3 Numerical solutions in R 5.3.1 for loops Often times one has to write a script to perform the same (or similar) task many times. Like all programming languages, R has commands that call for repetition of the same commands multiple times. These structures are called . In this section you will learn about the for loop, which looks like this: for (i in 1:10) { body of the loop } For example, the following script will print out “Hello!” ten times like this: for (i in 1:10) { print(&quot;Hello!&quot;) } ## [1] &quot;Hello!&quot; ## [1] &quot;Hello!&quot; ## [1] &quot;Hello!&quot; ## [1] &quot;Hello!&quot; ## [1] &quot;Hello!&quot; ## [1] &quot;Hello!&quot; ## [1] &quot;Hello!&quot; ## [1] &quot;Hello!&quot; ## [1] &quot;Hello!&quot; ## [1] &quot;Hello!&quot; The for loop starts with the keyword for, then has the expression in parentheses (i in 1:10). i is called the loop variable, in is another keyword, and 1:10 is a loop vector. After the first line there is a curly bracket { and everything that follows until the closing bracket } is the loop body. The loop body is executed as many times as there are elements in the loop vector (in this example, 10 times) and the only thing that changes is the values of the loop variable i, which starts with the first element of the vector (1) and goes until it reaches the last element (10) and then stops: for (i in 1:10) { print(paste(&quot;Iteration number&quot;, i)) } ## [1] &quot;Iteration number 1&quot; ## [1] &quot;Iteration number 2&quot; ## [1] &quot;Iteration number 3&quot; ## [1] &quot;Iteration number 4&quot; ## [1] &quot;Iteration number 5&quot; ## [1] &quot;Iteration number 6&quot; ## [1] &quot;Iteration number 7&quot; ## [1] &quot;Iteration number 8&quot; ## [1] &quot;Iteration number 9&quot; ## [1] &quot;Iteration number 10&quot; It can be used to do repetitive calculations, for example, adding up all the integers in an array (of course the built-in function sum() will do it too): # add up the integers from 1 to 10 and print out the total total&lt;-0 for (i in 1:10) { total&lt;-total+i } print(total) ## [1] 55 5.3.2 using vectors with loops It is especially useful to use loops to assign vector variables one element at a time. The loop variable (e.g. i) is usually used for indexing the vector variable. There are several necessary features of using vectors in for loops, so let us take an example script and break them down in this example of calculating a vector of the sum of integers from 1 up to the current number: Pre-allocate the vector variable: create a vector of the correct length prior to the loop by filling it with a placeholder value, e.g. 0 or NA (not a value). max &lt;- 10 total_vec &lt;- rep(0, max + 1) # pre-allocate the vector Only put in the loop what has to be repeated (e.g. don’t put the pre-allocation inside the loop); for (i in 1:max) { # iterate for i from 1 to max total_vec[i+1] &lt;- total_vec[i] + i # assign to the next value of total_vec the sum of i and the current value of total_vec } Be careful with indexing inside for loops - this means paying careful attention to the lowest and the highest index that you’re using inside the loop. For example, if your loop vector goes from 1 to max (as above), the total number of elements in your vector will be max+1, because we assigned the element with index i+1 in the loop: print(length(total_vec)) # print the number of elements of total_vec ## [1] 11 print(total_vec) ## [1] 0 1 3 6 10 15 21 28 36 45 55 Here is an example of a loop calculating the famous Fibonacci sequence, where the next value is defined to be the sum of two previous values in the sequence, and the plotting the resulting sequence, together with an exponential function that approximates it. num_steps &lt;- 20 # number of steps fib &lt;- rep(1, num_steps + 2) # pre-allocate vector with correct number of elements with all 1s for (i in 1:num_steps) { fib[i+2] &lt;- fib[i+1] + fib[i] # add elements i and i+1 and assign it to element i+2 } time &lt;- 0:(num_steps+1) # define a &quot;time&quot; vector as the independent variable plot (time, fib, lwd = 3, xlab = &#39;time steps&#39;, ylab = &quot;Fibonacci&quot;) # an exponential function wiht the Golden ratio base phi &lt;- (1+sqrt(5))/2 # golden ratio sol &lt;- 0.75*phi^time # calculate the exponential function vector (no loop required) lines(time, sol, col=&#39;red&#39;, lwd = 3) # overlay the exponential function plot legend(&quot;topleft&quot;, # legend placement c(&quot;Fibonacci&quot;, &quot;exponential&quot;), # vector containing labels for the legend col=1:2, # vector containing color codes (1 is black, 2 is red) lty=c(0,1), # vector specifying line types (0 is none, 1 is regular line) pch=c(1,NA), # vector specifying types of point markers (1 is cicle, NA is none) lwd=3) 5.3.3 Exercises: Assign the value 5 to a variable and multiply it by 1.03, replacing the old value of the variable. The new value should be 5.15. Write a script to take a number and multiply it by 1.03 one hundred times using a for loop (see section 4.4.2) Starting with the initial value 5, the script should return the value 96.093. Preallocate a vector of values to be all zeros, assign the first element to 5, and then use a for loop to calculate 100 values by multiplying the previous one by 1.03 and assigning them to sequential elements of the vector, then plot that vector. You should see an exponentially growing curve, with the last element (101st) equal to 96.093. Fix the indexing error in the script below for calculating a vector of factorials. Each element of the vector factorial_vec[i] should be equal to \\(i!= 1*2*3...*i\\), so for example factorial_vec[2] should be 2 and factorial_vec[3] should be 6. # multiply integers from 1 to 10 and assign the factorials to the vector max.num &lt;- 10 factorial_vec &lt;- rep(1,max.num) for (i in 1:max.num) { factorial_vec[i+1]&lt;-factorial_vec[i]*i } print(factorial_vec) 5.4 R Assignment In this part you will calculate numeric solutions of difference equations. Use the code from the last of the exercises above to write a script to iteratively update the variable for a specified number of steps and store it as a vector, and plot it as function of time. Hint: your code will be nearly identical for all the tasks, so once you get the first script to work properly, copy and paste it and modify the line in the for loop that calculates the next values. Please use descriptive names for variables and label your axes! For each of the following questions, do these tasks: write down the difference equation for the model; calculate the numerical solution for the model with the given initial values and for the specified time period; plot the solution over time (create vector for time!); describe how the solution behaves in the long term and whether it depends on the initial value (try different initial values!) The number of infected people in a population grows by 8% per day and those who become infected remain infected. If initially there are 8 infected, how many individuals will be infected in 45 days? ANSWER: about 255 # YOUR CODE HERE YOUR ANSWERS GO HERE Suppose bacteria in a population divide in 2 every hour and 90% of the current population dies after reproduction, not including the new offspring. If the population initially has 10 million bacteria, predict how many there will be in 24 hours. ANSWER: about 98497327 # YOUR CODE HERE YOUR ANSWERS GO HERE In a rabbit population, each pair produces 2.2 offspring every year (i.e. 1.1 per capita) and the adults have a 0.9 annual death rate after reproduction (and assume the whole population is paired up into mating pairs.) If initially there are 60 rabbits, predict how many there will be in 25 years. ANSWER: about 5724 # YOUR CODE HERE YOUR ANSWERS GO HERE Consider the same rabbit population, but now a python that lives nearby eats exactly 1 rabbit per month. If initially there are 60 rabbits, predict how many there will be in 25 years. ANSWER: 60 # YOUR CODE HERE YOUR ANSWERS GO HERE 10 fish are added to an aquarium every month, while 80% of those present survive every month and there is no reproduction. If initially there are 10 fish in the aquarium, predict how many there will be in a year. ANSWER: about 47 # YOUR CODE HERE YOUR ANSWERS GO HERE "],["linear-ordinary-differential-equations.html", "6 Linear ordinary differential equations 6.1 Building differential equations 6.2 Solutions of ordinary differential equations 6.3 Numeric solutions and the Forward Euler method 6.4 Forward Euler method in R 6.5 Applications of linear ODE models 6.6 R Assignment", " 6 Linear ordinary differential equations He felt a restless, vague ambition A craving for a change of air (A most unfortunate condition, A cross not many choose to bear.) – Alexander Pushkin, Eugene Onegin In the last chapter we considered discrete time models, in which time is counted in integers. This worked well to describe processes that happen in periodic cycles, like cell division or heart pumping. Many biological systems do not work this way. Change can happen continuously, that is, at any moment in time. For instance, the concentration of a biological molecule in the cell changes gradually, as does the voltage across the cell membrane in a neuron. There are at least two good reasons to use differential equations for many applications. First, they are often more realistic than discrete time models, because some events happen very frequently and non-periodically. The second reason is mathematical: it turns out that dynamical systems with continuous time, described by differential equations, are better behaved than difference equations. This has to do with the essential “jumpiness” of difference equations. Even for simple nonlinear equations, the value of the variable after one time step can be far removed from its last value. This can lead to highly complicated solutions, as we saw in some of the numerical solutions in the last chapter. That kind of erratic behavior is impossible in one-variable differential equations (provided they have continuous defining functions.) In this chapter you will learn to do the following: %The models for continuously changing variables require their own set of mathematical tools. Instead of difference equations, we are going to see our first differential equations, which use derivatives to describe how a variable changes with time. There is a tremendous amount of knowledge accumulated by mathematicians, physicists and engineers for analyzing and solving differential equations. There are many classes of differential equations for which it is possible to find analytic solutions, often in the form of so-called special functions. Differential equations courses for physicists and engineers are typically focused on learning about the variety of existing tools for solving a few types of differential equations. For the purposes of biological modeling, knowing how to solve a limited number of differential equations is of limited usefulness. We will instead focus on learning how to analyze the behavior of differential equations in general, without having to solve them on paper. build differential equations based on stated assumptions find analytic solutions of linear differential equations compute numerical solutions of differential equations using the Forward Euler method 6.1 Building differential equations 6.1.1 from discrete time to continuous In this chapter we investigate continuous time dynamical systems, for which it does not make sense to break time up into equal intervals. Instead of equations describing the increments in the dependent variable from one time step to the next, we will see equations with the instantaneous rate of the change (derivative) of the variable. Let us see the connection between the discrete and continuous dynamic models by reducing the step size of the bacteria-division population model. First, suppose that instead of dividing every hour, the population of bacteria divide every half-hour, but only half of the population does. That half is chosen randomly, so we don’t have to keep track of whether each bacterium divided the last time around or not. Therefore, each half-hour exactly half of the population is added to the current population: \\[ N(t+0.5) = N(t) + 0.5N(t) = 1.5N(t)\\] The solution for this model can be figured out from the linear difference equation solution we derived in section Every half-hour, the population is multiplied by 1.5, so we can write: \\[ N(t) = 1.5^{2t} N(0) = (1.5^2)^t N(0)\\] Compare this solution with the one for the every-hour model, \\(N(t) = 2^t N(0)\\) by plugging in a few numbers for \\(t\\). The half-hour model grows faster, because it has the base of 2.25 instead of 2. Now, suppose that the bacteria can divide four times an hour, but only a quarter of the population reproduces at any given time. The model can be written similarly: \\[N(t+0.25) = N(t) + 0.25N(t) = 1.25N(t)\\] The solution for this model is once again exponential, with the difference that each half contains 4 division events: \\[N(t) = 1.25^{4t}N(0) = (1.25^4)^t N(0)\\] This solution has the exponential base is \\(1.25^4\\), which is larger than \\(1.5^2\\). So what happens when we take this further? Suppose the bacteria divide \\(m\\) times an hour, with time step \\(1/m\\). Then extending our models above, we can write down the model and the solution: \\[ N(t + 1/m) = N(t) + 1/m N(t) = (1+1/m) N(t) \\] \\[ N(t) = (1+1/m)^{mt} N(0) = [(1+1/m)^m]^t N(0)\\] Now we can do what mathematicians enjoy the most: take things to the limit. What if \\(m\\) were 100? A million? A gazillion? Let us re-write the model equation: \\[ N(t+1/m) - N(t) = 1/m N(t) \\Rightarrow \\frac{N(t+1/m) - N(t) }{1/m} = N(t)\\] The expression on the left is known as Newton’s quotient that you encounter in the definition of a derivative. It measures the rate of change of the population \\(N\\) from some time \\(t\\) to the next time step \\(t+1/m\\). If \\(m\\) is increased to make the time step smaller, this makes both the numerator and the denominator smaller, and the quotient approaches the instantaneous rate of change of \\(N(t)\\). So, if bacteria divide at any point in time, with the , the model becomes a differential equation: \\[ \\frac{dN}{dt} = N(t)\\] We can do a similar procedure to the formula of the solution of the model. The dependence on \\(m\\) is all on the left-hand side, in the expression \\((1+1/m)^m\\), which is the base of the exponential function. What happens to this number as \\(m\\) becomes larger? Does it increase without bound? You can investigate this numerically by plugging in progressively larger numbers \\(m\\), and see that the number approaches a specific value: 2.71828… This is the special constant \\(e\\), called the base of the natural logarithm. So, if bacteria divide at any point in time, with the average rate of 1 per hour, the solution of the model becomes: \\[ N(t) = e^t N(0)\\] 6.1.2 Exercises Here we will explore the effect of changing the step size on the solution of a discrete time dynamic model. We will use a very simple model of bacterial population growth, in which we assume that bacteria divide once an hour and there are no deaths. Calculate the solution for this population, assuming that all bacteria divide exactly once an hour - in other words, a birth rate of one per individual. Starting with one bacterium use a for loop to calculate the solution for 10 hours and print out the last value. # YOUR CODE HERE Suppose that these bacteria can divide twice an hour, but only half of the population divides each time - in other words, a per capita birth rate of 0.5 per half an hour. Change your model so it calculates a solution vector with the time step of 30 minutes over 10 hours, print out the number of bacteria after 10 hours and compare it with the previous value. # YOUR CODE HERE Suppose that these bacteria divide every 15 minutes, but only one quarter of the population divides each time - in other words, a per capita birth rate of 0.25 per quarter hour. Change your model so it calculates a solution vector with the time step of 15 minutes over 10 hours, print out the number of bacteria after 10 hours and compare it with the previous value. # YOUR CODE HERE Suppose that these bacteria divide every 1 minute, but only 1/60 of the population divides each time - in other words, a per capita birth rate of 1/60 per minute. Change your model so it calculates a solution vector with the time step of 1 minute over 10 hours, print out the number of bacteria after 10 hours and compare it with the previous value. # YOUR CODE HERE Suppose that these bacteria divide every second, but only 1/3600 of the population divides each time - in other words, a per capita birth rate of 1/3600 per second. Change your model so it calculates a solution vector with the time step of 1 second over 10 hours, print out the number of bacteria after 10 hours and compare it with the previous value. # YOUR CODE HERE Produce a plot of the five solutions of bacterial population dividing with different time steps. Take the five code chunks from above, and copy them all into the chunk below. For each calculation add a time vector that corresponds to each time step (e.g. one for every hour for the first one, one for every second for the last one) and make a plot of each of the solutions as function of time on the same plot - use plot() for the first one and lines() for all the rest, with different colors and add a legend indicating different time steps. # YOUR CODE HERE What behaviors do you see for the solutions with different time steps? What effect does shrinking the time step have on the solution? What do you expect would happen if the time step were a millisecond, or a microsecond? 6.1.3 growth proportional to population size We will now build some common differential equations models. First, a simple population growth model with a constant growth rate. Suppose that in a population each individual reproduces with the average reproductive rate \\(r\\). This is reflected in the following differential equation: \\[\\begin{equation} \\frac{d x} {dt} = \\dot x = r x \\label{eq:linear_ode} \\end{equation}\\] This expression states that the rate of change of \\(x\\), which we take to be population size, is proportional to \\(x\\) with multiplicative constant \\(r\\). We will sometimes use the notation \\(\\dot x\\) for the time derivative of \\(x\\) (which was invented by Newton) for aesthetic reasons. First, we apply dimensional analysis to this model. The units of the derivative are population per time, as can be deduced from the Newton’s quotient definition. Thus, the units in the equation have the following relationship: \\[ \\frac{[population]}{[time]} = [r] [population] = \\frac{1}{[time]}[population] \\] This shows that as in the discrete time models, the dimension of the population growth rate \\(r\\) is inverse time, or frequency. The difference with the discrete time population models lies in the time scope of the rate. In the case of the difference equation, \\(r\\) is the rate of change per one time step of the model. In the differential equation, \\(r\\) is the instantaneous rate of population growth. It is less intuitive than the growth rate per single reproductive cycle, just like the slope of a curve is less intuitive than the slope of a line. The population growth happens continuously, so the growth rate of \\(r\\) individuals per year does not mean that if we start with one individual, there will be \\(r\\) after one year. In order to make quantitative predictions, we need to find the solution of the equation, which we will see in the next section. 6.1.4 chemical kinetics Reactions between molecules in cells occur continuously, driven by molecular collisions and physical forces. In order to model this complex behavior, it is generally assumed that reactions occur with a particular speed, known as the kinetic rate constant. As mentioned in chapter 2, a simple reaction of conversion from one type of molecule (\\(A\\)) to another (\\(B\\)) can be written as follows: \\[ A \\xrightarrow{k} B \\] In this equation the parameter \\(k\\) is the kinetic rate rate constant, describing the speed of conversion of \\(A\\) into \\(B\\), per concentration of \\(A\\). Chemists and biochemists use differential equations to describe the change in molecular concentration during a reaction. These equations are known as the laws of mass action. For the reaction above, the concentration of molecule \\(A\\) decreases continuously proportionally to itself, and the concentration of molecule \\(B\\) increases continuously proportionally to the concentration of \\(A\\). This is expressed by the following two differential equations: \\[\\begin{eqnarray} \\label{eq:lin_chem_kin} \\dot A &amp;=&amp; - k A \\\\ \\dot B &amp;=&amp; kA \\end{eqnarray}\\] Several conclusions are apparent by inspection of the equations. First, the dynamics depend only on the concentration of \\(A\\), so keeping track of the concentration of \\(B\\) is superfluous. The second observation reinforces the first: the sum of the concentrations of \\(A\\) and \\(B\\) is constant. This is mathematically demonstrated by adding the two equations together to obtain the following: \\[ \\dot A + \\dot B = -kA + kA = 0\\] One of the basic properties of the derivative is that the sum of derivatives is the same as the derivative of the sum: \\[\\dot A + \\dot B = \\frac{d(A+B)}{dt} = 0\\] This means that the sum of the concentrations of \\(A\\) and \\(B\\) is a constant. This is a mathematical expression of the law of conservation in chemistry: molecules can change from one type to another, but they cannot appear or disappear in other ways. In this case, a single molecule of \\(A\\) becomes a single molecule of \\(B\\), so it follows that the sum of the two has to remain the same. If the reaction were instead two molecules of \\(A\\) converting to a molecule of \\(B\\), then the conserved quantity is \\(2A + B\\). The concept of conserved quantity is very useful for the analysis of differential equations. We will see in later chapters how it can help us find solutions, and explain the behavior of complex dynamical systems. 6.2 Solutions of ordinary differential equations In this section we will investigate how to write down analytic solutions for ordinary differential equations (ODEs). Let us first define the mathematical terms that will be used in this discussion. Definition 6.1 An ordinary differential equation is an equation that contains derivatives of the dependent variable (e.g. \\(x\\)) with respect to an independent variable (e.g. \\(t\\)). For example: \\[ \\frac{dx^2}{dt^2}+ 0.2 \\frac{dx}{dt} - 25 = 0 \\] For the time being, we will restrict ourselves to ODEs with the highest derivative being of first order, called first-order ODEs. Second-order ODEs are common in models derived from physics, but can actually be converted to first-order ODEs, though it requires an additional dependent variable. Third and higher order ODEs are very uncommon. To be precise: Definition 6.2 A first-order ODE is one where the derivative \\(dx/dt\\) is equal to a defining function \\(f(x,t)\\), like this: \\[\\frac{dx} {dt} = \\dot x = f(x,t)\\] The defining function may potentially depend on both the dependent variable \\(x\\) and the independent variable \\(t\\). If it only depends on \\(x\\), it is called an autonomous ODE, for example: \\[ \\frac{dx}{dt} = \\dot x = rx \\] or \\[\\frac{dx}{dt} = \\dot x = 5x -4\\] On the other hand, if the defining function depends only on the independent variable \\(t\\), it may be called a pure-time ODE, for example: \\[ \\frac{dx}{dt} = \\dot x = 5t \\] or \\[ \\frac{dx}{dt} = \\dot x = 20 - 0.3 \\sin(4 \\pi t) \\] An ODE is if every term involves either the dependent variable \\(x\\) or its derivative. \\end{mosdef} For instance, \\(\\dot x = x^2 + \\sin(x)\\) is homogeneous, while \\(\\dot x = -x + 5t\\) is not. Most simple biological models that we will encounter in the next two chapters are autonomous, homogeneous ODEs. However, nonhomogeneous equations are important in many applications, and we will encounter them at the end of the present section. 6.2.1 separate and integrate method Definition 6.3 The analytic (or exact) solution of an ordinary differential equation is a function of the independent variable that satisfies the equation. If no initial value is given, then the general solution function will contain an uknown integration constant. If an initial value is specified, the integration constant can be found to obtain a specific solution. This means that the solution function obeys the relationship between the derivative and the defining function that is specified by the ODE. To verify that a function is a solution of a given ODE, take its derivative and check whether it matches the other side of the equation. Example. The function \\(x(t) = 3t^2 +C\\) is a general solution of the ODE \\(\\dot x = 6t\\), which can be verified by taking the derivative: \\(\\dot x (t) = 6t\\). Since this matches the right-hand side of the ODE, the solution is valid. Example. The function \\(x(t) = Ce^{5t}\\) is a general solution of the ODE \\(\\dot x = 5x\\). This can be verified by the taking the derivative: \\(\\dot x = 5C e^{5t}\\) and comparing it with the right-hand side of the ODE: \\(5x = 5 Ce^{5t}\\). Since the two sides of the equation agree, the solution is valid. In contrast with algebraic equations, we cannot simply isolate \\(x\\) on one side of the equal sign and find the solutions as one, or a few numbers. Instead, solving ordinary differential equations is very tricky, and no general strategy for solving an arbitrary ODE exists. Moreover, a solution for an ODE is not guaranteed to exist at all, or not for all values of \\(t\\). We will discuss some of the difficulties later, but let us start with equations that we can solve. The most obvious strategy for solving an ODE is integration. Since a differential equation contains derivatives, integrating it can remove the derivative. In the case of the general first order equation, we can integrate both sides to obtain the following: \\[ \\int \\frac{dx}{dt} dt = \\int f(x,t) dt \\Rightarrow x(t) + C = \\int f(x,t) dt\\] The constant of integration \\(C\\) appears as in the standard antiderivative definition. It can be specified by an initial condition for the solution \\(x(t)\\). Unless the function \\(f(x,t)\\) depends only on \\(t\\), it is not possible to evaluate the integral above. Instead, various tricks are used to find the analytic solution. The simplest method of analytic solution of a first-order ODEs, which I call separate-and-integrate consists of the following steps: use algebra to place the dependent and independent variables on different sides of the equations, including the differentials (e.g. \\(dx\\) and \\(dt\\)) integrate both sides with respect to the different variables, don’t forget the integration constant solve for the dependent variable (e.g. \\(x\\)) to find the general solution plug in \\(t=0\\) and use the initial value \\(x(0)\\) to solve for the integration constant and find the the specific solution Example. Consider a very simple differential equation: \\(\\dot x = a\\), where \\(\\dot x\\) stands for the time derivative of the dependent variable \\(x\\), and \\(a\\) is a constant. It can be solved by integration: \\[ \\int \\frac{dx}{dt} dt = \\int a dt \\Rightarrow x(t) + C = at \\] This solution contains an undetermined integration constant; if an initial condition is specified, we can determine the complete solution. Generally speaking, if the initial condition is \\(x(0) = x_0\\), we need to solve an algebraic equation to determine \\(C\\): \\(x_0 = a*0 - C\\), which results in \\(C = -x_0\\). The complete solution is then \\(x(t) = at + x_0\\). To make the example more specific, if \\(a = 5\\) and the initial condition is \\(x(0) = -3\\), the solution is \\[x(t) = 5t -3\\] Example. Let us solve the linear population growth model in equation : \\(\\dot x = rx\\). The equation can be solved by first dividing both sides by \\(x\\) and then integrating: \\[ \\int \\frac{1}{x} \\frac{d x}{dt} dt = \\int \\frac{dx}{x} = \\int r dt \\Longrightarrow \\log |x| = rt + C \\Longrightarrow x = e^{rt+C} = Ae^{rt}\\] We used basic algebra to solve for \\(x\\), exponentiating both sides to get rid of the logarithm on the left side. As a result, the additive constant \\(C\\) gave rise to the multiplicative constant \\(A=e^C\\). Once again, the solution contains a constant which can be determined by specifying an initial condition \\(x(0) = x_0\\). In this case, the relationship is quite straightforward: \\(x(0) = A e^0 = A\\). Thus, the complete solution for equation is: \\[ x(t) = x_0e^{rt}\\] 6.2.2 behavior of solutions of linear ODEs As in the case of the discrete-time models, population growth with a constant birth rate has exponential form. Once again, please pause and consider this fact, because the exponential solution of linear equations is one of the most basic and powerful tools in applied mathematics. Immediately, it allows us to classify the behavior of linear ODE into three categories: \\(r &gt; 0\\): \\(x(t)\\) grows without bound \\(r &lt; 0\\): \\(x(t)\\) decays to 0 \\(r = 0\\): \\(x(t)\\) remains constant at the initial value The rate \\(r\\) being positive reflects the dominance of birth rate over death rate in the population, leading to unlimited population growth. If the death rate is greater, the population will decline and die out. If the two are exactly matched, the population size will remain unchanged. Example. The solution for the biochemical kinetic model in equation is identical except for the sign: \\(A(t) = A_0 e^{-kt}\\). When the reaction rate \\(k\\) is positive, as it is in chemistry, the concentration of \\(A\\) decays to 0 over time. This should be obvious from our model, since there is no back reaction, and the only chemical process is conversion of \\(A\\) into \\(B\\). The concentration of \\(B\\) can be found by using the fact that the total concentration of molecules in the model is conserved. Let us call it \\(C\\). Then \\(B(t) = C - A(t) = C- A_0e^{-kt}\\). The concentration of \\(B\\) increases to the asymptotic limit of \\(C\\), meaning that all molecules of \\(A\\) have been converted to \\(B\\). 6.2.3 solutions of nonhomogeneous ODEs ODEs that contain at least one term without the dependent variable are a bit more complicated. If the defining function is \\(f(x,t)\\) is linear in the dependent variable \\(x\\), they can be solved on paper using the same separate-and-integrate method, modified slightly to handle the constant term. Here are the steps to solve the generic linear ODE with a constant term \\(\\dot x = ax +b\\): separate the dependent and independent variables on different sides of the equations, by dividing both sides by the right hand side \\(ax+b\\), and multiplying both sides by the differential \\(dt\\) integrate both sides with respect to the different variables, don’t forget the integration constant! solve for the dependent variable (e.g. \\(x\\)) plug in \\(t=0\\) and use the initial value \\(x(0)\\) to solve for the integration constant Example. Let us solve the following ODE model using separate and integrate with the given initial value: \\[\\frac{dx}{dt} = 4x -100; \\; x(0) = 30\\] separate the dependent and independent variables: \\[ \\frac{dx}{4x - 100} = dt\\] integrate both sides: \\[ \\int \\frac{dx}{4x -100} = \\int dt \\Rightarrow \\frac{1}{4} \\int \\frac{du}{u} = \\frac{1}{4} \\ln | 4x- 100 | = t + C\\] The integration used the substitution of the new variable \\(u=4x -100\\), with the concurrent substitution of \\(dx = du/4\\). solve for the dependent variable: \\[ \\ln | 4x- 100 | = 4t + C \\Rightarrow 4x-100 = e^{4t} B \\Rightarrow x = 25 + Be^{4t}\\] Here the first step was to multiply both sides by 4, and the second to use both sides as the exponents of \\(e\\), removing the natural log from the left hand side, and finally simple algebra to solve for \\(x\\) as a function of \\(t\\). solve for the integration constant: \\[ x(0) = 25 + B = 30 \\Rightarrow B = 5\\] Here the exponential ``disappeared’’ because \\(e^0=1\\). \\end{enumerate} Therefore, the complete solution of the ODE with the given initial value is \\[x(t) = 25 + 5e^{4t}\\] At this point, you might have noticed something about solutions of linear ODEs: they always involve an exponential term, with time in the exponent. Knowing this, it is possible to bypass the whole process of separate-and-integrate by using the following short-cut. Important fact: Any linear ODE of the form $x= ax +b $ has an analytic solution of the form: \\[ x(t) = Ce^{at} + D\\] This can be tested by plugging the solution back into the ODE to see if it satisfies the equation. First, take the derivative of the solution to get the left-hand side of the ODE: \\(\\frac{dx}{dt} = Ca e^{at}\\); the plug in \\(x(t)\\) into the right hand side of the ODE: \\(aCe^{at} + aD +b\\). Setting the two sides equal, we get: \\[Ca e^{at} = aCe^{at} + aD +b\\] which is satisfied if \\(aD + b = 0\\), which means \\(D= -b/a\\). This is consistent with the example above, the additive constant in the solution was 25, which is \\(-b/a= -(-100)/4 = 25\\). In short, if you want to solve a linear ODE \\(\\dot x= ax +b\\) , you can bypass the separate-and-integrate process, because the general solution always has the form: \\[\\begin{equation} x(t) = Ce^{at} - \\frac{b}{a} \\label{eq:ch15_ode_sol} \\end{equation}\\] The unknown constant \\(C\\) can be determined from a given initial value. So the upshot is that all linear ODEs have solutions which are exponential in time with exponential constant coming from the slope constant \\(a\\) in the ODE. The dynamics of the solution are determined by the sign of the constant \\(a\\): if \\(a&gt;0\\), the solution grows (or declines) without bound; and if \\(a&lt;0\\), the solution approaches an asymptote at \\(-b/a\\) (from above or below, depending on the initial value). Go back and read section for a review of exponential functions if this is not clear. 6.2.4 Exercises Solve the following linear ODEs and use the specified initial values to determine the integration constant. Describe how the solution behaves over a long time (e.g. grows without bound, goes to zero, etc.). Plug the solution back into the ODE to check that it satisfies the equation. \\[ \\frac{dx}{dt} = 0.1; \\; x(0)= 100 \\] \\[ \\frac{dx}{dt} = 2\\sin(4t) -0.4t; \\; x(0)= 5 \\] \\[ \\frac{dx}{dt} = 3x; \\; x(0) = 0.4 \\] \\[ \\frac{dx}{dt} = -5x; \\; x(0) = -300 \\] \\[ \\frac{dx}{dt} = -0.5x + 100 ; \\; x(0) = 20 \\] \\[ \\frac{dx}{dt} = 1 + x; \\; x(0) = 4 \\] \\[ \\frac{dx}{dt} = -10 - 0.2x; \\; x(0) = 10 \\] \\[ \\frac{dx}{dt} = -4 + 0.5x; \\; x(0) = 6 \\] 6.3 Numeric solutions and the Forward Euler method Analytic solutions are very useful for a modeler because they allow prediction of the variable of interest at any time in the future. However, for many differential equations they are not easy to find, and for many others they simply cannot be written down in a symbolic form. Instead, one can use a numerical approach, which does not require an exact formula for the solution. The idea is to start at a given initial value (e.g. \\(x(0)\\)) and use the derivative from the ODE (e.g. \\(dx/dt\\)) as the rate of change of the solution (e.g. \\(x(t)\\)) to calculate the change or increment for the solution over a time step. Essentially, this means replacing the continuous change of the derivative with a discrete time step, thus converting the differential equation into a difference equation and then solving it. The solution of the difference equation is not the same as the solution of the ODE, so numeric solutions of ODEs are always approximate. I will use the letter \\(y(t)\\) to denote the numerical solution to distinguish it from the exact solution \\(x(t)\\). The fundamental difference between them is that \\(y(t)\\) is not a formula that can be evaluated at any point in time, but instead is a sequence of numbers calculated every time step, which hopefully are close to the exact solution \\(x(t)\\). Let us introduce all the players: first, we need to pick the time step \\(\\Delta t\\), which is the length of time between successive values of \\(y\\). In the difference equation notation one can use \\(y_i\\) to mean \\(y(i\\Delta t)\\), the value of the numerical solution after \\(i\\) time steps. Then we need to calculate the derivative, or the rate of change at a particular point in time. For any first-order ODE of the form \\[ \\frac{d x} {dt} = \\dot x = f(x,t)\\] the rate of change depends (potentially) on the values of \\(x\\) and \\(t\\). This rate of change based on the numerical solution after \\(i\\) time steps is \\(f(y(i\\Delta t), i\\Delta t) = f(y_i, t_i)\\). Finally, to calculate the change of the dependent variable we need to multiply the rate of change by the time step. This should make sense in a practical context: if you drive for two hours (time step) at 60 miles per hour (rate of change), the total distance (increment) is \\(2*60=120\\) miles. By the same token, we can write down how to calculate the next value of the numerical solution \\(y_{i+1}\\) based on the previous one: \\[\\begin{equation} y_{i+1} = y_i + \\Delta t f(y_i, t_i) \\label{eq:ch15_FE} \\end{equation}\\] This method of computing a numerical solution of an ODE is called the Forward Euler method, after the famous mathematician who first came up with it. It is called a forward method because it uses the value of the dependent variable and its derivative at time step \\(i\\) to predict the value at the next time step \\(i+1\\). The method is iterative, so it needs to be repeated in order to calculate a set of values of the approximate solution \\(y(t)\\). Here are a couple of simple examples of computing numerical solution using FE: Let us numerically solve the ODE \\(\\dot x = -0.1\\) using the Forward Euler method. This means the defining function in the formulation of FE above is \\(f(x,t)=-0.1\\). We can calculate the numeric solution for a couple of steps and compare the values with the exact solution, since we now know that it is \\(x(t) = x_0 -0.1t\\). Let us pick the time step \\(\\Delta t = 0.2\\) and begin with the initial value \\(x(0)=1\\). Here are the first three steps using the FE method: \\[ y(0.2) = y(0) + \\Delta t f(y(0)) = 1 + 0.2*(-0.1) = 0.98\\] \\[ y(0.4) = y(0.2) + \\Delta t f(y(0.2)) = 0.98+ 0.2*(-0.1) = 0.96\\] \\[ y(0.6) = y(0.4) + \\Delta t f(y(0.4)) = 0.96+ 0.2*(-0.1) = 0.94\\] Since the rate of change in this ODE is constant, the solution declines by the same amount every time step. In this case, the numerical solution is actually exact, and perfectly matches the analytic solution. Table (right) shows the numerical solution for 3 time steps along with the exact solution. Let us numerically solve the ODE \\(\\dot x = -0.1x\\) using the Forward Euler method. This means the defining function in the formulation of FE above is \\(f(x,t)=-0.1x\\). We can calculate the numeric solution for a couple of steps and compare the values with the exact solution, since we now know that it is \\(x(t) = x_0 e^{-0.1t}\\). Let us pick the time step \\(\\Delta t = 0.2\\) and begin with the initial value \\(x(0)=100\\). Here are the first three steps using the FE method: \\[ y(0.2) = y(0) + \\Delta t f(y(0)) = 100 + 0.2*(-0.1*100) = 98\\] \\[ y(0.4) = y(0.2) + \\Delta t f(y(0.2)) = 98+ 0.2*(-0.1*98) \\] \\[ = 96.04\\] \\[ y(0.6) = y(0.4) + \\Delta t f(y(0.4)) = 96.04+ 0.2*(-0.1*96.04) \\approx \\] \\[ \\approx 94.12\\] In this case, the derivative is not constant and the numerical solution is not exact, which is demonstrated in table (left). The error in the numerical solution grows with time, which may be problematic. We will further investigate how to implement the computation of numerical solutions using R in the next section. Numerical solutions of the ODE \\(\\dot x = -0.1\\) using Forward Euler \\(y\\) calculated for 3 steps of size \\(\\Delta t = 0.2\\) as well as the exact solution \\(x\\), both rounded to two digits after the decimal, and the error of the numerical solution. t x y error 0 1 1 0 0.2 0.98 0.98 0 0.4 0.96 0.96 0 0.6 0.94 0.94 0 Numerical solution of the ODEs \\(\\dot x = -0.1x\\) (right) using Forward Euler \\(y\\) calculated for 3 steps of size \\(\\Delta t = 0.2\\) as well as the exact solution \\(x\\), both rounded to two digits after the decimal, and the error of the numerical solution. t x y error 0 100 100 0 0.2 98.02 98 0.02 0.4 96.08 96.04 0.04 0.6 94.18 94.12 0.06 6.3.1 Exercises Use the Forward Euler method to solve the following differential equations with time step \\(\\Delta t=0.5\\) for 2 steps to compute \\(y(1)\\) (the value of the numerical solution at \\(t=1\\).) \\[ \\frac{dx}{dt} = 0.1; \\; x(0)= 100 \\] \\[ \\frac{dx}{dt} = 2\\sin(4t) -0.4t; \\; x(0)= 5 \\] \\[ \\frac{dx}{dt} = 3x; \\; x(0) = 0.4 \\] \\[ \\frac{dx}{dt} = -5x; \\; x(0) = -300 \\] \\[ \\frac{dx}{dt} = -0.5x + 100 ; \\; x(0) = 20 \\] \\[ \\frac{dx}{dt} = 1 + x; \\; x(0) = 4 \\] \\[ \\frac{dx}{dt} = -10 - 0.2x; \\; x(0) = 10 \\] \\[ \\frac{dx}{dt} = -4 + 0.5x; \\; x(0) = 6 \\] 6.4 Forward Euler method in R 6.4.1 implementation In practice, the most common approach to finding solutions for differential equations is using a computer to calculate a numerical solution, for example using the Forward Euler method. This means using a computer program to construct a sequence of values of the dependent variable that approximate the true solution. Below is an outline of the algorithm that can be translated into a programming language, like R, to solve ODEs. assign the time step dt, length of time Tmax, number of time steps numstep pre-allocate the vector of numeric solution values y of length numstep+1 assign the initial value for the ODE to the first element of the solution assign the vector of time values t from 0 to Tmax of length numstep+1 for loop starting at 1 to numstep assign the next solution value to be the current solution value plus the time step multiplied by the defining function at the current solution value plot numeric solution y as a function of time t To implement the algorithm, one need to know the defining function \\(f(x,t)\\), the initial value, the time step, and the total time. The output is the solution vector \\(y\\), which contains a sequence of values that approximate the solution of the ODE, along with the vector of time values spaced by the time step. Below is an example implementation of the Forward Euler method for the following linear ODE with initial value 1000: \\[ \\frac{dx}{dt} = -0.5 x \\] x0 &lt;- 1000 dt &lt;- 0.01 # set time step Tmax &lt;- 10 # set length of time numstep &lt;- Tmax/dt # assign number of time steps pop &lt;- rep(x0, numstep+1) # initialize solution with y0 for (i in 1:numstep) { # do the Euler! pop[i+1] &lt;- pop[i]+dt*(-0.5*pop[i]) } time &lt;- seq(0,Tmax, dt) plot(time, pop, type=&#39;l&#39;) Notice that it is very similar to the script for numerical solution of a difference equation we saw in with the major difference being the presence of a time step, whereas in difference equations the time step is always 1. There is one more important point for the implementation: usually one needs to solve the ODE for a particular length of time \\(T\\) with a specified time step \\(\\Delta t\\) . This dictates that the required number of iterations be \\(T/\\Delta t\\); in other words, for a given time period the number of time steps is inversely proportional to the time step. 6.4.2 Exercises Consider a slightly different linear ODE: \\[ \\frac{dx}{dt} = 0.2 x \\] Calculate the numeric solution of the ODE for one time step using Forward Euler, for time step dt=0.1, starting with initial value x(0) = 5. Answer: 5.1 # YOUR CODE HERE Write a script to solve the ODE using the Forward Euler method based on the outline above. Set the time step dt=0.1 and report the solution after 100 time steps. Answer: 36.22323 # YOUR CODE HERE Change the time step to be dt=0.01 and report the solution after 1000 time steps. Answer: 36.87156. # YOUR CODE HERE 6.4.3 error analysis Numeric solutions of ODE are always approximate, because they use discrete time steps to approximate continuous change (derivatives). Thus numeric solutions always have error, which is the difference between the exact or analytic solution and the numeric solution. If we know the exact solution of an ODE, we can calculate the error using vector subtraction in R. For the same linear ODE we solved above: \\[ \\frac{dx}{dt} = -0.5 x \\] The analytic solution is \\(x(t) = x_0 e^{-0.5t}\\), where \\(x_0\\) is the initial value. Here is an example of computing the numeric solution (as we did above) and then calculating the analytic solution and plotting it: x0 &lt;- 1000 dt &lt;- 0.5 # set time step Tmax &lt;- 10 # set length of time numstep &lt;- Tmax/dt # assign number of time steps pop &lt;- rep(x0, numstep+1) # initialize solution with y0 for (i in 1:numstep) { # do the Euler! pop[i+1] &lt;- pop[i]+dt*(-0.5*pop[i]) } time &lt;- seq(0,Tmax, dt) plot(time, pop, type=&#39;l&#39;, main = &quot;Numeric and analytic solutions of an ODE&quot;) # plot the numeric solution exact &lt;- x0*exp(-0.5*time) # calculate the exact solution lines(time, exact, col = &#39;red&#39;) # plot the exact solution legend(&#39;topright&#39;, col=c(&#39;black&#39;, &#39;red&#39;), lty=1, legend = c(&#39;numeric&#39;, &#39;analytic&#39;)) Now we can calculate the error of the numeric solution and plot it: error &lt;- abs(exact - pop) plot(time, error, type = &#39;l&#39;, main=&#39;Error of the numeric solution&#39;) What is the sources of this error? There are at least two distinct sources of error in numerical solutions: a) and b) . Roundoff error is caused by computers representing real numbers by a finite string of bits on a computer using what is known as a representation. In many programming languages variables storing real numbers can be single or double precision, which typically support 24 and 53 significant binary digits, respectively. Any arithmetic operation involving floating point numbers is only approximate, with an error that depends on the way the numbers are stored in the memory. Truncation error is caused by approximations inherent in numerical algorithms, such as Forward Euler, which represent instantaneous rate of change in an ODE with discrete steps and thus are always a bit off from the true analytic solution. In practice, roundoff error is not a big concern in contemporary computation for most modelers. Truncation error, on the other hand, can cause big problems, but luckily it is within your control. One can decrease the error in the case of finite difference methods by choosing smaller time steps, or by choosing an algorithm with a higher order of accuracy, which we’ll leave for a more advanced discussion. Returning specifically to the Forward Euler method, it is called a first-order method because the total error of the solution (after some number of time steps) depends linearly on the time step \\(\\Delta t\\). One can show this by using the Taylor expansion of the solution \\(y(t)\\) to derive the forward Euler method, with \\(\\tau(\\Delta t)\\) representing the truncation error after one time step: \\[y(t+\\Delta t) = y(t) + \\Delta t \\frac {dy(t) }{dt} + \\tau(\\Delta t)\\] As you might have learned in calculus, the error remaining after the linear term in the Taylor series is proportional to the the square of the small deviation \\(\\Delta t\\). This only describes the error after 1 time step, but since the errors accumulate every time step, the total error after \\(N\\) time steps accumulates \\(N \\tau(\\Delta t)\\). As we saw in the implementation above, for a given length of time, \\(N\\) is inversely proportional to \\(\\Delta t\\). Therefore, the total error is proportional to the \\(\\Delta t\\) and so FE is a first-order method. The exercise above shows that new errors in FE method accumulate in proportion with the time step. The next question is, what happens to these errors over time? Do they grow or dissipate with more iterations? This is known as the stability of a numerical method, and unlike the above question about the order of accuracy, the answer depends on the particular ODE that one needs to solve. Below I show an example of error analysis for a linear ODE: Example. To numerically solve the equation \\(\\dot x = ax\\), we substitute the function \\(ax\\) for the function \\(f(x,t)\\), and obtain the FE approximation for this particular ODE: \\[y_{i+1} = y_i + \\Delta t a y_i = (1+a\\Delta t) y_i\\] The big question is what happens to the truncation error: does it grow or decay? To investigate this question, let us denote the error at time \\(t_i\\) , that is the difference between the true solution \\(x(t_i)\\) and the approximate solution \\(y(t_i)\\), by \\(\\epsilon_i\\). It follows that \\(y_i = x_i + \\epsilon_i\\). Then we can wrote the following difference equations involving the error: \\[y_{i+1} = x_{i+1} + \\epsilon_{i+1} = (x_i + \\epsilon_i) (1+a\\Delta t) = x_i (1+a\\Delta t) + \\epsilon_i(1+a\\Delta t)\\] Let us set aside the terns in the equation that involve \\(x\\) (since it is just the equation for forward Euler). The remaining difference equation for \\(\\epsilon\\) describes the change in the error: \\[\\epsilon_{i+1} = \\epsilon_i(1+a\\Delta t) \\] This states that the error in this numerical solution is repeatedly multiplied by the constant \\((1+a\\Delta t)\\). As we saw in section , this linear difference equation has an exponential solution \\(\\epsilon_n = (1+a\\Delta t)^n \\epsilon_0\\), which decays to 0 if \\(|1+a\\Delta t| &lt; 1\\) or grows without bound if \\(|1+a\\Delta t| &gt; 1\\). The first inequality is called the stability condition for the FE scheme, since it guarantees that the old errors decay over time. Since \\(\\Delta t &gt;0\\), the only way that the left hand side can be less than 1 is if \\(a&lt;0\\). Therefore, the condition for stability of the FE method for a linear ODE: \\[|1 + a\\Delta t| &lt; 1 \\Rightarrow \\Delta t &lt; -2/a\\] Thus, if \\(a&gt;0\\), the errors will eventually overwhelm the solution. If \\(a&lt;0\\), if the time step is small enough (less than \\(-2/a\\)) then FE is stable. Generally speaking, however, Forward Euler is about the worst method to use for practical numerical solutions of ODEs, due to its low accuracy and to its lack of stability under certain conditions. 6.4.4 Exercises \\[ \\frac{dx}{dt} = 0.2 x \\] Calculate the error of the numeric solution of this ODE after one time step with dt=0.1 and initial value x(0) = 5 by subtracting it from the exact (analytic) solution \\(x(t) = e^{0.2t}x(0)\\), with the same initial value. Answer: about 0.001. # YOUR CODE HERE Compute the error of the two numeric solution over Tmax = 10 by subtracting the numeric solution vector from the analytic solution calculating over the same time vector and report the mean of that error vector. Answers: for dt=0.1 the mean error is 0.722, for dt=0.01 the mean error is 0.0737. # YOUR CODE HERE 6.5 Applications of linear ODE models 6.5.1 model of pharmacokinetics Describing and predicting the dynamics of drug concentration in the body is the goal of pharmacokinetics. Any drug that humans take goes through several stages: first it is administered (put into the body), then absorbed, metabolized (transformed), and excreted (removed from the body) . Almost any drug has a dose at which it has a toxic effect, and most can kill a human if the dose is high enough. Drugs which are used for medical purposes have a therapeutic range, which lies between the lowest possible concentration (usually measured in the blood plasma) that achieves the therapeutic effect and the concentration which is toxic. One of the basic questions that medical practitioners need to know is how much and how frequently to administer a drug to maintain drug concentration in the therapeutic range. The concentration of a drug is a dynamic variable which depends on the rates of several processes, most directly on the rate of administration and the rate of metabolism. Drugs can be administered through various means (e.g. orally or intravenously) which influences their rate of absorption and thus how the concentration increases. Once in the blood plasma, drugs are metabolized primarily by enzymes in the liver, converting drug molecules into compounds that can be excreted through the kidneys or the large intestine. The process of *metabolism proceeds at a rate that depends on both the concentration of the drug and on the enzyme that catalyzes the reaction. For some drugs the metabolic rate may be constant, or independent of the drug concentration, since the enzymes are already working at full capacity and can’t turn over any more reactions, for example alcohol is metabolized at a constant rate of about 1 drink per hours for most humans. Figure a shows the time plots of the blood alcohol concentration for 4 males who ingested different amounts of alcohol, and the curves are essentially linear with the same slope after the peak. For other drugs, if the plasma concentration is low enough, the enzymes are not occupied all the time and increasing the drug concentration leads to an increase in the rate of metabolism. One can see this behavior in the metabolism of the anti-depressant drug bupropion in figure b, where the concentration curve shows a faster decay rate for higher concentration of the drug than for lower concentration. In the simplest case, the rate of metabolism is linear, or proportional to the concentration of the drug, with proportionality constant called the first-order metabolic rate. Blood alcohol content after ingesting different numbers of drinks, from 4 in the top curve to 1 in the bottom (figure from the National Institute on Alcohol Abuse and Alcoholism in public domain) Blood concentration of bupropion for two different drugs in clinical trials (image by CMBJ based on FDA data under CC-BY 3.0 via Wikimedia Commons) Let us build an ODE model for a simplified pharmacokinetics situation. Suppose that a drug is administered at a constant rate of \\(M\\) (concentration units per time unit) and that it is metabolized at a rate proportional to its plasma concentration \\(C\\) with metabolic rate constant \\(k\\). Then the ODE model of the concentration of the drug over time \\(C(t)\\) is: \\[ \\frac{dC}{dt} = M - kC\\] The two rate constants \\(M\\) and \\(k\\) have different dimensions, which you should be able to determine yourself. The ODE can be solved using the separate-and-integrate method: Divide both sides by the right hand side \\(M-kC\\), and multiply both sides by the differential \\(dt\\) \\[ \\frac{dC}{M-kC} = dt\\] integrate both sides with respect to the different variables, don’t forget the integration constant! \\[ \\int \\frac{dC}{M-kC} = \\int dt \\Rightarrow\\] \\[ -\\frac{1}{k} \\log |M-kC| = t + A\\] solve for the dependent variable \\(C(t)\\) \\[ \\exp(\\log |M-kC| ) = -\\exp(kt +A) \\Rightarrow \\] \\[ M - kC = B e^{-kt} \\Rightarrow \\] \\[ C(t) = \\frac{M}{k}- Be^{-kt}\\] Notice that I changed the values of integration constants \\(A\\) and \\(B\\) during the derivation, which shouldn’t matter because they have not been determined yet. plug in \\(t=0\\) and use the initial value \\(x(0)\\) to solve for the integration constant If we know the initial value \\(C(0) = C_0\\), then we can plug it in and get the following algebraic expression: \\[ C_0 = \\frac{M}{k} - B \\Rightarrow \\] \\[ B = C_0 - \\frac{M}{k}\\] Then the complete solution is: \\[ C(t) = \\frac{M}{k} - (C_0- \\frac{M}{k})e^{-kt}\\] The solution predicts that after a long time the plasma concentration will approach the value \\(M/k\\), since the exponential term decays to zero. Notice that mathematically this is the same type of solution we obtained in equation for a generic linear ODE with a constant term. 6.5.2 Discussion questions Describe in words the dependence of the long-term plasma concentration of the drug on the parameters. Does this prediction make intuitive sense? Explain in practical terms the assumption that the administration of the drug results in a constant rate of growth of the concentration. Under what circumstances does this match reality?} Explain in practical terms the assumption that the drug metabolism rate is proportional to the plasma concentration. Under what circumstances does this match reality?} Discuss how you could modify the ODE model to describe other circumstances, or to add other effects to it.} The following questions encourage you to think critically about the pharmacokinetic model above.} 6.6 R Assignment In this part you will solve the linear population model ODE: \\[ \\frac{dN}{dt} = (b-d) N \\] The model parameters are per capita birth and death rates (b and d, respectively), which may be expressed in percent population change per year (e.g. d= 0.12 is a death rate of 12% per year). Assume that the initial population is 500 individuals, and perform the following tasks: Write a Forward Euler script to solve this ODE for any given values of b and d (they should be inputs into your defining function). Set b=0.5 and d=0.12, plot the numeric solution, and report whether the solution behaves as you would expect it to, for Tmax=50 and dt=0.1. Describe the behavior of the solution of the ODE over time. #YOUR CODE HERE YOUR ANSWERS HERE Change the birth rate to b=0.01 and calculate the solution with the same initial value and same values of Tmax and dt. Plot the solution, describe its behavior, and compare its behavior to the one in the previous task. #YOUR CODE HERE YOUR ANSWERS HERE Keeping b=0.01 and d=0.12, calculate the numeric solution of the ODE with different time steps: dt = 1, 5, 10. Plot the solutions and report how they behave for larger values of the time step. What time step would you consider optimal for this problem? #YOUR CODE HERE YOUR ANSWERS HERE Use your FE script to compute and plot the numerical solution of the ODE for b=0.5, d=0.12, with dt=0.1 over the span of 0 to 50 years. Calculate the analytic solution (see section 15.2) from the same time vector (0 to 50 with time step 0.1) and overlay the analytic solution on the same plot. Calculate and plot the vector of errors (absolute difference between the analytic and numerical solution) and plot it over time. Describe how the error behaves in this plot. #YOUR CODE HERE YOUR ANSWERS HERE Keeping the same parameters b=0.5 and d=0.12, calculate the mean error of the numerical solution by using the mean function on the absolute difference between the analytic solution vector and the numerical solution vector, for three values of the time step: dt=1, 0.1, 0.01 and Tmax=50. Report how the reduction in the error compares with each decrease in the time step. #YOUR CODE HERE YOUR ANSWERS HERE Use the birth rate to b=0.01, keep d=0.12 and again calculate and plot the numerical solution along with the analytic solution for the same values of Tmax and dt. Calculate and plot the vector of errors (absolute difference between the analytic and numerical solution) and plot it over time in a separate plot. Describe how the error behaves in this plot. #YOUR CODE HERE YOUR ANSWERS HERE Calculate the mean error of the numerical solution for three values of the time step: dt=1, 0.1, 0.01 with Tmax=50. Report how the reduction in the error compares with each decrease in the time step. #YOUR CODE HERE YOUR ANSWERS HERE "],["graphical-analysis-of-ordinary-differential-equations.html", "7 Graphical analysis of ordinary differential equations 7.1 Building nonlinear ODEs 7.2 Qualitative analysis of ODEs 7.3 Functions in R 7.4 Modeling the spread of infectious disease spread 7.5 R Assignment", " 7 Graphical analysis of ordinary differential equations I find the great thing in this world is not so much where we stand, as in what direction we are moving. – Oliver Wendell Holmes, Sr., The Autocrat of the Breakfast Table We now proceed from linear ODEs to more complicated nonlinear equations. In contrast to linear differential equations, which can be solved in general, nonlinear differential equations may not be solvable even theoretically. Even though the solutions cannot be written down, they exist and can exhibit much more interesting behaviors than the exponential solutions we have seen. When solutions cannot be found on paper, we have two options: 1) use qualitative or graphical tools, such as finding equilibrium points and their stability, to predict the long-term dynamics of the solution; 2) construct numerical solutions that approximate the true solution. In this chapter we concentrate on the qualitative approach to analyzing ODEs, which allows one to predict the behavior of solutions of any autonomous ODE based on the graph of the defining function of the equation. After working through this chapter you will learn to do the following: find equilibrium values of an ODE analyze the stability of equilibria based on the graph of the defining function write down stability conditions analytically use graphical techniques to predict the behavior of the solution of a difference equation without solving it understand basic compartment epidemiology models 7.1 Building nonlinear ODEs The simple, linear population growth models we have seen in the last two chapters assume that the per capita birth and death rates are constant, that is, they stay the same regardless of population size. The solutions for these models either grow or decay exponentially, but in reality, populations do not grow without bounds. It is generally true that the larger a population grows, the more scarce the resources, and survival becomes more difficult. For larger populations, this could lead to higher death rates, or lower birth rates, or both. How can we incorporate this effect into a quantitative model? We will assume there are separate birth and death rates, and that the birth rate declines as the population grows, while the death rate increases. Suppose there are inherent birth rates \\(b\\) and \\(d\\), and the overall birth and death rates \\(B\\) and \\(D\\) depend linearly on population size \\(P\\): \\(B = b - aP\\) and \\(D = d + cP\\). To model the rate of change of the population, we need to multiply the rates \\(B\\) and \\(D\\) by the population size \\(P\\), since each individual can reproduce or die. Also, since the death rate \\(D\\) decreases the population, we need to put a negative sign on it. The resulting model is: \\[ \\dot P = BP - DP = [(b-d)-(a+c)P]P \\] The parameters of the model, the constants \\(a,b,c,d\\), have different meanings. Performing dimensional analysis, we find that \\(b\\) and \\(d\\) have the dimensions of \\(1/[t]\\), the same as the rate \\(r\\) in the exponential growth model. However, the dimensions of \\(a\\) (and \\(c\\)) must obey the relation: \\([P]/[t] = [a][P]^2\\), and thus, \\[[a]=[c] = \\frac{1}{[t][P]}\\] This shows that the constants \\(a\\) and \\(c\\) have to be treated differently than \\(b\\) and \\(d\\). Let us define the inherent growth rate of the population, to be \\(r_0=b-d\\) (if the death rate is greater than the birth rate, the population will inherently decline). Then let us introduce another constant \\(K\\), such that \\((a+c)=r_0/K\\). It should be clear from the dimensional analysis that \\(K\\) has units of \\(P\\), population size. Now we can write down the logistic equation in the canonical form: \\[\\begin{equation} \\dot P = r\\left(1-\\frac{P}{K}\\right)P \\label{eq:log_cont_model} \\end{equation}\\] This model can be re-written as \\(\\dot P = aP -bP^2\\), so it is clear that there is a linear term (\\(aP\\)) and a nonlinear term (\\(-bP^2\\)). When \\(P\\) is sufficiently small (and positive) the linear term is greater, and the population grows. When \\(P\\) is large enough, the nonlinear term wins and the population declines. It should be apparent that there are two fixed points, at \\(P=0\\) and at \\(P=K\\). The first one corresponds to a population with no individuals. On the other hand, \\(K\\) signifies the population at which the negative effect of population size balances out the inherent population growth rate, and is called the carrying capacity of a population in its environment . We will analyze the qualitative behavior of the solution, without writing it down, in the next section of this chapter. 7.2 Qualitative analysis of ODEs In this section we will analyze the behavior of solutions of an autonomous ODE without solving it on paper. Generally, ODE models for realistic biological systems are nonlinear, and most nonlinear differential equations cannot be solved analytically. We can make predictions about the behavior, or dynamics of solutions by considering the properties of the defining function, which is the function on the right-hand-side of a general autonomous ODE: \\[ \\frac{dx}{dt} = f(x)\\] 7.2.1 graphical analysis of the defining function The defining function relates the value of the solution variable \\(x\\) to its rate of change \\(dx/dt\\). For different values of \\(x\\), the rate of change of \\(x(t)\\) is different, and it is defined by the function \\(f(x)\\). There are only three options: if \\(f(x) &gt; 0\\), \\(x(t)\\) is increasing at that value of \\(x\\) if \\(f(x) &lt; 0\\), \\(x(t)\\) is decreasing at that value of \\(x\\) if \\(f(x) = 0\\), \\(x(t)\\) is not changing that value of \\(x\\) To determine for which values of \\(x\\) the solution \\(x(t)\\) increases and decreases, it enough to look at the plot of \\(f(x)\\). On the intervals where the graph of \\(f(x)\\) is above the \\(x\\)-axis \\(x(t)\\) increases, on the intervals where the graph of \\(f(x)\\) is below the \\(x\\)-axis, \\(x(t)\\) decreases. The roots (zeros) of \\(f(x)\\) are special cases, they separate the range of \\(x\\) into the intervals where the solution grows and and where it decreases. This seems exceedingly simple, and it is, but it provides specific information about \\(x(t)\\), without knowing how to write down its formula. For an autonomous ODE with one dependent variable, the direction of the rate of change prescribed by the differential equation can be graphically represented by sketching the flow on the line of the dependent variable. The flow stands for the direction of change at every point, specifically increasing, decreasing, or not changing. The flow is plotted on the horizontal x-axis, so if \\(x\\) is increasing, the flow will be indicated by a rightward arrow, and if it is decreasing, the flow will point to the left. The fixed points separate the regions of increasing (rightward) flow and decreasing (leftward) flow. Figure 7.1: a) plot of the defining function of the ODE dx/dt = 4x-100 with direction of flow x(t) indicated with arrows on the x-axis; b) plot of solutions x(t) of the ODE staring with four initial values. Example. Consider a linear ODE the likes of which we have solved in section : \\[\\frac{dx}{dt} = 4x -100\\] The defining function is a straight line vs. \\(x\\), its graph is shown in figure a. Based on this graph, we conclude that the solution decreases when \\(x&lt;25\\) and increases when \\(x&gt;25\\). Thus we can sketch the solution \\(x(t)\\) over time, without knowing its functional form. The dynamics depends on the initial value: if \\(x(0)&lt;25\\), the solution will keep decreasing without bound, and go off to negative infinity; if \\(x(0)&gt;25\\), the solution will keep decreasing without bound, and go off to positive infinity. This is shown by plotting numerical solutions of this ODE for several initial values in figure b. The dotted line shows the location of the special value of 25 which separates the interval of growth from the interval of decline. Example. Now let us analyze a nonlinear ODE, specifically the logistic model with the following parameters: \\[\\frac{dP}{dt} =0.3P\\left(1-\\frac{P}{40}\\right)\\] The defining function is a downward-facing parabola with two roots at \\(P=0\\) and \\(P=40\\), as shown in figure a. Between the two roots, the defining function is positive, which means the derivative \\(dP/dt\\) is positive too, so the solution grows on that interval. For \\(P&lt;0\\) and \\(P&gt;40\\), the solution decreases. Therefore, we can sketch the graphs of the solution \\(P(t)\\) starting with different initial conditions, as show in figure b. To summarize, the defining function of the ODE determines the rate of change of the solution \\(x(t)\\) depending on the value of \\(x\\). The graphical approach to finding areas of right and left flow is based on graphing the function \\(f(x)\\), and dividing the x-axis based on the sign of \\(f(x)\\). In the areas where \\(f(x) &gt; 0\\), its graph is above the x-axis, and the flow is to the right; conversely, when \\(f(x) &lt; 0\\), its graph is below the x-axis, and the flow is to the left. The next subsection puts this approach in a more analytic framework. Figure 7.2: a) plot of the defining function of the ODE dP/dt = 0.3P(1-P/40) with direction of flow of P(t) indicated with arrows on the P-axis; b) plot of solutions P(t) of the ODE staring with three initial values. 7.2.2 fixed points and stability We have seen that the dynamics of solutions of differential equations depend on the initial value of the dependent variable: for some values the solution increases, for others it decreases, and for intermediate values it remains the same. Those special values separating intervals of increase and decrease are called fixed points (or equilibria), and the first step to understanding the dynamics of an ODE is finding its fixed points. A fixed point is a value of the solution at which the dynamical system stays constant, thus, the derivative of the solution must be zero. Here is the formal definition: Definition 7.1 For an ordinary differential equation \\(\\dot x = f(x)\\), a point \\(x^*\\) which satisfies \\(f(x^*)=0\\) is called a fixed point or equilibrium, and the solution with the initial condition \\(x(0)=x^*\\) is constant over time \\(x(t)=x^*\\). Example. The linear equation \\(\\dot x = rx\\) has a single fixed point at \\(x^* = 0\\). For a more interesting example, consider a logistic equation: \\(\\dot x = x - x^2\\). Its fixed points are the solutions of \\(x - x^2 = 0\\), therefore there two fixed points: \\(x^* = 0, 1\\). We know that if the solution has either of the fixed points as the initial condition, it will remain at that value for all time. Locating the fixed points is not sufficient to predict the global behavior of the dynamical system, however. What happens to the solution of a dynamical system if the initial condition is very close to an equilibrium, but not precisely at it? Put another way, what happens if the equilibrium is perturbed? The solution may be attracted to the equilibrium value, that is, it approaches it ever-closer, or else it is not. In the first case, this is called a stable equilibrium, because a small perturbation does not dramatically change the long-term behavior of the solution. In the latter case, the equilibrium is called unstable, and the solution perturbed from the equilibrium never returns. These concepts are formalized in the following definition Definition 7.2 A fixed point \\(x^*\\) of an ODE \\(\\dot x = f(x)\\) is called a stable fixed point (or sink) if for a sufficiently small number \\(\\epsilon\\), the solution \\(x(t)\\) with the initial condition \\(x_0 = x^* + \\epsilon\\) approaches the fixed point \\(x^*\\) as \\(t \\rightarrow \\infty\\). If the solution \\(x(t)\\) does not approach \\(x^*\\) for all nonzero \\(\\epsilon\\), the fixed point is called an unstable fixed point (or source). To determine whether a fixed point is stable analytically we use the approach called linearization, which involves replacing the function \\(f(x)\\) with a linear approximation. Let us define \\(\\epsilon(t)\\) to be the deviation of the solution \\(x(t)\\) from the fixed point \\(x^*\\), so we can write \\(x(t) = x^* + \\epsilon(t)\\). Assuming that \\(\\epsilon(t)\\) is small, we can write the function \\(f(x)\\) using Taylor’s formula: \\[ f(x^*+\\epsilon(t))= f(x^*)+f&#39;(x^*) \\epsilon(t) + ... = f&#39;(x^*) \\epsilon(t) + ... \\] The term \\(f(x^*)\\) vanished because it is zero by definition of a fixed point. The ellipsis indicates all the terms of order \\(\\epsilon(t)^2\\) and higher, which are very small if \\(\\epsilon(t)\\) is small, and thus can be neglected. Thus, we can write the following approximation to the ODE \\(\\dot x = f(x)\\) near a fixed point: \\[ \\dot x = \\frac{d(x^* + \\epsilon(t))}{dt} = \\dot \\epsilon(t) = f&#39;(x^*) \\epsilon(t)\\] Thus we replaced the complicated nonlinear ODE near a fixed point with a linear equation, which approximates the dynamics of the deviation \\(\\epsilon(t)\\) near the fixed point \\(x^*\\); note that the derivative \\(f&#39;(x^*)\\) is a constant for any given fixed point. In section we classified the behavior of solutions for the general linear ODE \\(\\dot x = rx\\), and now we apply this classification to the behavior of the deviation \\(\\epsilon(t)\\). If the multiple \\(f&#39;(x^*)\\) is positive, the deviation \\(\\epsilon(t)\\) is growing, the solution is diverging away from the fixed point, and thus the fixed point is unstable. If the multiple \\(f&#39;(x^*)\\) is negative, the deviation \\(\\epsilon(t)\\) is decaying, the solution is converging to the fixed point, and thus the fixed point is stable. Finally, there is the borderline case of \\(f&#39;(x^*) = 0\\) which is inconclusive, and the fixed point may be either stable or unstable. The derivative stability analysis is summarized in the following: \\(f&#39;(x^*) &gt; 0\\): the slope of \\(f(x)\\) at the fixed point is positive, then the fixed point is . \\(f&#39;(x^*) &lt; 0\\): the slope of \\(f(x)\\) at the fixed point is negative, then the fixed point is . \\(f&#39;(x^*) = 0\\): stability cannot be determined from the derivative. Therefore, knowing the derivative or the slope of the defining function at the fixed point is enough to know its stability. If the derivative has the discourtesy of being zero, the situation is tricky, because then higher order terms that we neglected make the difference. We will mostly avoid such borderline cases, but they are important in some applications . A word of caution: The derivative of the defining function \\(f&#39;(x)\\) is not the second derivative of the solution \\(x(t)\\). This is a common mistake, because the function \\(f(x)\\) is equal to the time derivative of \\(x(t)\\). However, the derivative \\(f&#39;(x)\\) is not with respect to time, it is with respect to x, the dependent variable. In other words, it reflects the slope of the graph of the defining function \\(f(x)\\), not the curvature of the graph of the solution \\(x(t)\\). To summarize, here is an outline of the steps for analyzing the behavior of solutions of an autonomous one-variable ODE. These tasks can be accomplished either by plotting the defining function \\(f(x)\\) and finding the fixed points and their stability based on the plot, or by solving for the fixed points on paper, then finding the derivative \\(f&#39;(x)\\) and plugging in the values of the fixed points to determine their stability. Either approach is valid, but the analytic methods are necessary when dealing with models that have unknown parameter values, which makes it impossible to represent the defining function in a plot. 7.2.3 Outline of qualitative analysis of an ODE find the fixed points by setting the defining function \\(f(x)=0\\) and solving for values of \\(x^*\\) divide the domain of \\(x\\) into intervals separated by fixed points \\(x^*\\) determine on which interval(s) the solution \\(x(t)\\) is increasing and on which it is decreasing use derivative stability analysis (graphically or analytically) to determine which fixed points are stable sketch the solutions \\(x(t)\\) starting at different initial values, based on the stability analysis and whether the solution is increasing or decreasing in a particular interval Example: linear model. Consider the linear ODE that we analyzed above \\(dx/dt = 4x -100\\). Let us go through the steps of qualitative analysis: find the fixed points by setting the defining function to 0: \\(0 = 4x -100\\), so there is only one fixed point \\(x^* = 25\\) divide the domain of \\(x\\) into intervals separated by fixed points \\(x^*\\): the intervals are \\(x&lt;25\\) and \\(x&gt;25\\) the solution is decreasing on the interval \\(x&lt;25\\) because \\(f(x)&lt;0\\) there, and the solution is increasing on the interval \\(x&gt;25\\) because \\(f(x)&gt;0\\) the derivative \\(f&#39;(x)\\) at the fixed point is 4, so the fixed point is unstable solutions \\(x(t)\\) starting at different initial values are shown in figure b and they behave as follows: solutions with initial values below \\(x^*=25\\) decreasing, and those with initial values above \\(x^*=25\\) increasing. ** Example: logistic model.** Consider the logistic model from the previous subsection, \\(dP/dt =0.3P(1-P/40)\\). We have analyzed the stability of the two fixed points using the plot in figure , and saw that the flow takes the solution away from \\(P=0\\), and toward \\(P=K\\), thus the first fixed point is unstable, while the second is stable. Let us repeat the analysis using analytic tools: find the fixed points by setting the defining function to 0: \\(0 = 0.3P(1-P/40)\\). The two solutions are \\(P^*=0\\) and \\(P^*=40\\). divide the domain of \\(P\\) into intervals separated by fixed points \\(P^*\\): the intervals are \\(P&lt;0\\); \\(0&lt;P&lt;40\\); and \\(P&gt;40\\) the solution is decreasing on the interval \\(P&lt;0\\) because \\(f(P)&lt;0\\) there, the solution is increasing on the interval \\(0&lt;P&lt;40\\) because \\(f(P)&gt;0\\), and the solution is decreasing for \\(P&gt;40\\) because \\(f(P)&lt;0\\) there the derivative is \\(f&#39;(P)=0.3-0.3P/20\\); since \\(f&#39;(0)=0.3 &gt; 0\\), the fixed point is unstable; since \\(P&#39;(40)=-0.3&lt;0\\), the fixed point is stable solutions \\(P(t)\\) starting at different initial values are shown in figure b and they behave as follows: solutions with initial values below \\(P^*=0\\) decreasing, those with initial values between 0 and 40 are increasing and asymptotically approaching 40, and those with initial values above 40 decreasing and asymptotically approaching 40. This can be done more generally using the derivative test: taking the derivative of the function on the right-hand-side (with respect to \\(P\\)), we get \\(f&#39;(P) = r(1-2\\frac{P}{K})\\). Assuming \\(r&gt;0\\) (the population is viable), \\(f&#39;(0)= r\\) is positive, and the fixed point is therefore unstable. This makes biological sense, since we assumed positive inherent population growth, so given a few individuals, it will increase in size. On the other hand, \\(P&#39;(K) = r(1-2) = -r\\), so this fixed point is stable. Thus, according to the logistic model, a population with a positive inherent growth rate will not grow unchecked, like in the exponential model, but will increase until it reaches its carrying capacity, at which it will stay (if all parameters remain constant). Example: semi-stable fixed point. Consider the ODE \\(dx/dt = -x^3 + x^2\\), whose defining function is plotted in figure a, showing two fixed points at \\(x = 0, 1\\). find the fixed points by setting the defining function to 0: \\(0 = -x^3 + x^2\\). The two fixed points are \\(x^*=0\\) and \\(x^*=1\\). divide the domain of \\(x\\) into intervals separated by fixed points \\(x^*\\): the intervals are \\(x&lt;0\\); \\(0&lt;x&lt;1\\); and \\(x&gt;1\\) the solution is increasing on the interval \\(x&lt;0\\) because \\(f(x)&gt;0\\) there, the solution is increasing on the interval \\(0&lt;x&lt;1\\) because \\(f(x)&gt;0\\), and the solution is decreasing for \\(x&gt;1\\) because \\(f(x)&lt;0\\) there the derivative is \\(f&#39;(x)=-3x^2+2x\\); since \\(f&#39;(0)=0\\), the fixed point is undetermined; since \\(f&#39;(1)=-1&lt;0\\), the fixed point is stable. the solutions \\(x(t)\\) starting at different initial values are shown in figure b, and they behave as follows: solutions with initial values below 0 are increasing and asymptotically approaching 0, those with initial values between 0 and 1 are increasing and asymptotically approaching 1, and those with initial values above 1 are decreasing and asymptotically approaching 1. This example shows how graphical analysis can help when derivative analysis is undetermined. The red arrows on the x-axis of figure show the direction of the flow in the three different regions separated by the fixed points. Flow is to the right for \\(x&lt;1\\), to the left for for \\(x&gt;1\\); it is clear that the arrows approach the fixed point from both sides, and thus the fixed point is stable, as the negative slope of \\(f(x)\\) at \\(x=1\\) indicates. One the other hand, the fixed point at \\(x=0\\) presents a more complicated situation: the slope of \\(f(x)\\) is zero, and the flow is rightward on both sides of the fixed point. This type of fixed point is sometimes called semi-stable, because it is stable when approached from one side, and unstable when approached from the other. Figure 7.3: a) plot of the defining function of the ODE dx/dt = -x^3 + x^2 with direction of flow of x(t) indicated with arrows on the x-axis; b) plot of solutions x(t) of the ODE staring with three initial values.. 7.2.4 Exercises For the following differential equations: a) plot the defining function over the indicated range (use any computational tools you wish) to determine the intervals on which the dependent variable is increasing and decreasing; b) find the equilibria c) determine the stability of each equilibrium; d) based on your analysis in parts a-c, sketch (by hand) plots of the solutions with the specified initial values. \\[ \\frac{dC}{dt} = -0.2C + 60; \\; C \\in (0,500); \\; C(0) = 200; \\; C(0) = 400 \\] \\[ \\frac{dP}{dt} = 0.01P(800-P) - 0.5P; \\; P \\in (-1, 1000); \\; P(0) = 100; \\; P(0) = 800\\] \\[ \\frac{dR}{dt} = R(80-R) - 1200; \\; R \\in (-1, 100); \\; R(0) = 10; \\; R(0) = 80\\] \\[ \\frac{dI}{dt} = 0.1I(1-I) - 0.03I; \\; I \\in (-0.1, 1.1); \\; I(0) = 0.2; \\; I(0) = 0.9 \\] \\[ \\frac{dR}{dt} = \\frac{R}{1+R}-0.1R; \\; R \\in (-0.1, 10); \\; R(0) = 20; \\; R(0) = 0 \\] \\[ \\frac{dP}{dt} = 0.02P(P-100)(1200-P) \\; P \\in (-0.1, 1200); \\; P(0) = 20; \\; P(0) = 1000 \\] \\[ \\frac{dY}{dt} = 0.01Y(Y-100)(Y-200) \\; Y \\in (-0.1, 300); \\; Y(0) = 20; \\; Y(0) = 250 \\] (harder) The logistic function was defined in chapter 2, equation 2.4. Verify that the logistic function with independent variable \\(t\\) solves the logistic ODE in equation and relate the parameters in the function to the parameters \\(r\\) and \\(K\\) in the ODE. 7.3 Functions in R Like most programming languages, R allows one to define and use structures called functions. Some are already written and loaded into the R distribution, for example, the function mean() we use to compute the mean of a vector variable, while others can be defined by users. Functions are discrete chunks of code that can be called from the outside to perform some task. The function receives inputs from the call and returns the result back. Here is the general structure of a function in R: myfunction &lt;- function(arg1, arg2, ...) { statements return(answer) } A function is a piece of code that is defined separately and can be called by other pieces of code. The main purpose is to create a “black box” that does a specific job and can be used repeatedly just by calling the function (invoking its name), rather than copying the code repeatedly. A function generally has input variables (although sometimes there are none) and returns an output using the return() statement. It is important to distinguish between the inside of the function - the code between the curly braces in the function definition - and the outside, that is everything else. The inputs are passed to the function in the call (through the parentheses) and then used inside the function to do its business and produce an output, which is then returned back to the place in the code where the function was called. 7.3.1 defining a function Here is an example of a function definition, with input variables N and r. Between the curly braces is the body of the function, which in this case multiplies the two input variables and then returns them. my_funk &lt;- function(N,r){ ans &lt;- r*N # updating function f(N) return(ans) } Note that after running the code chunk above, you should see the name my_funk in your environment (under Functions). This means this function is defined in memory and ready to be called. 7.3.2 calling a function After a function is defined, it is ready to be called (executed) by invoking its name and giving the correct number of inputs. Here’s an example of a function call: a &lt;- 30 y &lt;- 1:10 print(my_funk(y, a)) ## [1] 30 60 90 120 150 180 210 240 270 300 Notice that the variable names in the fuction call do not have to be same as what they are called within the function. IMPORTANT: a function uses the order of variables in the function call, called external variables (y, a) to assign their names within the function, called internal variables (N, r). (There is a way to specify which input belongs to which internal variable, e.g. plot(x=time, y=sol) and in that case the order is not important.) 7.3.3 using a function to solve a difference equation We have solved discrete-time dynamic models (difference equations) using for loops. You can use a function to calculate the next value of the solution, by passing the current value and any parameters as inputs to the function, as you can see in the code chunk below: numsteps&lt;-30 # set number of steps sol &lt;- rep(0,numsteps+1) # pre-allocate sol1 sol[1] &lt;- 100 # set initial value r &lt;- 2 # define the multiplicative constant for (i in 1:numsteps) { # repeat for numsteps sol[i+1] &lt;- my_funk(sol[i], r) # calculate the next value } time &lt;- 0:numsteps # define time vector plot(time,sol,t=&#39;b&#39;,xlab=&#39;time&#39;,ylab=&#39;solution&#39;,lwd=2) 7.3.4 Exercises Write a function that takes the input variable and multiplies it by 1.03, like the mathematical function \\(f(x) = 1.03x\\). # YOUR CODE HERE Use the function to take a variable and multiply it by 1.03, replacing the old value of the variable. If the initial value is 5, the new value should be 5.15. # YOUR CODE HERE Write a script to take a variable and multiply it by 1.03 one hundred times, replacing the old value of the variable using a for loop and the function you created. Starting with the initial value is 5, the script should return the value 96.093. # YOUR CODE HERE Modify the script above to save all the intermediate values into a vector, and plot a graph of this vector vs. the iteration step (from 1 to 101). Hint: this is exactly like the example code above. # YOUR CODE HERE 7.4 Modeling the spread of infectious disease spread The field of What effect does changing the recovery rate \\(\\gamma\\) have on the basic reproductive rate? Explain the biological intuition behind this.} Discuss what assumptions are made by using compartment models, and when they might be justified.} Discuss the difference in assumptions in using a Markov model with Susceptible and Infected compartments compared to an ODE model with the same two compartments. Under what circumstances does it make sense to use one or the other?} Read the paper and discuss the strengths and limitations of the more complicated compartment model intended to account for human behavior.} 7.5 R Assignment # Define the function: DefFunk &lt;- function (x,a,b){ ans &lt;- a*x^2-b return(ans) } a &lt;- 1 b &lt;- 4 x&lt;-seq(-5,5,0.1) # define the array of values of x y&lt;-DefFunk(x,a,b) # calculate the values of y using a function plot(x,y,t=&quot;l&quot;,xlab=&quot;x&quot;, ylab=&quot;dx/dt&quot;) # plot the function abline(0,0) # draw a line for y=0 Notice that I used two input variables into the defining function (a and b) as parameters that can be changed outside the function. This is good coding practice - the best way to use a function is by changing its inputs, not monkeying with its internal code. After qualitative analysis, you will check whether solutions behave in the predicted way by running Forward Euler to calculate the numerical solution. 7.5.1 logistic population model This is the logistic model of population growth, with P representing population size, r is the growth rate parameter, and K is the carrying capacity parameter; P is measured in thousands, and time is measured in years. \\[ \\frac{dP}{dt} = rP(1-P/K) \\] For the values of r=0.3 and K=40, plot the graph of the defining function (right-hand side of the ODE) in R over some interval that includes all zeros of the function. Based on the graph, find the equilibria of the ODE, determine their stability, and predict the behavior of the solution for the following initial values: P(0) = 1; P(0) = 39; P(0) = 20. #YOUR CODE HERE YOUR ANSWERS HERE Uses the Forward Euler method to calculate the numeric solution of the logistic ODE for any given values of r and K (they should be inputs into your defining function). Use this script to solve the logistic ODE with parameters r=0.3 and K=40, with a small time step (e.g. dt=0.1) and the following initial values: P(0) = 1; P(0) = 39; P(0) = 20. Calculate the numerical solutions of the ODE over a sufficiently large time Tmax to observe convergence, and plot all three in the same figure. Do the solution plots look consistent with your prediction in question a? #YOUR CODE HERE YOUR ANSWERS HERE For the values of the parameters r=0.02 and K=120, plot the graph of the defining function (right-hand side of the ODE) in R over some interval that includes all zeros of the function. Based on the graph, find the equilibria of the ODE, determine their stability, and predict the behavior of the solution for the following initial values: P(0) = 10; P(0) = 50; P(0) = 150. #YOUR CODE HERE YOUR ANSWERS HERE Use the Forward Euler method to calculate the numeric solution of the logistic ODE with parameters r=0.02 and K=120 with a small time step (e.g. dt=0.1) and the following initial values: P(0) = 10; P(0) = 50; P(0) = 150. Plot the numeric solutions of the ODE over a sufficiently large time Tmax to observe convergence. Do the solution plots look consistent with your prediction in question a? #YOUR CODE HERE YOUR ANSWERS HERE Let r=0.3 and K=40 and start at P(0)=1. Gradually increase dt until you see the solution behaving differently than expected. Report at what value of dt this strange behavior begins, investigate what happens for even larger time steps, and describe what happens to the solutions. #YOUR CODE HERE YOUR ANSWERS HERE 7.5.2 SIS model of infectious disease The following ODE is a simple model of an infectious epidemic with only two kinds of individuals: susceptible and infected, and where the total population size stays the same. The variable I is the fraction of individuals in the population who are infected, and the parameters beta and gamma are the infection and recovery rates, respectively; time is measured in days. \\[ \\frac{dI}{dt} = \\beta I(1-I) - \\gamma I \\] For the values of beta = 0.4 and gamma=0.1 plot the graph of the defining function (right-hand side of the ODE) in R over the interval [0,1]. Based on the graph, find the equilibria of the ODE, determine their stability, and predict the behavior of the solution for the following initial values: I(0) = 0.01; I(0) = 0.8; I(0) = 0.5. #YOUR CODE HERE YOUR ANSWERS HERE Write a Forward Euler script to calculate the numeric solution of the SIS model ODE for any given values of beta and gamma (they should be inputs into your defining function). Use this script to solve the SIS model ODE with parameters beta = 0.4 and gamma=0.1 , with a small time step (e.g. dt=0.1) and the following initial values: I(0) = 0.01, I(0) = 0.5, I(0) = 0.9. Plot the numerical solutions of the ODE over a sufficiently large time Tmax to observe convergence. Report whether the epidemic persists or burns out, whether it depends on the initial value, and comment on whether the solution dynamics agrees with your prediction in question a). #YOUR CODE HERE YOUR ANSWERS HERE For the values of beta=0.1 and gamma=0.2 plot the graph of the defining function (right-hand side of the ODE) in R over the interval [0,1]. Based on the graph, find the equilibria of the ODE, determine their stability, and predict the behavior of the solution for the following initial values: I(0) = 0.01; I(0) = 0.5; I(0) = 0.8. #YOUR CODE HERE YOUR ANSWERS HERE Use the Forward Euler script to solve the SIS model ODE with parameters beta=0.1 and gamma=0.2 with a small time step (e.g. dt=0.1) and the following initial values: I(0) = 0.01; I(0) = 0.8; I(0) = 0.5. Plot the numerical solutions of the ODE over a sufficiently large time Tmax to observe convergence. Report whether the epidemic persists or burns out (converges to zero), how it depends on the initial value, and comment on whether the solution dynamics agrees with your prediction in question a. #YOUR CODE HERE YOUR ANSWERS HERE For I(0)=0.5 and beta=0.1, starting with gamma=0.3, progressively decrease the value of the recovery rate gamma, and report the behavior of the numerical solution (with a small enough dt and a large enough Tmax so you can see the asymptotic behavior). Report the critical value of gamma at which the fraction of infected converges to a positive equilibrium value (the disease does not burn out.) Keep decreasing the recovery rate and report what happens to the equilibrium fraction of infected people. Does it ever get to 1? #YOUR CODE HERE YOUR ANSWERS HERE "],["random-variables-and-distributions.html", "8 Random variables and distributions 8.1 Random variables and distributions 8.2 Examples of distributions 8.3 Random number generators in R", " 8 Random variables and distributions What is there then that can be taken as true? Perhaps only this one thing, that nothing at all is certain. – Rene Descartes Mathematical models can be divided into deterministic and stochastic models. Deterministic models assume that the future can be perfectly predicted based on complete information of the past. Stochastic models instead assume that even perfect knowledge of the past does not allow one to predict the future with certainty. Stochastic models may not sound very promising: after all, we want to make predictions, and randomness says that predictions are impossible! However, the word “random” in mathematics doesn’t mean “completely unpredictable” or “without rules,” as it does in common usage. It means that we can make probabilistic predictions, e.g. compute what fraction of molecules will diffuse from one place to another, or what fraction of genes mutate in one generation - we just can’t make a definite prediction for each individual molecule or gene. Biological processes are so complex and are subject to so much environmental noise, that stochastic models are absolutely essential for our understanding of many living systems. Here is what you will learn to do in this chapter: define probability in terms of outcomes and events know what is a random variable and its distribution compute means and variances of distributions use the binomial distribution to model strings of binary trials generate random numbers in R 8.1 Random variables and distributions 8.1.1 definition of probability In this section we will develop the terminology used in the mathematical study of randomness called probability. This begins with a random experiment which is a very broad term that can describe any natural or theoretical process whose outcome cannot be predicted with certainty. If the outcomes are numeric, they may be discrete (can be counted by integers) or continuous (corresponding to real numbers); they may also be categorical, meaning that they do not have a numeric meaning, like eye color. We will stick to experiments that have discrete outcomes in this chapter, but many important experiments produce continuous outcomes. The first step for studying a random process is to describe all of the outcomes it can produce: Definition 8.1 The collection of all possible outcomes of an experiment is called its sample space \\(\\Omega\\). An event is a subset of the sample space, which means an event may contain one or more experimental outcomes. An illustration of the sample space of all people with two events: tall people and those who like tea. Example. You can ask a person two questions: how tall are you (and classify them either as short or tall) and do you like tea (yes or no), and you’ve performed a random experiment. The randomness comes not from the answers (assuming the person doesn’t randomly lie) but from the selection of the respondent. We will discuss randomly selecting a sample from a population in the next chapter. This random experiment has four outcomes: tall person who likes tea, tall person who does not like tea, short person who likes tea, and short person who does not like tea. This sample space and events is illustrated in figure with a Venn diagram, which uses geometric shapes as representations of events as subsets of the entire sample space. These outcomes can be grouped into events by one of the responses: e.g. tall person (\\(A\\)) or person who doesn’t like tea (\\(-B\\)). Example. A random experiment with two outcomes, called a Bernoulli trial (after the famous Swiss mathematician), can describe a variety of situations: a coin toss (heads or tails), a competition with two outcomes (win or loss), the allele of a gene (normal or mutant). The sample space for a single Bernoulli trial consists of just two outcomes: \\(\\{H,T\\}\\) (for a coin toss). If the experiment is performed repeatedly, the sample space gets more complicated. For two Bernoulli trials there are four different outcomes \\(\\{HH, HT, TH, TT \\}\\). One can define different events for this sample space: the event of getting two heads in two tosses contains one outcome: \\(\\{HH\\}\\), the event of getting a single head contains two: \\(\\{TH, HT\\}\\). In order to to describe the composition of a sample space, we need to define the word probability . While it is familiar to everyone from everyday usage, it is difficult to define without using other similar words, such as likelihood or plausibility, which are also in need of definition. It is accepted that something with a high probability happens often, while something with a low frequency is seldom observed. The other notion is that probability can range between 0 (meaning something that never occurs) and 1 (something that occurs every time). These notions lead to the commonly accepted definition: Definition 8.2 The probability of an outcome or event in the sample space of a random experiment is the fraction of experiments with this outcome out of many repeated experiments. This definition is at the heart of the frequentist view of probability, due to the underlying assumption that the experiment can be repeated as many times as necessary to observe the frequency of outcomes. There is an alternative view that focuses on what is previously known about the experiment (or about systems that produce that kind of experiment) that is called the Bayesian view: Definition 8.3 The probability of an outcome or event in the sample space of a random experiment is the degree of certainty or belief that this outcome will occur based on prior experience. We will investigate the Bayesian approach in chapter 12. Most of traditional probability and classical statistics is based on the frequentist view, as it grew out of attempts to understand games of chance, like cards and dice, which can be easily repeated, or simple experiments like those in agriculture, where many plots can be planted and observed. These easily repeatable simple experiments can be described with mathematical distributions that we will describe in this chapter. However, many contemporary research problems are not so easily repeated, and often require a Bayesian approach that does not yield to neat mathematical description and can be addressed using computation. 8.1.2 axioms of probability One we have defined the probability of an outcome, one can calculate the probability of a collection of outcomes according to rules that ensure the results are self-consistent. These rules are called the axioms of probability: Definition 8.4 The probability \\(P(A)\\) of an event \\(A\\) in a sample space \\(\\Omega\\) is a number between 0 and 1, which obeys the following rules, called the axioms of probability: \\(P(\\Omega) = 1\\) \\(P(\\emptyset) = 0\\) \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\) Let us define some notation for sets: \\(A \\cup B\\) is called the union of two sets, which contains all outcomes that belong to either \\(A\\) or \\(B\\), this is equivalent to the logical OR operator because it is true if either A or B is true. \\(A\\cap B\\) is called the intersection of two sets, which contains all outcomes that are in both \\(A\\) and \\(B\\), this is is equivalent to the logical AND operator because it is true if both A and B are true. The \\(\\emptyset\\) denotes the empty set. Any event \\(A\\) has its complement, denoted \\(-A\\), which contains all outcomes of \\(\\Omega\\) which are not in \\(A\\). Applying them to the sample space and events in figure , the union of the two sets \\(A \\cup B\\) are all people who are either tall or like tea, the intersection of the two sets \\(A\\cap B\\) are all the tall people who like tea, and the intersection of the first set with the complement of the second \\(A \\cup - B\\) are all tall people who do not like tea. An illustration of the operation of intersection of sets A and B. An illustration of the operation of the union of sets A and B. An illustration of the intersection of A with -B The first two axioms connect easily with our intuition about probability: the first axiom says that the probability of some outcome from the sample space occurring is 1, while the second says that the probability of nothing in the sample space occurring is 0. The intuition behind axiom three is less transparent, but it can be see in a Venn diagram of two subsets \\(A\\) and \\(B\\) of the larger set \\(\\Omega\\), as in figure . Compare the size of the union of \\(A\\) and \\(B\\) and the sum of the sizes of sets \\(A\\) and \\(B\\) separately, and you will see that the intersection \\(A\\cap B\\) occurs in both \\(A\\) and \\(B\\), but is only counted once in the union. This is why it needs to be subtracted from the sum of \\(P(A)\\) and \\(P(B)\\). There are several useful rules that immediately follow from the axioms. First, if two events are mutually exclusive, meaning their intersection is empty (\\(A\\cap B = \\emptyset\\)), then the probability of either of them happening is the sum of their respective probabilities: \\(P(A \\cup B) = P(A) + P(B)\\) (from axiom 3). Further, since an event \\(A\\) and its complement \\(-A\\) are mutually exclusive, their union is the entire sample space \\(\\Omega\\): \\(P(A) + P(-A) = P(A \\cup -A) = P(\\Omega) = 1\\), therefore \\(P(A) = 1-P(-A)\\). Example. Assume one is using a fair coin, so the probability of a single head and a single tail is 1/2. The probability of getting two heads in a row is 1/4, because exactly half of those coins that come up heads once will come up heads again. In fact, the probability of getting any particular sequence of two coin toss results is 1/4. Here are some examples of what we can calculate: the probability of getting one head of out of two tosses is \\(1-1/4-1/4=1/2\\) (by the complement rule). the probability of getting two heads is \\(1-1/4 = 3/4\\) (by the complement rule). the probability of getting either 0, 1, or 2 heads is 1 (by axiom 1). the probability of getting three heads is 0 (since this event is not in the sample space). Example. Suppose one is testing people for a mutation which has the probability (prevalence) of 0.2 in the population, so for each person there are two possible outcomes: normal or mutant. The probability of drawing two mutants in a row is \\(0.2*0.2=0.04\\) by the same argument as above; the probability of drawing two normal people is \\(0.8*0.8 =0.64\\). Based on this, we can calculate the following the probability of one mutant of out two people is \\(1-0.04-0.64=0.32\\) (by the complement rule). the probability of not having two mutants is \\(1-0.04 = 0.96\\) (by the complement rule). the probability of either 0, 1, or 2 mutants is 1 (by axiom 1). the probability of getting three mutants is 0 (since this event is not in the sample space). Example (from Danny and Gaines Sarcastic fringeheads are a tropical ocea fish that engage in aggressive mouth-wrestling matches for their rocky residences. Let us treat each match as a stochastic experiment with two outcomes: win or loss. Then the sample space is equivalent to our coin-tossing experiment, e.g. for two matches the sample space is \\(\\{ WW, WL, LW, LL \\}\\). However, the probability distribution may different, for example if a particular fringehead wins 3/4 of its matches, then the probability distribution would be: \\(P(\\{ WW \\}) = 9/16\\), \\(P(\\{ LW \\}) = P(\\{ WL \\}) = 3/16\\), and $ P({ LL }) = 1/16$. Thus, the same sample space may have different probability distributions defined on it. 8.1.3 random variables The outcomes of experiments may be expressed in numbers or words, but we generally need numbers in order to report and analyze results. One can describe this mathematically as a function (recall its definition form section ) that assigns numbers to random outcomes . In practice, a random variable describes the measurement that one makes to describe the outcomes of a random experiment. Definition 8.5 A random variable is a number or category associated to each outcome in a sample space \\(\\Omega\\). This association has to follow the rules of a function as defined in chapter 2. Example. Define the random variable to be the number of heads out of two coin tosses. This random variable will return numbers 0, 1, or 2, corresponding to different events. The random variable of the number of mutants out of two people (assuming there are only two outcomes, mutant and normal) has the same set of values. This random variable is a function on the sample space because it returns a unique value for each outcome. Example. (Danny and Gaines) Suppose that our sarcastic fringehead, upon losing a wrestling match, has to search for another home for three hours. Then we can define the random variable of time wasted over two wrestling matches, which can be either 0, 3, or 6 hours, depending on the events defined above. Once again, this is a function because there is an unambiguous number associated with each outcome. A random variable has a set of possible values, and each of those values may come up more or less frequently in an random experiment. The frequency of each measurement corresponds to the probability of the outcomes in the sample space that produce that particular value of the random variable. One can describe the behavior of the random variable in terms of the collection of the probabilities of its outcomes. Definition 8.6 The probability of a random variable \\(X\\) taking some value \\(a\\), written as \\(P(X=a)\\), but usually simplified to \\(P(a)\\) is the probability of the event corresponding to the value \\(a\\) of the random variable. This function \\(P(a)\\) is called the probability distribution of the random variable \\(X\\). One important property of probability distribution functions for a discrete random variable is that all of its values have to add up to 1: \\[\\sum_{i=1}^N P(a_i) =1\\] The graph of a probability distribution function lies above zero because all probabilities are between 0 and 1. The graph of a probability distribution is very similar to a histogram, in that it represents the frequency of occurrence of each value of the random variable. A histogram of a variable from a data set can be thought is an approximation of the true probability distribution based on the sample. For a large sample size, the histogram approaches the graph of the probability distribution function, something which we will discuss in chapter 9. Example. Assuming that each coin toss has probability 1/2 of resulting in heads, the probability distribution function for the number of heads out of two coin tosses is \\(P(0) = 1/4; \\; P(1) = 1/2; \\; P(2) = 1/4\\) (as we computed in the example in the previous section). Note that the probabilities add up to 1, as they should. Example. For the random variable of the number of mutants out of two people, for mutation prevalence of 0.2, the probability distribution function is \\(P(0) = 0.64; \\; P(1) = 0.32; \\; P(2) = 0.04\\) (as we computed in the example in the previous section). Note that the probabilities add up to 1, as they should. Example. For the time wasted by a fringehead, the distribution is \\(P(0)= 9/16; \\; P(3) = 3/16; \\; P(6) = 1/16\\). Note that other values of the random variable have probability 0, because they correspond to the empty set in sample space. 8.1.4 expectation of random variables Definition 8.7 The expected value (or mean) of a discrete random variable \\(X\\) with probability distribution \\(P(X)\\) is defined as: \\[ E(X) = \\mu_X = \\sum_{i=1}^N a_i P(a_i)\\] This sum is over all values \\(\\{a_i\\}\\) that the random variable \\(X\\) can take, multiplied by the probability of the random variable taking that value (meaning the probability of the event in sample space that corresponds to that value). This corresponds to the definition of the mean of a data set given in section , if you consider \\(P(a_i)\\) to be the number of times \\(a_i\\) occurs divided by the number of total measurements \\(N\\). As in the case of the histogram and the distribution function, the mean of a sample for a large sample size \\(N\\) approaches the mean of the random variable, which we will discuss in more detail in the next chapter. Sometimes we will use the more concise \\(\\mu_X = E(X)\\) to represent the mean (expected) value. Here are some mathematical properties of the expectation: Expectation of a random variable which is always constant (\\(c\\)) is equal to \\(c\\), since the probability of \\(c\\) is 1: \\(E(c) = cP(c) = c\\) Expectation of a constant multiple of a random variable is: \\[E(cX) = \\sum_i c x_iP(x_i) = c \\sum_i x_iP(x_i) = c \\mu_X\\] Expectation of a sum of two random variables is the sum of their expectations. This is a more complicated argument, so let us break it down. First, all possible values of the random variable \\(X+Y\\) come from going through the possible values of \\(X\\) (\\(a_i\\)) and \\(Y\\) (\\(b_i\\)), and each combination of values has its own probability (called the joint probability distribution) \\(P(a_i, b_j)\\): \\[E(X+Y) = \\sum_i \\sum_j (a_i+b_j) P(a_i, b_j)\\] We can split the sum into two terms by the distributive property of multiplication and then take out the values \\(a_i\\) and \\(b_j\\) out of the sum that they do not depend on: \\[E(X+Y) = \\sum_i \\sum_j a_i P(a_i, b_j) + \\sum_i \\sum_j b_j P(a_i, b_j)=\\] \\[=\\sum_i a_i \\sum_j P(a_i, b_j) + \\sum_j b_j \\sum_i P(a_i, b_j) \\] The joint distributions added up over all values of one variable, become single-variable distributions, so this leaves us with two sums which are the two separate expected values: \\[E(X+Y) = \\sum_i a_i P(a_i) + \\sum_j b_j P(b_j) = E(X) + E(Y) \\] Example. The expected value of the number of heads out of two coin tosses can be calculated using the probability distribution function we found above: \\[ E(X) = 0\\times P(0) + 1 \\times P(1) + 2 \\times P(2) = 0+1/2+2 \\times 1/4 = 1\\] The expected number of heads out of 2 is 1, if each head comes up with probability 1/2, which I think you will find intuitive. Example. The expected value of the number of mutants out of two people can be calculated using the probability distribution function we found above: \\[ E(X) = 0 \\times P(0) + 1 \\times P(1) + 2 \\times P(2) = 0+1 \\times 0.32+2 \\times 0.04 = 0.4\\] The expected number of mutants in a sample of two people is 0.4, which may seem a bit strange. Recall that mean or expected values do not have to coincide with values that are possible, as we discussed in section , but are instead a weighted average of values, according to their frequencies or probabilities. Example. Find the expected value of the number of wins out of two matches for a fringehead which has the probability of winning of 3/4. \\[E(X) = 0 \\times 1/16 + 1 \\times 6/16 + 2 \\times 9/16 = 24/16 = 3/2\\] 8.1.5 variance of random variables Knowledge of the expected value says nothing about how the random variable actually varies: expectation does not distinguish between a random variable which is constant and one which can deviate far from the mean. In order to quantify this variation, one might be tempted to compute the mean differences from the mean value, but it does not work: \\[ E(X-\\mu_X) = \\sum_i (x_i-\\mu_x)P(x_i) = \\sum_i x_i P(x_i) - \\mu_x \\sum_i P(x_i) = \\mu_x - \\mu_x = 0\\] The problem is, if we add up all the differences from the mean, the positive ones end up canceling the negative ones and the expected value of those deviations is exactly zero. This is why it makes sense to square the differences and add them up: Definition 8.8 The variance of a discrete random variable \\(X\\) with probability distribution \\(P(x)\\) is \\[ Var(X) = E((X-\\mu_X)^2) = \\sum_{i=1}^N (x_i-\\mu_x)^2P(x_i)\\] One useful property of the variance is: \\[ Var(X) = \\sum_i (x_i^2 - 2x_i\\mu_x + \\mu_x^2)P(x_i) =\\] \\[= \\sum_i x_i^2 P(x_i) - 2\\mu_x\\sum_i x_i P(x_i) + \\mu_x^2 \\sum_i P(x_i) = E(X^2) - E(X)^2 \\] So variance can be calculated as the difference between the expectation of the variable squared and the squared expectation. Note that the variance is given in units of the variable squared, so in order to measure the spread of the variable in the same units, we take the square root of the variance and call it the standard deviation: \\[\\sigma_x = \\sqrt{Var(X)}\\] While the expectation of a sum of random variables is the sum of their expectations, for any random variables, the same is not true for the variance. However, there is a special condition under which this is true. First, let us write the variance of a sum of two random variables \\(X\\) and \\(Y\\): \\[Var(X+Y) = E \\left[ (X+Y)-(\\mu_X+\\mu_Y) \\right]^2 =\\] \\[ = E[ (X-\\mu_X)^2 +(Y-\\mu_Y)^2 - 2(X-\\mu_X)(Y-\\mu_Y)] = \\] \\[=E (X-\\mu_X)^2 + E(Y-\\mu_Y)^2 -2 E[(X-\\mu_X)(Y-\\mu_Y)] = \\] \\[ = Var(X) + Var(Y) -2 E[(X-\\mu_X)(Y-\\mu_Y)] \\] If you write out the last term as a sum, it is none other than the covariance of the two random variables \\(X\\) and \\(Y\\), which we saw in the chapter on linear regression. So for any two random variables that have zero covariance, their variance is additive! Example. The variance of the number of heads out of two coin tosses can be calculated using its probability distribution function and the expected value (1) from above: \\[ Var(X) = (0-1)^2 \\times P(0) + (1-1)^2 \\times P(1) + (2-1)^2 \\times P(2) = 1/4+0+1/4 = 1/2\\] Since the variance is 1/2, the standard deviation, or the expected distance from the mean value is \\(\\sigma= \\sqrt{1/2}\\). Example. The variance of the number of mutants out of two people can be calculated using its probability distribution function and the expected value (0.4) from above: \\[ E(X) = (0-0.4)^2 \\times P(0) + (1-0.4)^2 \\times P(1) + (2-0.4)^2 \\times P(2) =\\] \\[ = 0.4^2 \\times 0.64+0.6^2 \\times 0.32+1.6^2 \\times 0.04 = 0.32\\] Since the variance is 0.32, the standard deviation, or the expected distance from the mean value is \\(\\sigma= \\sqrt{0.32}\\). Example. We have computed the expected value for the number of wins in two fringehead fights, so now let us find the variance and standard deviation. We already know the possible values of \\(X\\), and the associated probabilities, so we calculate: \\[ E(X^2) = 0^2 \\times 1/16 + 1^2 \\times 6/16 + 2^2 \\times 9/16 = 42/16\\] Then the variance is: \\[ Var(X) = E(X^2) - E(X)^2 = 42/12 - 9/4 = (42-27)/16 = 15/16\\] and the standard deviation is \\(\\sigma = \\sqrt{15}/4\\) or just under 1. 8.1.6 Exercises Calculate the expected values and variances of the following probability distributions, where the possible values of the random variable are in curly brackets, and the probability of each value is indicated as \\(P(x)\\). \\(X=\\{0, 1\\}\\) and \\(P(0) = 0.1, P(1) = 0.9\\). \\(X=\\{1,2,3\\}\\) and \\(P(1) = P(2) = P(3)=1/3\\). \\(X=\\{10, 15, 100\\}\\) and \\(P(10) = 0.5, P(15) = 0.3, P(100)=0.2\\). \\(X=\\{0, 1, 2, 3, 4\\}\\) and \\(P(0) = 1/8, P(1) = P(2) = P(3) = 1/4, P(4) = 1/8\\). \\(X=\\{-1.5, -0.4, 0.3, 0.9\\}\\) and \\(P(-1.5) = 0.4, P(-0.4) = 0.2, P(0.3) = 0.35, P(0.9) = 0.05\\). 8.2 Examples of distributions 8.2.1 uniform distribution Perhaps the simplest random variable (besides a constant, which is not really random) is the uniform random variable, for which every outcome has equal probability. The distribution of a fair coin is uniform with two values, \\(H\\) or \\(T\\), or 0 and 1, each with probability 1/2. More generally, a discrete uniform random variable has \\(N\\) outcomes and each one has probability \\(1/N\\). This is what people often mean when they use the word random - an experiment where each outcome is equally likely. We can calculate the expectation and variance of a uniform random variable \\(U\\): \\[ E(U) = \\sum_{i=1}^n a_i P(a_i) = \\frac{1}{n} \\sum_{i=1}^n a_i \\] So the expected value is the mean of all the values of the uniform random variable. Example. In the special case of the uniform distribution of \\(n+1\\) integers between 0 and \\(n\\) (\\(a_i = i\\), for \\(i=0,..., n\\)), each value has probability \\(P = 1/(n+1)\\). The expected value is the average of the maximum and minimum values (using the fact that \\(\\sum_{i=0}^n i = n(n+1)/2\\)): \\[ E(U) = \\frac{n(n+1)}{2(n+1)} = \\frac{n}{2} \\] Generalizing, for a random variable on integers between \\(a\\) and \\(b\\), the expectation is \\[ E(U) = \\frac{a+b}{2}\\] We can also write down the expression for the variance of the discrete uniform distribution as follows: \\[ Var(U) = E(U^2) - E(U)^2 = \\frac{1}{n} \\sum_{i=1}^n a_i^2 - \\frac{1}{n^2} \\left(\\sum_{i=1}^n a_i \\right)^2\\] Example. In the special case of the uniform distribution of \\(n+1\\) integers between 0 and \\(n\\) (\\(a_i = i\\), for \\(i=0,..., n\\)), each value has probability \\(P = 1/(n+1)\\). The variance can be calculated using the formula for the sum of squares: \\(\\sum_{i=0}^n i^2 =n(n+1)(2n+1)/6\\). \\[ Var(U) = \\frac{(n+1)(2n+1)n}{6(n+1)} - \\frac{n^2}{4} = \\frac{2n^2+n}{6} - \\frac{n^2}{4} = \\frac{n(n+2)}{12} \\] This can be generalize to a uniform random variable on integers between \\(a\\) and \\(b\\) (omitting the algebraic details) so the variance for that uniform random variable is: \\[ Var(U) = \\frac{(b-a+1)^2 - 1}{12} = \\frac{(b-a)^2 + 2(b-a)}{12} \\] Figure 8.1: Two uniform random distributions with integer values with different ranges. 8.2.2 binomial distribution We have introduced binary or Bernoulli trials in section . Assume that the two values of the random variable \\(X\\) are 0 and 1, with probability \\(1-p\\) and \\(p\\), respectively. Then we can calculate the expectation and variance of a single Bernoulli trial: \\[E(X) = 0 \\times (1-p) + 1 \\times p = p\\] \\[ Var(X) = E(X^2) - E(X)^2 = 0^2 \\times (1-p) + 1^2 \\times p - p^2= p(1-p)\\] The first result is likely intuitive, but the second deserves a comment. Note that depending on the probability of 1, the variance, or the spread in outcomes of a Bernoulli trial is different. The highest variance occurs when \\(p=1/2\\), or equal probability of 0 or 1, but when \\(p\\) approaches 0 or 1, the variance approaches 0. Thus, as the probability approaches zero or one the random variable approaches a constant (either always 1 or 0); hence, no variance. One can extend this scenario and ask what happens in a string of Bernoulli trials, for instance, in a string of 10 coin tosses, or in testing 20 randomly selected people for a mutation. The mathematical problem is to calculate the probability distribution of the number of success out of many trials. This is known as the binomial random variable, which is defined as the sum of \\(n\\) independent, identical Bernoulli random variables. Definition 8.9 Given \\(n\\) independent Bernoulli trials \\(X\\) with the same probability of success \\(p\\), the binomial random variable is defined as: \\[B = \\sum_{i=1}^n X_i\\] where \\(X_i\\) is the random variable from the i-th Bernoulli trial, which takes values of 1 and 0. In this definition I use the term independence without defining it properly, which will be done in chapter 10. Intuitively, independence between two Bernoulli trials (e.g. coin tosses) means that the outcome of one trial does not change the probability of the outcomes of any other trials. This amounts to the assumption that the probability of an outcome followed by another one is the product of the separate probabilities of the two outcomes. For example, if the two outcomes are wins and losses, then \\(P(\\{WL\\}) = P(W)P(L)\\). This will be used below in the calculation of the variance of the binomial random variable. To find the probability distribution of the binomial random variable, we need to define the event of \\(k\\) wins out of \\(n\\) trials. Consider the case of 4 trials. It is easy to find the event of 4 wins, as it is comprised only of the outcome \\(\\{WWWW\\}\\). Then, \\(P(4) = p^4\\), based on the independence assumption. The event of winning 3 times consists of four strings: \\(\\{LWWW, WLWW, WWLW, WWWL\\}\\) so the probability of obtaining 3 wins is the sum of the four probabilities, each equal to $ p^3(1-p)$ from the independence assumption above, so \\(P(3) = 4p^3(1-p)\\). The event of winning 2 times is even more cumbersome, and consists of six strings: \\(\\{ LLWW, WLLW, WWLL, WLWL, LWLW, LWWL\\}\\), so \\(P(2) = 6p^2(1-p)^2\\) by the same reasoning. Now imagine doing this to calculate 50 wins out of 100 trials. The counting gets ugly very fast. We need a general formula to help us count the number of ways of winning \\(k\\) times out of \\(n\\) trials. We denote this number \\(\\binom{n}{k}\\), also known as “\\(n\\) choose \\(k\\)” because it corresponds to the number of ways of choosing \\(k\\) distinct objects out of \\(n\\) without regard to order. The connection is as follows: let us label each trial from 1 to \\(n\\). Then to construct a string with \\(k\\) wins, we need to specify which trials resulted in a win (the rest are of course losses). It does not matter in which order those wins are selected - it still results in the same string. Therefore the number of different strings of \\(n\\) binary trials with \\(k\\) successes is the same as the number of ways of selecting \\(k\\) different objects out of \\(n\\) different ones. The number itself can be derived as follows: there are \\(n\\) possibilities for choosing the number of the first win, then \\(n-1\\) possibilities for choosing the number of the second win, etc, and finally when choosing the \\(k\\)-th win there are \\(n-k+1\\) possibilities (note that \\(k \\leq n\\), and if \\(n=k\\) there is only one option left for the last choice.) Thus, the total number of such selections is: \\(n(n-1)...(n-k+1) = n!/(n-k)!\\) But note that we overcounted, because we considered different strings of wins depending on the order in which a win was selected, even if the resulting strings are the same (example: \\(n=4\\) and \\(k=4\\) gives us \\(4!\\) although there is only one string of 4 wins out of 4). In order to correct for the overcounting, we need to divide by the total number of ways of selecting the same string of \\(k\\) wins out of \\(n\\). This is number of ways of rearranging \\(k\\) wins, or \\(k!\\) Thus, the number we seek is: \\[\\binom{n}{k} = \\frac{n!}{k! (n-k)!}\\] We can now calculate the general probability of winning \\(k\\) times out of \\(n\\) trials. First, each string of \\(k\\) wins and \\(n-k\\) losses has the probability \\(p^k (1-p)^{n-k}\\). Since we now know that the number of such strings is \\(C^n_k\\), the probability is: \\[ P(\\mathrm{k \\; wins \\; in \\; n \\; trials}) = P(B=k)= \\binom{n}{k} p^k (1-p)^{n-k} \\] This is the probability distribution of the binomial random variable \\(B\\). The binomial random variable has much simpler formulas for the mean and the variance. First, we know that the mean of a sum of random variables is the sum of the means and the binomial random variable is a sum of \\(n\\) Bernoulli random variables \\(X\\). Let us say \\(X\\) takes only the values of 0 and 1 with probabilities \\(1-p\\) and \\(p\\), so we can use the additive property of expected value to calculate \\(E(B)\\): \\[ E(B) = E\\left[\\sum_{i=1}^n X\\right] = \\sum_{i=1}^n E(X) = \\sum_{i=1}^n p = np \\] This means that the expected number of heads/successes is the product of the probability of 1 head/success and the number of trials, e.g. if the probability of success is 0.3, then the expected number of successes out of 100 is 30. Now let us calculate the variance, for which in general the same additive property is not true. But remember that in the section on variance above we showed that the variance of a sum of two random variables is the sum of their two separate variances as long as their covariance is zero. It turns out that for random variables that satisfy the product rule \\(P(x, y) = P(x)P(y)\\) their covariance is 0: \\[E((X-\\mu_X)(Y-\\mu_Y)) = \\sum_i \\sum_j (x_i-\\mu_X) (y_j-\\mu_Y) P(x_i, y_j) = \\] \\[ = \\sum_i(x_i-\\mu_X)P(x_i) \\sum_j (y_j-\\mu_Y) P(y_j) \\] We saw in section on variance above that the expected value of deviations from the mean is zero, which gives us: \\[E((X-\\mu_X)(Y-\\mu_Y)) = E(X-\\mu_X)E(Y-\\mu_Y) = 0\\] The demonstrates that for independent variables the variance of their sum is the sum of the variances and we can use this to compute the variance of the binomial random variable: \\[ Var(B) = Var\\left[\\sum_{i=1}^n X\\right] = \\sum_{i=1}^n Var(X) =\\sum_{i=1}^n p(1-p) = np(1-p) \\] For any given number of Bernoulli trials, the variance has a quadratic dependence on probability of success \\(p\\): if \\(p=1\\) or \\(p=0\\), corresponding to all successes, or all failures, respectively, then the variance is zero, since there is no spread in the outcome. For a fair coin \\(p=1/2\\) the variance is highest. This can be seen in the plots of binomial random variables for \\(n=2\\), \\(n=5\\), and \\(n=50\\), shown in figures below. Figure 8.2: The binomial distribution for \\(n=2\\) and \\(p=0.2\\) and \\(p=0.5\\) (you should be able to tell which one is which!) Figure 8.3: The binomial distribution for \\(n=5\\) and \\(p=0.2\\) and \\(p=0.5\\) (you should be able to tell which one is which!) Figure 8.4: The binomial distribution for \\(n=50\\) and \\(p=0.2\\) and \\(p=0.5\\) (you should be able to tell which one is which!) 8.2.3 Exercises Calculate the means and variances based on the plotted distributions using the definitions and and compare your calculations against equations and (for uniform random variables) and equations and (for binomial random variables) Calculate the mean and the variance for the two uniform distributions plotted in figure 8.1. Calculate the mean and the variance for the two binomial distributions plotted in figure 8.2. Calculate the mean and the variance for the two binomial distributions plotted figure 8.3. Estimate (approximately) the mean and the variance for the two binomial distributions plotted in figure 8.4. 8.2.4 testing for mutants Suppose that you’re screening people for a particular genetic abnormality. It is known from prior experience that about 5% of this population carry this mutation. You run your tests on a group of 20 people, and the results indicate that 3 of them are carriers. Clearly, this is higher than you expected - 3/20 is 15%, or 3 times higher than the estimate. One of your colleagues exclaims, What are the odds of this? To answer this question, one must start by stating your assumptions. First, the people tested must be chosen from the same population, so we can assume a priori each had probability 5% of being a carrier. Second, the people must be selected without bias, that is, selection of one must be unlinked or independent of others. As a counter-example, if your selection included an entire biological family, that would be a biased selection - it may be that the whole family has the mutation, or maybe they don’t, but either way probability is no longer determined on a person-by-person basis. If these assumptions are made, then one can calculate the probability of making a selection of 20 people that includes 3 carriers of the mutation, using the binomial distribution. The formula for the binomial distribution in equation provides the answer for any given number of mutants. For example, the probability of 3 people out of 20 being carriers for the mutation is: \\[P(\\mathrm{3 \\ out \\ of \\ 20}; \\ p=0.05) = \\binom{20}{3} \\times 0.05^3 \\times 0.985^{17} = \\] \\[ = 1140 \\times 0.05^3 \\times 0.985^{17} \\approx 0.0596\\] One may want to ask a different question: what is the probability that there are at least 3 mutants in the sample of 20 people? To most efficient way to calculate this it is to answer the complementary question first: what is the probability that there are fewer than 3 mutants out of 20 people? This corresponds to three values of the random variable: 0, 1, or 2. We can calculate the total probability by adding up the three separate probabilities, since they represent non-overlapping events (one can’t have 1 and 2 mutants in a sample simultaneously): \\[ P(B &lt; 3; \\ p=0.05) = P(B=0) + P(B=1) + P(B=2) = \\] \\[ = \\binom{20}{2} \\times 0.05^2 \\times 0.985^{18} +\\binom{20}{1} \\times 0.05^1 \\times 0.985^{19} +\\binom{20}{0} \\times 0.05^0 \\times 0.985^{20} \\approx \\] \\[ \\approx 0.925 \\] The answer to the original question is found by taking the complementary probability \\(1-0.925=0.075\\). Thus the probability of finding at least 3 mutants in a sample of 20 with individual probability 0.0015 is approximately 0.075. The answer is close to the probability of having exactly 3 mutants because the probability of finding more than 3 mutants is very low. 8.3 Random number generators in R Simulating randomness with a computer is not a simple task. Randomness is contrary to the nature of a computer, which is designed to perform operations exactly. However, there are algorithms that produce a string of numbers that are for all intents and purposes random: there is no obvious connection between one number and the next, and the values don’t form any pattern. Such algorithms are called random number generators, although to be more precise they produce pseudo-random numbers. The reason is that they actually produce a perfectly predictable string of numbers, which eventually repeats itself, but with a humongous period. One can even produce the same random number, or the same string of random numbers, by specifying the seed for the random number generator. This is very useful if one wants to reproduce the results of a code that uses random numbers. Of course, random variable are not all the same - they have different distributions. R has a number of functions for producing random numbers from different distributions. For example, to produce random numbers from a set of values with a uniform probability distribution, use the function sample(). For instance, the following command produces a random integer between 1 and 20. Repeating the same command produces a new random number, which (most likely) is not the same as the first. The first input argument (1:20) is the vector of values from which to draw the random number, and the second is the size of the sample: x &lt;- sample(1:20,1) y &lt;- sample(1:20,1) print(x) ## [1] 8 print(y) ## [1] 18 To generate 10 randomly chosen integers between 1 and 20, see the following two commands, which differ in setting the value of the option replace. The first command doesn’t specify the value for replace, and by default it is set to FALSE, so the command draws numbers without replacing them (meaning that all the numbers in the sample are unique). In the second command replace is set to TRUE, so the numbers that were selected can be chosen again. In both cases, repeatedly running the command results in a different set of randomly chosen numbers, which you should investigate by copying the commands into R and running them yourself. x &lt;- sample(1:20,10) print(x) ## [1] 16 14 11 17 20 6 4 10 15 13 y &lt;- sample(1:20,10,replace=TRUE) print(y) ## [1] 14 11 1 20 17 6 19 5 8 13 If you need to generate a random number from the binomial distribution, R has you covered. The command is rbinom(s, n, p) and it requires three input values: s is the number of observations (sample size), n is the number of binary trials in one observation, and p is the probability of success in one binary trial. The following two commands generate a single random number, the number of successes out of 20 trials with probability of success 0.2 and 0.6: x &lt;- rbinom(1,20,0.2) print(x) ## [1] 7 y &lt;- rbinom(1,20,0.6) print(y) ## [1] 14 To generate an entire sample of random numbers, change the first input parameter to 10. As you’d expect, the samples of 10 observations are (most likely) noticeably different: when the probability p is 0.2, the number of successes tend to be less than 6, while for probability 0.6, the numbers are usually greater than 10. x &lt;- rbinom(10,20,0.2) print(x) ## [1] 4 7 5 3 3 6 3 1 4 4 y &lt;- rbinom(10,20,0.6) print(y) ## [1] 12 13 8 12 12 17 12 12 9 12 Notice that the range of possible values of this random variable is between 0 and 20, but unlike the uniform random numbers produced with the sample() function, the probability of obtaining different numbers are different, and depend on the parameter p. Calculation and plotting of the binomial distribution function can be accomplished with the command dbinom(x,n,p), where \\(x\\) is the value of the random variable (between 0 and n), \\(n\\) is the number of trials, and p is the probability of success. For instance, the following script calculate the probability of obtaining 1 success out of 20 with probability \\(p=0.2\\): n &lt;- 20 p &lt;- 0.2 print(dbinom(1,n,p)) ## [1] 0.05764608 The script above calculates the probabilities of all of the possible values of the random variable by substituting the vector of these values (e.g. 0 to 20) instead of the number 1, generating the probability distribution vector. This vector is plotted vs. the values of the random variable using the barplot() function, producing an aesthetically pleasing plot of the binomial distribution. The script plots two binomial probability distributions, both with \\(n=20\\), the first with \\(p=0.2\\) and the second with \\(p=0.6\\). Notice also the use of the axis labels in barplot() using the same options xlab and ylab as in plot() and use the main option to produce a title above each plot. values.vec &lt;- 0:n prob.dist &lt;- dbinom(values.vec,n,p) barplot(prob.dist,names.arg=values.vec,xlab=&#39;binomial RV&#39;,ylab=&#39;probability&#39;, main=&#39;binom dist with n=20 and p=0.2&#39;) p&lt;-0.6 prob.dist &lt;- dbinom(values.vec,n,p) barplot(prob.dist,names.arg=values.vec,xlab=&#39;binomial RV&#39;,ylab=&#39;probability&#39;, main=&#39;binom dist with n=20 and p=0.6&#39;) Figure 8.5: The binomial distribution for two different values of n and p produced using dbinom() function. "],["sampling-distribution-and-estimation.html", "9 Sampling distribution and estimation 9.1 Law of large numbers 9.2 Central limit theorem 9.3 Sampling and confidence intervals in R", " 9 Sampling distribution and estimation Pragmatism!? Is that all you have to offer? – Tom Stoppard, Rosencrantz and Guildenstern Are Dead One of the most common tasks in any experimental science is to measure the value of a quantity by performing repeated experiments. A single experiment is not sufficient because there are always random factors making any single observation imperfect: natural variation of the random variable, experimental error, etc. For example, the number of new mutations (those not present in either parent) in an offspring is a random variable, so each individual may have a different number. A set of experimental measurements is called the sample. In this chapter you will learn to do the following: understand the sample mean as a random variable calculate the standard error for a sample mean calculate the confidence interval for the mean understand the meaning of the parameters of the normal distribution simulate sampling of a random variable using R 9.1 Law of large numbers 9.1.1 sample mean The goal in this chapter is to calculate the best estimate of the true mean value of a random variable. To do this, we need to repeat the experiment several times and take the mean of the data set: if one measurement is too high while another one is too low, their average is much closer to the true value. One important consideration here is that the errors cannot be systematically biased. If your experiment always overestimates or underestimates the true value, then averaging the measurements will result in a biased estimate. Second consideration is that the errors cannot depend on each other. This means that if one measurement is above average, it should have no effect on the probability of other measurements being above average. In statistical parlance, a perfectly representative, unbiased sample is called a simple random sample. Speaking precisely, for a sample with \\(n\\) observations (size \\(n\\)) this requires that all subsets of the population of the same size have equal probability of being selected. In practice, this is not easy to verify. Sampling bias occurs if the way a sample is selected results in some participants being systematically overrepresented, and other underrepresented. With a biased sample, any conclusions from a study are suspect, and the mathematics is no longer applicable. In statistics, it is customary to distinguish between the population and the sample. The term population doesn’t always refer to a collection of people or other living creatures; instead it may refer to all possible outcomes of an experiment - essentially, the entire sample space that we introduced in the previous chapter. Out of this ocean of possibilities, an experimenter fishes out a subset called the sample, and uses it to describe the whole population. It should be clear that the sample needs to be representative of the whole population, if this endeavor is to have any hope of success. As mentioned in the last section, the most common estimation involves the mean. Here we need to distinguish two concepts: the true mean and the sample mean. The true mean refers to the quantity we are trying to measure (estimate) and it is considered the mean of a probability distribution of a random variable across the entire population. Thus the true mean is a parameter which we are trying to estimate. The sample mean \\(\\bar X\\) is the average of a sample of experimental measurements, an it is a random variable with its own probability distribution. Suppose you’ve collected a sample of size \\(n\\) and measured its mean \\(\\bar X_1\\). If you collect another sample of the same size, its mean \\(\\bar X_2\\) will be (most likely) a different number. Repeating this process many times will result in different values of the sample mean, all dancing around the true mean. Provided that each time the data collection was unbiased, producing simple random samples, the variation is due to a combination of variance of the random variable in the population and the sampling error. The different sample means result from selecting samples which may randomly contain more numbers higher than the true mean, than those lower (or the opposite). In this and the following sections we will describe the variation in the sample mean. It is a remarkable fact that we can describe this distribution, called the sampling distribution of the mean, in general, and connect it to the true distribution of the variable in the population. The first result describes the mean of the sampling distribution: Theorem 9.1 (Law of Large Numbers, part 1) For a sample of \\(n\\) independent measurements with the same distribution with mean \\(\\mu\\), the sample mean approaches \\(\\mu\\) as \\(n\\) becomes large. More formally, if \\(X_1, X_2, ... , X_n\\) are independent, identically distributed random variables with mean \\(\\mu\\), then: \\[ \\lim_{n \\rightarrow \\infty} \\frac{X_1 + X_2 + ... + X_n} {n} = \\lim_{n \\rightarrow \\infty} \\bar X = \\mu \\] This result is called the Law of Large Numbers and it is intuitive: as the size of an unbiased random sample increases, its sample mean approaches the true mean. If we are dealing with a finite population, then of course if the sample includes the entire population, its mean will be the true mean. One consequence of this is that the best estimator for the true mean is the sample mean; in fact it is what is called an unbiased estimator, provided the sampling process is unbiased. However, this is of only of limited use to someone trying to estimate a quantity, because the theorem doesn’t say how large the sample size needs to be in order for its mean to be reasonably close to the true mean. To address this practical question, let us consider the variance of the sampling distribution. 9.1.2 sample size and standard error As mentioned above, repeated samples from the same populations have varying sample means, and we would like to describe the variance. This variation depends on the sample size, as suggested by the law of large numbers, for large sample sizes the sample mean approaches the true mean. The intuitive fact is that the larger the sample size, the less variation is in the sampling distribution. Theorem 9.2 (Law of Large Numbers, part 2) For a sample of \\(n\\) independent measurements with the same distribution with variance \\(\\sigma^2\\), the variance of the sampling distribution for a sufficiently large \\(n\\) approaches the variance divided by the sample size: \\[ Var(\\bar X) = \\frac{\\sigma^2}{n}\\] In other words, for a sample of \\(n\\) independent measurements the variance of the sample mean is inversely proportional to the sample size. Below I will sketch a calculation to justify (if not prove) the theorem. Suppose that the distribution of experimental measurements has the expected value of \\(\\mu\\) and the variance \\(\\sigma^2\\). Then for a sample of \\(n\\) independent measurements \\(\\{X_i\\}\\), the variance of the sample mean is the following: \\[ Var(\\bar X) = Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_i\\right) = \\frac{1}{n^2} \\sum_{i=1}^{n}Var(X_i) = \\frac{1}{n^2} n \\sigma^2 = \\frac{\\sigma^2}{n}\\] The critical step in that calculation is the second equal sign, where the variance passes inside the sum. This is true because, as we saw in section , the variance of a sum of independent random variables is equal to the sum of the variances. It is often useful to deal with standard deviations to describe the spread of a distribution. Since the standard deviation is the square root of variance, we have the following definition: Definition 9.1 The standard deviation of the mean of a sample of \\(n\\) independent, identically distributed random variables with standard deviation \\(\\sigma\\) is called the standard error of the sample mean. For large sample size \\(n\\), according to the law of large numbers it approaches \\(s= \\sigma/\\sqrt{n}\\). The standard error represents the spread in estimation of the true mean based on the data set of size \\(n\\). This means that increasing the sample size by a factor of 100 will lead to a reduction in the spread of the sample mean by a factor of 10. For example, if we obtain a sample of size 1000 instead of 10, the sample mean will have much less volatility. 9.2 Central limit theorem 9.2.1 normal distribution In the last section we calculated the mean and the standard deviation of sampling distributions for a large sample size. There is an even more remarkable fact about sampling distributions: they all look the same! Regardless of the distribution of the random variable being sampled, the plot of the distribution of sample means approaches the famous bell-shaped curve called the normal distribution. This is one of the most fundamental and useful results in all of mathematics, and is called the Central Limit Theorem. The theorem is much more subtle in both its statement and implications than the law of large numbers, so I need to spell out a few preliminaries. First, the sample mean for a large sample size can take on a whole range of values, so we will think of it as a continuous random variable. Say you’re sampling from the uniform distribution of integers between 1 and 10. If your sample size is 2, the mean may be either an integer (e.g. 5 if your sample is 6 and 4) or a fraction with denominator 2 (e.g. 11/2 if your sample is 1 and 10). As the sample size grows larger, the denominator of the sample mean increases, and the possible means are smeared out more densely between the values of the discrete distribution, e.g. for sample size 100, you may observe a sample mean of 3.62. As the sample sizes become larger, it is convenient to stipulate that the range of values of the sample mean is continuous, that is contains all real numbers between the maximum and the minimum values of the original distribution. This brings us to the magnificent, famous, and indispensable normal, or Gaussian distribution. More correctly, it is the probability density function of the normal random variable, which has the range from negative infinity to infinity, that is, any real number. Below is the mathematical form of its density function \\(\\rho(x)\\), with parameters \\(\\mu\\) and \\(\\sigma\\) representing the mean and the standard deviation, respectively . \\[\\rho(x) = \\sqrt{\\frac{1}{2\\pi \\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2 \\sigma^2}}\\] Plots of normal probability densities with different parameter values. The mean \\(\\mu\\) determines the center of the distribution and the standard deviation \\(\\sigma\\) controls the width. The function of the random variable \\(x\\) is plotted in figure , for different values of the mean and the standard deviation. The shape of the distribution is the famous bell-shaped curve, and the mean indicates the position of the peak on the \\(x\\)-axis. The standard deviation parameter is responsible for the width of the distribution: the larger \\(\\sigma\\), the broader and more spread out the distribution. Those are the only parameters in the normal distribution, everything else about it remains the same. This probability density function differs from the discrete distribution functions we saw in the last chapter in one key way: although you can plug in a particular value of the normal random variable, the number it returns is not the probability of that particular value. Properly speaking, the probability of any particular value of the normal random variable, like 3.2, is indistinguishable from zero, because there are infinitely many such values and if they all had nonzero probabilities, the total probability of the distribution would be infinite. In fact, the total probability of the normal random variable must be 1, as dictated by the axioms of probability seen in section . Instead, in order to extract probability from a density function, it must be integrated. The integral is the equivalent of a sum when you need to add up numbers from a continuous range. So instead of calculating the probability of one value, for continuous random variables we can calculate the probability of a range of values by taking the integral of the density function \\(\\rho(x)\\) on that range. For example, the probability of the normal random variable with \\(\\mu=1\\) and standard deviation \\(\\sigma=0.8\\) being in the range between 0 and 1 is: \\[ P(0 &lt; x &lt; 1) = \\sqrt{\\frac{1}{2\\pi 0.8^2}} \\int_{0} ^1 e^{-\\frac{(x-1)^2}{2 \\times 0.8^2}}dx\\] These integrals may look intimidating, and rightly so: there is no method for solving them the way you may have done in a calculus course. But although we cannot write down an algebraic formula, we can still find the answer , that is as a number. In the old days, one consulted a table in the back of the probability or statistics textbook that contained the values of these integrals. We can do this much more efficiently using R, as will be demonstrated in the computational section below. Now that we have a nodding acquaintance with the normal distribution, here is its most wonderful property: the means of samples, for large sample sizes, are distributed normally. Theorem 9.3 (Central Limit Theorem) The distribution of the mean of a sample of \\(n\\) independent, identically distributed random variables with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) approaches for large \\(n\\) the with mean \\(\\mu\\) and standard deviation \\(\\sigma/\\sqrt{n}\\). If the above conditions are met, the probability that a sample mean will fall between values \\(a\\) and \\(b\\) is given by the integral of the probability density function: \\[ P(a &lt; \\bar X &lt; b) = \\sqrt{\\frac{n}{2\\pi \\sigma^2}} \\int_{a} ^b e^{-\\frac{(x-\\mu)^2}{2 \\sigma^2/n}}dx\\] This theorem gifts us the ability to predict how widely the means of a sample can vary around the true mean. If you know the true mean \\(\\mu\\), the true standard deviation \\(\\sigma\\), and the sample size \\(n\\) is large enough (ignoring for a second what that means exactly) then the probability of the sample mean being off by 0.1 from the true mean in either direction is: \\[ P(\\mu-0.1&lt; \\bar X &lt; \\mu+0.1) = \\sqrt{\\frac{n}{2\\pi \\sigma^2}} \\int_{\\mu-0.1} ^{\\mu+0.1} e^{-\\frac{(x-\\mu)^2}{2 \\sigma^2/n}}dx\\] To evaluate this integral (numerically) we need to know the values of the three parameters: \\(\\mu\\), \\(\\sigma\\), and \\(n\\). In practice, only the sample size \\(n\\) is known, while \\(\\mu\\) and \\(\\sigma\\) are parameters of the true distribution, and can be only estimated. The next section is devoted to sorting out the statistical details. 9.2.2 confidence intervals The central limit theorem is a purely mathematical result, but it is used in a wide range of practical applications, from public opinion polling to medical risk assessments. In this section we journey from the abstract land of probability to the data-driven domain of statistics on a quest for correct estimation of means. When reporting any experimental measurement, it is mandatory to include error bars around the mean of a data set to indicate a range of plausible values of the estimated quantity, usually in the form of \\(\\bar X \\pm \\epsilon\\). The meaning of these error bars varies: sometimes the standard deviation of the measurements (\\(\\sigma_X\\)) is used, other times the standard error, but the correct way to report uncertainty in estimation is to calculate the confidence interval . Definition 9.2 A confidence interval is a range of values calculated from a data set to estimate the true value of a quantity (e.g. mean). The associated confidence level \\(\\alpha\\) (between 0 and 1) is the likelihood that the confidence interval contains the true value. The meaning of confidence intervals is pretty subtle. This is in large part because of the word likelihood in the definition, which in everyday language is interchangeable with probability, but as mathematical terms they are not. Here is the wrong way to think about them: a confidence interval for a mean at \\(\\alpha\\) level does not mean that the true mean has probability \\(\\alpha\\) of being in that interval. The true mean is not a random variable, it is a parameter of the probability distribution that we assume exists. Instead, it means that if sampling were repeated many times, out of the resulting confidence intervals fraction \\(\\alpha\\) would contain the true mean. You can see from this definition that there is more than one confidence interval one can report from a single data set, because its size depends on the confidence level. At first glance, you might be tempted to make \\(\\alpha\\) as large as possible - after all, you’d like to have maximum confidence in your estimate. Unfortunately, it is impossible to provide a confidence interval in which the true mean is guaranteed to reside, short of making the interval infinitely wide. However, one generally wants the estimate to be precise, which means making the confidence internal as narrow as possible. This brings us to the cruel fact of the estimation business: the goal of making a useful (precise) estimate is in opposition to the goal of making a confidence interval with a large confidence level. One can always make a very precise estimate, but it will have a lower confidence level, or one may increase the confidence by making the interval larger. Illustration of the normal probability density in terms of standard deviations from the mean. So, given a data set, how do we construct a confidence interval? First, we need to choose the desired confidence level, for example \\(\\alpha=0.95\\), which is commonly used. The data set has a sample mean \\(\\bar X\\), sample standard deviation \\(\\sigma_X\\), and sample size \\(n\\), which are at our disposal. The central limit theorem tells us that the distribution of sample means is normal (provided \\(n\\) is large enough) with mean \\(\\mu\\) and standard deviation \\(s=\\sigma/\\sqrt n\\). One can calculate the deviation around the \\(\\mu\\) that define a confidence interval using fancy integrals or computers. Figure shows the probability of falling within a a certain deviation from the mean for integer multiples of standard deviation \\(\\sigma\\); for example, the probability of falling within one standard deviation of the mean for a normal random variable is approximately 68%. Luckily, these probability levels are the same for all normal distributions, so they can be used to calculate the confidence interval for any mean. For instance, the range of the normal random variable that contains 95% probability is approximately \\((\\mu - 1.96 s, \\mu + 1.96 s)\\), that is between 1.96 times the standard deviation to the left of the mean to 1.96 the standard deviation to the right of the mean. So to construct the confidence interval one needs the mean \\(\\mu\\), the standard error \\(s\\), and the 95% confidence level. If you were not too bedazzled by different symbols, you might have noticed that there was a bait-and-switch in the argument above. The data set provides the sample mean \\(\\bar X\\), but the confidence interval requires the true mean \\(\\mu\\). The law of large numbers says that the former approaches the latter for large \\(n\\), but they are not the same. However, there is no choice but to use the imperfect sample mean \\(\\bar X\\) as the central point for the confidence interval - because \\(\\mu\\) is what we are trying to estimate! Further, the standard error \\(s\\) actually depends on the true standard deviation \\(\\sigma\\), but we have to make do with the sample standard deviation \\(\\sigma_X\\), for the exact same reason: the true value is not available. (The topic of proper estimation of standard deviation is another kettle of fish, which I leave aside.) These approximations are not always sufficiently appreciated, but they can lead to notable discrepancies between the theoretical confidence level and the actual confidence level, which will be illustrated in the computational assignments in section . To summarize, here is the recipe for calculating a confidence interval for the true mean at a given confidence \\(\\alpha\\) based on a data set \\(X\\) of size \\(n\\): Compute the sample mean \\(\\bar X\\), which is the best estimate we have of the true mean \\(\\mu\\). Compute the sample standard deviation \\(\\sigma_{X}\\), which is the best estimate we have of the actual standard deviation \\(\\sigma\\) 3.Compute the standard error \\(s = \\sigma_X/\\sqrt{n}\\) Use a table or R to calculate the multiple called \\(z_\\alpha\\) for the desired confidence level \\(\\alpha\\). For example, \\(z_{0.9} \\approx 1.65\\), \\(z_{0.95} \\approx 1.96\\), \\(z_{0.99} \\approx 2.58\\). Build the confidence interval by adding and subtracting \\(z_\\alpha s\\) from the sample mean \\(\\bar X\\). For example, the 95% confidence interval is $X z_{0.95} s= X _{X}/ $. \\end{enumerate} There are a couple of important points about error bars that make them different in practice from their theoretical setup. The definition states that if you take a bunch of sample and compute their 95% confidence intervals, about 95% of them will contain the true mean. If you actually perform this experiment using R, you will find out that this is not the case, especially for small sample sizes. The reason for this is that the theoretical definition assumes that we use the true mean and true standard deviation to calculate the confidence interval. But the best we can do is to use the sample mean and sample stander deviation, so of course the confidence interval will be off. This is an important caution about being overly confident about confidence intervals. Another caution comes from the fact that they are based on the assumption of perfect independence between different measurements. Bear this is mind when you see a 95% confidence interval reported in a paper, especially one with a small sample size. 9.2.3 estimating relative risk Clinical trials to evaluate medical treatments or drugs usually proceed by dividing a group of people into two subgroups: one which receives the treatment and one which does not (control group). If this is done in an unbiased random manner, it is called a randomized controlled trial (RCT), which is typically considered the best study design (for most purposes). The two groups are then compared for the outcomes, such as mortality or morbidity (illness). The comparison is usually done in the form of relative risk, or the ratio between the fractions of those with an undesirable outcome in one group and the other. In an idealized case, if the relative risk is 1, there is no difference between the groups, and thus the treatment has no effect. If relative risk is not 1, then it makes a difference (either good or bad). The astute reader has likely noted that this idealization has no practical value. Relative risk is almost never exactly 1 even if the treatment does nothing, due to chance alone. The actual relative risk from a study may be 1.12 or 0.96. Is this sufficiently different from 1 to say that the treatment has an effect? If only we had a way of computing a range of values to estimate the true value of a quantity… But wait, we do! It’s called a confidence interval. The statistics necessary to compute confidence intervals for relative risk are different than what we have seen, because relative risk is not distributed normally. It turns out that its distribution is log-normal (under some assumptions). We will not delve into the details here, but the basic ideas are the same: choose the confidence level, calculate the confidence interval based on the distribution (in this case log-normal) and the statistics of the data. The main question for a clinical trial is, does this confidence interval include 1 or not? If not, then the treatment has an effect (whether positive or negative) at that confidence level. The following discussion questions are based on the paper “Autism occurrence by MMR vaccine status among children with older siblings with and without autism”. Read the paper and discuss with your peers or colleagues using the following questions as a starting point: How are confidence intervals used in the study to conclude that there is no effect of MMR vaccination on development of autism spectrum disorders (ASD)? What are some limitations of the study? How does its design differ from a classic RCT? Does dividing children by whether or not they have a sibling with ASD make the study stronger? 9.3 Sampling and confidence intervals in R As we saw in section , R can be used to generate random numbers from a particular distribution, for example the uniform distribution of real numbers between 0 and 1. This distribution differs from the discrete uniform distribution discussed in section , because the values of the random variable can be any real number between 0 and 1, so it is a continuous variable. The true mean of this distribution is 0.5, but what is the mean of a sample of numbers drawn from a uniform distribution? We can perform this numerical experiment using the R function runif(), which generates a specified number of random numbers from the uniform distribution. If you repeat the same command, you will get a different set of values, since they are generated randomly every time. The following script produces two random samples of size 10 and prints out the means of the samples: sample &lt;- runif(10) print(sample) ## [1] 0.49240915 0.80199500 0.22788038 0.06003781 0.88929087 0.95645231 ## [7] 0.71959634 0.66803953 0.23229357 0.13518338 mean(sample) ## [1] 0.5183178 sample &lt;- runif(10) print(sample) ## [1] 0.07194764 0.92833155 0.65420946 0.30879086 0.47116142 0.07211446 ## [7] 0.74815810 0.27164594 0.42046886 0.18175842 mean(sample) ## [1] 0.4128587 If you copy this script and run it yourself, you will obtain different numbers and if you run it several times you will notice that the mean of sample of 10 values is prone to considerable volatility. This leads to two related questions: 1) how large does the sample need to be in order to obtain a good estimate of the true mean? 2) given a random sample and its sample mean, what is the reasonable range of values for the estimate of the true mean? We addressed these questions theoretically in section , and here we investigate them by generating multiple random samples using R. The script used to produce figure generates 100 random samples of size 10 (using the uniform random number generator), saves the means of the ten samples into a vector variable, and plots its histogram; then it does the same thing for 100 random samples of size 100. The histograms of sample means in figure demonstrate the effect of sample size that we previously discussed theoretically. As a smaller sample size, the sample means vary more widely, in other words, the distribution of sample means has a larger variance for smaller sample sizes. Although the sampling process is random, and every time the script is run it produces a new set of sample means, in the figure you can see clearly that sample means for samples of size 10 are much more spread out than those for sample size 100. numsamples &lt;- 100 samplemeans&lt;-rep(NA, numsamples) samplesize&lt;-10 for (i in 1:numsamples) { sample &lt;- runif(samplesize) samplemeans[i] &lt;- mean(sample) } title_text &lt;- paste(numsamples, &quot;means of samples of size&quot;, samplesize) hist(samplemeans, main=title_text,cex.axis=1.5,cex.lab=1.5) samplemeans&lt;-rep(NA, numsamples) samplesize&lt;-100 for (i in 1:numsamples) { sample &lt;- runif(samplesize) samplemeans[i] &lt;- mean(sample) } title_text &lt;- paste(numsamples, &quot;means of samples of size&quot;, samplesize) hist(samplemeans, main=title_text,cex.axis=1.5,cex.lab=1.5) Figure 9.1: Distribution of means of samples drawn from the uniform distribution for different sample sizes: 10 (left) and 100 (right). The central limit theorem predicts that for large sample size \\(n\\) the distribution of sample means is close to the normal distribution. To demonstrate this, the scripts below generate 1000 samples of size 200 and 500 from the uniform random variable and plot the histograms of sample means shown in figure @(fig:sample-dist). In addition to the histograms, we also overlay plots of the normal distribution with mean \\(\\mu=0.5\\) and standard deviation \\(s = \\sigma/\\sqrt{n}\\), where \\(\\sigma\\) is the standard deviation of the continuous uniform distribution between 0 and 1, which happens to be \\(1/\\sqrt{12}\\). Notice that for sufficiently many samples (1000), the histograms are clearly bell-shaped, as predicted, despite the random samples being generated from the uniform distribution. The normal distribution curve matches the distribution of samples of size 1000 somewhat better than those of size 100, but the difference is not dramatic. The most important difference is in the spread of the sampling distribution, which is noticeably smaller for the larger sample size. This illustrates how much more accurate an estimate of the mean from sample of size 1000 is compared to sample of size 100. Figure 9.2: Central limit theorem in action: comparison of histograms of sample means and the normal distribution: on the left is a histogram of means of samples of size 100, on the right means of samples of size 1000 9.3.1 computing confidence intervals Now let us compute a confidence interval for the mean based on a sample, following the recipe at the end of section . To do this we will use the function qnorm(), which is the inverse normal density function. This means that for a given probability level \\(p\\), the function returns the value \\(x\\) of the standard normal random variable with mean \\(\\mu=0\\) and standard deviation \\(\\sigma=1\\), such that the total probability of the distribution being less than \\(x\\) is equal to \\(p\\). For instance, since the standard normal is a symmetric distribution with mean 0, the probability of the random variable being below 0 is 0.5, as shown by the command in the script below. The second command in the script calculates the value of the standard normal so that 95% of the probability is to its left. qnorm(0.5) # the value that divides the density function in two ## [1] 0 qnorm(0.95) # the value such that 95% of density is to its left ## [1] 1.644854 The function qnorm() is necessary for computing the \\(z_\\alpha\\) values for a given confidence value \\(\\alpha\\). The \\(z_\\alpha\\) is the value of the standard normal such that the random variable \\(z\\) had probability \\(\\alpha\\) of being within \\(z_\\alpha\\) of the mean \\(\\mu=0\\); in other words, it requires that the probability of being outside of the interval \\((-z_\\alpha, z_\\alpha)\\) is \\(1-\\alpha\\). The function qnorm(p) returns the value \\(x\\) so that the standard normal has probability \\(p\\) of being less than \\(x\\); in other words, that probability being greater than \\(x\\) is \\(1-p\\). There are two ways of being outside the interval \\((-z_\\alpha, z_\\alpha)\\): less than \\(-z_\\alpha\\) (called the left tail of the distribution) and greater than \\(z_\\alpha\\) (called the right tail of the distribution). The values \\(z_\\alpha\\), by definition, represent tails of \\((1-\\alpha)/2\\), because there are two tails, and the normal distribution is symmetric. The script below generates a sample from the uniform distribution, computes its mean and standard deviation, and the standard error. Then it calculates the value \\(z_\\alpha\\) based on the defined \\(\\alpha\\) value, and calculates the variables right and left, the respective boundaries of the confidence interval for the true mean. The script shows confidence intervals calculated from a sample of size 10 and a sample of size 100. size &lt;- 10 # sample size alpha &lt;- 0.95 # significance level sample&lt;-runif(size) s &lt;- sd(sample)/sqrt(size) # standard error z &lt;- qnorm((1-alpha)/2) # z-value left &lt;- mean(sample)+s*z right &lt;- mean(sample)-s*z print(right) ## [1] 0.5110677 print(left) ## [1] 0.2615152 size &lt;- 100 # sample size sample&lt;-runif(size) s &lt;- sd(sample)/sqrt(size) # standard error z &lt;- qnorm((1-alpha)/2) # z-value right &lt;- mean(sample)+s*z left &lt;- mean(sample)-s*z print(right) ## [1] 0.4533459 print(left) ## [1] 0.5679654 Every time you run this script, it will produce new samples and therefore different confidence intervals, but most of the time (theoretically, 95% of the time, although that is not practically true) the confidence interval will contain the true mean of 0.5. As you would expect, the confidence interval for sample size 10 is much wider than the one for sample size 100. To obtain a reliable estimate of the true mean, you must obtain a sufficient number of measurements. "],["independence.html", "10 Independence 10.1 Contingency tables to summarize data 10.2 Conditional probability 10.3 Independence of events 10.4 Independence of variables", " 10 Independence Unconnected and free No relationship to anything. – They Might Be Giants, Unrelated Thing In the first part of the book we learned how to describe data sets and probability distributions of random variables. So far we have not discussed how two or more variables may influence each other, and the next four chapters will be devoted to relationships between two variables. Many experiments in biology result in observations that naturally fall into a few categories, for example: sick or healthy patients, presence or absence of a mutation, etc. The resulting data sets are called categorical. Unlike numerical data sets that we will investigate later in chapters 8 and 9, they are not usually represented by numbers. Although it is possible, for instance, to denote mutants with the number 1 and wild type with 0, such designation does not add any value. Categorical variables require different tools for analysis than numerical ones; one cannot compute a linear regression between two categorical variables, because there is no meaningful way to place categories on axes. In this chapter you will learn the following: notion of conditional probability definition of independence for events and random variables produce a categorical data table compute a table of expected values based on independence 10.1 Contingency tables to summarize data What kind of relationship can there be between categorical variables? It cannot be expressed in algebraic form, because without numeric values we cannot talk about a variable increasing or decreasing. Instead, the question is, does one variable being in a particular category have an effect on which category the second variable falls into? Let us say you want to know whether the age of the mother has an effect on the child having trisomy 21 (a.k.a. Down’s syndrome), a genetic condition in which an embryo receives three chromosomes 21 instead of the normal two. The age of the mother is a numerical variable, but it can be classified into two categories: less than 35 and 35 or more years of age. The trisomy status of a fetus is clearly a binary, categorical variable: the fetus either has two chromosomes 21 or three. The data are presented in a two-way or contingency table, which is a common way of presenting a data set with two categorical variables. The rows in such tables represent different categories of one variable and the columns represent the categories of the other, and the cells contain the data measurements of their overlaps. Table shows a contingency table for the data set on Down’s syndrome and maternal age, in which the rows represent the two categories of maternal age and the columns represent the presence or absence of the syndrome. Each internal cell (as opposed to the total counts on the margins) corresponds to the number of measurement where both variables fall into the specified category, for instance the number of fetuses with the syndrome and a mother under 35 is 28. Maternal age No DS DS Total &lt; 35 29,806 28 29,834 &gt;= 35 8,135 64 8,199 Total 37,941 92 38,033 Contingency table for maternal age and incidence of Down’s syndrome. Numbers represent counts of patients belonging to both categories in the row and the column. DS = Down’s syndrome. From . Once the data are organized into a contingency table, we can address the main question stated above: does the age of the mother have an effect on whether a fetus inherits three chromosomes 21? Perhaps the first approach that suggests itself is to compare the fraction of mothers carrying a fetus with DS for the two age categories. In this case, the fraction for the under-35 category is \\(28/29834 \\approx 0.00094\\), while for the 35-and-over category the fraction is \\(64/8199 \\approx 0.0078\\). The two fractions are different by almost a factor of 10, which suggests a real difference between the two categories. However, all data contain an element of randomness and a pinch of error, thus there needs to be quantifiable way of deciding what constitutes a real effect. But to determine if there is a relationship, we first have to define what it means to not have one. 10.2 Conditional probability Let us return to the abstract description of probability introduced in section 8.1. There we used the notion of sample space and its subsets, called events, to describe collections of experimental outcomes. Suppose that you have some information about a random experiment that restricts the possible outcomes to a particular subset (event). In other words, you have ruled out some outcomes, so the only possible outcomes are those in the complementary set. This will affect the probability of other events in the sample space, because your information may have ruled out some of the outcomes in that event as well. Definition 10.1 For two events \\(A\\) and \\(B\\) in a sample space \\(\\Omega\\) with a probability measure \\(P\\), the probability of \\(A\\) given \\(B\\), called the conditional probability is defined as: \\[P(A|B) = \\frac{P(A\\&amp; B)}{P(B)}\\] \\(A \\&amp; B\\) represents the intersection of events \\(A\\) and \\(B\\), also known as \\(A\\) and \\(B\\), the event that consists of all outcomes that are in both \\(A\\) and \\(B\\). In words, given the knowledge that an event \\(B\\) occurs, the sample space is restricted to the subset \\(B\\), which is why the denominator in the definition is \\(P(B)\\). The num`erator is all the outcomes we are interested in, which is \\(A\\), but since we are now restricted to \\(B\\), the numerator consists of all the elements of \\(A\\) which are also in \\(B\\), or \\(A \\&amp; B\\). The definition makes sense in two extreme cases: if \\(A = B\\) and if \\(A\\) and \\(B\\) are mutually exclusive: \\(P(B|B) = P(B \\&amp; B) /P(B) = P(B)/P(B) = 1\\) (probability of \\(B\\) given \\(B\\) is 1) if \\(P(A\\&amp; B) =0\\), then \\(P(A|B) = 0/P(B) = 0\\) (if \\(A\\) and \\(B\\) are mutually exclusive, then probability of \\(A\\) given \\(B\\) is 0) A Venn diagram of the sample space of all people with two events: tall people (\\(A\\)) and those who like tea (\\(B\\)) with probabilities of \\(A\\), \\(B\\) and their intersection indicated. There are some common misunderstandings about conditional probability, which are usually the result of discrepancies between everyday word usage and precise mathematical terminology. First, the probability of \\(A\\) given \\(B\\) is not the same as probability of \\(A\\) and \\(B\\). These concepts seem interchangeable because the statement “what are the odds of finding a tall person who likes tea?” is hard to distinguish from ‘’what are the odds that a person who is tall likes tea?’’ The difference in these concepts can be illustrated using a Venn diagram, shown in figure . Based on the probabilities indicated there, the probability of randomly selecting a person who is both tall and likes tea is \\(P(A \\&amp; B) = 0.1\\), while the probability that a tea drinker is tall is \\(P(A | B) = 0.1/0.3 = 1/3\\), which are different values. A similar misconception is to be cavalier about the order of conditionality. In general, \\(P(A | B) \\neq P(B |A)\\), except in special cases. Going back to the illustration in figure , the probability that a tea drinker is tall \\(P(A | B) = 1/3\\) is the different than the probability that a tall person is a tea drinker \\(P(B|A) = 0.1/0.5 = 0.2\\). One must take care when interpreting written statements to carefully distinguish what is known and what remains under investigation. In the statement \\(P(A | B)\\), \\(B\\) represents what is known, and \\(A\\) represents what is still to be investigated. Example. Let us return to the data set in the previous section. Data table describes a sample space with four outcomes and several different events. One can calculate the probability of a fetus having Down’s syndrome (event) based on the entire data set of 38,033 mothers, and 92 total cases of DS, so the probability is \\(92/38,033 \\approx 0.0024\\). Similarly, we can calculate the probability of a mother being above 35 as \\(8,199/ 38,033 \\approx 0.256\\). Now we can calculate the conditional probability of a mother over 35 having a DS fetus, but first we have to be clear about what information is known and what is not. If the age of the mother is known to be over 35 (mature age or MA), then we calculate \\(P(DS | MA) = 64/8,199 \\approx 0.008\\). Notice that the denominator is restricted by the information that the mother is over 35, and thus only women in that category need to be considered for the calculation. On the other hand, if we have the information that the fetus has DS, we can calculate the reversed conditional probability, what is the probability that a fetus with DS has a mother above age 35? \\(P(MA | DS) = 64/92 \\approx 0.7\\). Notice that is both calculations the numerators are the same, since they both are the intersection between the two events, but the denominators are different, because they depend on which event is given. Punnet square of a cross of two heterozygous pea plants showing the possible genotypes and phenotypes of offspring (figure by Madprime in public domain via Wikimedia Commons.) 10.2.1 Exercises In figure there is a table of genotypes from the classic Mendelian experiment with genetics and color of pea flowers. The parents are both heterozygous, meaning each has a copy of the dominant (purple) allele B and the recessive (white) allele b. The possible genotypes of offspring are shown inside the square, and all four outcomes have equal probabilities. Based on this information, answer the following questions. What is the probability of an offspring having purple flowers? white flowers? What is the probability of an offspring having genotype \\(BB\\)? genotype \\(Bb\\)? genotype \\(bb\\)? What is the probability of an offspring having genotype \\(BB\\), given that its flowers are purple? What is the probability of an offspring having genotype \\(Bb\\), given that its flowers are purple? What is the probability of an offspring having genotype \\(BB\\), given that its flowers are white? What is the probability of an offspring having genotype \\(Bb\\), given that its flowers are white?} What is the probability of an offspring having purple flowers, given that its genotype is \\(BB\\)? 10.3 Independence of events We first encountered the notion of independence in chapter 3, where two events were said to be independent if they did not affect each other. The mathematical definition uses the language of conditional probability to make this notion precise. It says that \\(A\\) and \\(B\\) are independent if given the knowledge of \\(A\\), the probability of \\(B\\) remains the same, and vice versa. Definition 10.2 Two events \\(A\\) and \\(B\\) are independent if \\(P(A|B) = P(A)\\), or equivalently if \\(P(B|A) = P(B)\\). Independence is not a straightforward concept. It may be confused with mutual exclusivity, as one might surmise that if \\(A\\) and \\(B\\) have no overlap, then they are independent. That however, is false by definition, since \\(P(A|B)\\) is 0 for two mutually exclusive events. The confusion stems from thinking that if \\(A\\) and \\(B\\) are non-overlapping, then they do not influence each other. But the notion of influence in this definition is about information; so of course if \\(A\\) and \\(B\\) are mutually exclusive, the knowledge that one of them occurs has an influence of the probability of the other one occurring. A useful way to think about independence is in terms of fractions of outcomes. The probability of \\(A\\) is the fraction of outcomes out of the entire sample space which is in \\(A\\), while the probability of \\(A\\) given \\(B\\) is the fraction of outcomes in \\(B\\) which are also in \\(A\\). The definition of independence equates the two fractions, therefore, if \\(A\\) occupies 1/2 of sample space, in order for \\(A\\) and \\(B\\) to be independent, events in \\(A\\) must constitute 1/2 of the event \\(B\\). In the illustration in figure , the fraction of tall people is 0.5 of the sample space, but the fraction of tea-drinkers who are tall is \\(0.1/0.3=1/3\\). Since the two fractions are different, \\(A\\) and \\(B\\) are not independent. (ch6/indep_ex2.png)(ch6/indep_ex3.png) 10.3.1 Exercises Consider three examples of events and their intersections in figure . Based on the two non-overlapping (mutually exclusive) events, calculate the conditional probability \\(P(A|B)\\) and compare it with \\(P(A)\\). Are \\(A\\) and \\(B\\) independent? Based on the two partially overlapping events, calculate the conditional probability \\(P(A|B)\\) and compare it with \\(P(A)\\). Are \\(A\\) and \\(B\\) independent? Based on the two completely overlapping events, calculate the conditional probability \\(P(A|B)\\) and compare it with \\(P(A)\\). Are \\(A\\) and \\(B\\) independent? 10.3.2 product rule The definition of independence is abstract, but it has a direct consequence of great computational value. From the definition of conditional probability, \\(P(A|B) = P(A\\cap B)/P(B)\\), and if \\(A\\) and \\(B\\) are independent then \\(P(A|B)\\) can be replaced with \\(P(A)\\), leading to the expression \\(P(A) = P(A\\&amp; B)/P(B)\\). Multiplying both sides by \\(P(B\\)) gives us the formula called the product rule, which states that for two independent events the probability of both of them occuring is the product of their separate probabilities: \\[ P(A \\&amp; B) = P(B)P(A) \\] The product rule is extremely useful for computing probability distributions of complicated random variables. Recall that the binomial distribution, which we saw in section is based on a string of \\(n\\) Bernoulli trials which are independent of each other, which allows the calculation of the probability of a string of successes and failures, or heads/tails, etc. In practice, independence between processes is rarely true in the idealized mathematical sense. However, computing the probability of two random variables without independence is extremely difficult, so it is useful to make the independence assumption and then test it against the data. If it stands up, you have a good predictive model, and if it does not, you have learned that two processes are somehow linked, which is very useful. 10.4 Independence of variables The product rule enables us to extend the notion of independence from events to variables. The concepts of independence is the same in both contexts, since the probability of a value \\(x\\) of a random variable \\(X\\) corresponds to the probability of the event that gets mapped to \\(x\\) by the variable. In order to make independence applicable to variables, the condition must hold true for all possible values of both random variables. That way, knowing the value of one variable has no effect on the probability of the other. In order to make it simpler to calculate, we will use the product rule as the equivalent condition for independence: Definition 10.3 Two random variables \\(X\\) and \\(Y\\) are independent if for all possible values of \\(X\\) and \\(Y\\) it is true that \\[ P(X=a \\&amp; Y=b) = P(X=a)P(Y=b)\\] This allows us to address the question posed at the beginning of the chapter: how can one determine whether a data set has independent variables? The definition allows us to calculate what we would expect if the variables were independent. Given a data set in the form of a contingency table, such as table , we can first calculate the probabilities of the two variables separately, and then from that predict the probabilities of the two variables together. Example. Let us calculate the expected probabilities and frequencies of Down’s syndrome in pregnant women in the two age categories. First, compute the probabilities of having Down’s syndrome (and not having it), based on all the pregnancies in the data set: \\(P(DS) = 92/38033 \\approx 0.002419\\); the complementary probability is \\(P(no \\ DS) = 1 - P(DS)\\). Similarly, we can calculate the probability that a pregnant woman is 35 or over, based on the entire data set (let’s denote this event \\(MA\\) for mature age). \\(P(MA) = 8199/38033 \\approx 0.21558\\); the complementary probability is \\(P(YA) = 1 - P(MA)\\) (\\(YA\\) stands for young age). These separate probabilities were calculated from the data, and now we can use them to calculate the predicted probabilities of different outcome, based on the assumption of independence. The probability of a mature-age woman having a pregnancy with Down’s syndrome, based on the product rule is \\(P(MA \\&amp; DS) \\approx 0.0024 \\times 0.216 = 0.000518\\). Similarly, we can calculate the probabilities of the other three outcomes: $P(YA &amp; DS) $; \\(P(MA \\&amp; no \\ DS) \\approx 0.2156\\); \\(P(YA \\&amp; no \\ DS) \\approx 0.782\\). These probabilities are the predictions based on the assumption that the two variables are independent. To compare the predictions with the data, we need to take one more step: convert the probabilities into counts, or frequencies of each occurrence. Since the probability is a fraction out of all outcomes, to generate the predicted frequency we need to multiply the probability by the total number of data points, in this case pregnant patients. The results of this calculation are seen in table with expected frequencies shown instead of experimental observations. Maternal age No DS DS Total &lt; 35 29,761.8 72.2 29,834 &gt;= 35 8,179.2 19.8 8,199 Total 37,941 92 38,033 Expected frequencies of Down’s syndrome for two different age groups of mothers, assuming that age and Down’s syndrome are independent. Notice that expected frequencies do not need to be integers, because they are the result of prediction and not a data measurement. Now that we have a prediction, we can compare it with the measurements in the data table . The numbers are substantially different, and we can see that the predicted frequency of Down’s syndrome for women under 35 is larger than the frequencies for women at or above 35, due to the larger fraction of patients in the younger age group. We can calculate the differences between the observed and expected contingency tables to measure how much reality differs from the assumption of independence: Maternal age No DS DS Total &lt; 35 44.2 -44.2 29,834 &gt;= 35 -44.2 44.2 8,199 Total 37,941 92 38,033 Differences between the observed frequencies of Down’s syndrome for different maternal ages and the expected frequencies based on the assumption of independence. The table of differences shows that the observed frequency of DS in the data set are higher than expected by 44.2 for women above 35 years of age and is lower than expected by the same number for women below age 35. This demonstrates that mathematically speaking, the two variables of age and DS are not independent. However, real data is messy and subject to randomness of various provenance. First, there is sampling error that we explored in chapter 9, which means that samples from two perfectly independent variables can and will differ from expected frequencies. Second, measurement errors or environmental noise can contribute more randomness to the data. Thus, simply checking that observed frequencies are different from expected is not enough to conclude that the variables are not independent. We need a method to decide what scale of differences is enough to declare that there is an effect e.g. of maternal age on the likelihood of DS. To do this, we leave the cozy theoretical confines of probability and venture into the wild and treacherous world of statistics. "]]
