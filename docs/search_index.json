[
["index.html", "Quantifying Life Purpose and purview 0.1 A brief motivation of mathematical modeling 0.2 Purpose of this book 0.3 Organization of the book", " Quantifying Life Dmitry Kondrashov 2020-09-02 Purpose and purview What is a man, said Athos, who has no landscape? Nothing but mirrors and tides. – Anne Michaels, Fugitive Pieces This is an online book to help biologists and biology-adjacent folks learn quantitative skills through the practice of programming in R. These skills can be roughly sorted into four types: Building models and understanding assumptions Writing code to perform computational tasks Performing mathematical analysis of models Working with data and using statistical tools These skills interface, intertwine, and reinforce each other in the practice of biological research and are thus presented concurrently in this book, instead of being corralled into separate courses taught by different departments, like mathematics, statistics, and computer science. Here I combine ideas and skills from all of these disciplines into an educational narrative organized by increasing exposure to programming concepts. 0.1 A brief motivation of mathematical modeling A mathematical model is a representation of some real object or phenomenon in terms of quantities (numbers). The goal of modeling is to create a description of the object in question that may be used to pose and answer questions about it, without doing hard experimental work. A good analogy for a mathematical model is a map of a geographic area: a map cannot record all of the complexity of the actual piece of land, because then the map would need to be size of the piece of land, and then it wouldn’t be very useful! Maps, and mathematical models, need to sacrifice the details and provide a birds-eye view of reality in order to guide the traveler or the scientist. The representation of reality in the model must be simple enough to be useful, yet complex enough to capture the essential features of what it is trying to represent. Mathematical modeling has long been essential in physics: for instance, it is well known that distance traveled by an object traveling at constant speed \\(v\\) is proportional to the time traveled (called \\(t\\)). This mathematical model can be expressed as an equation: %\\[d = vt\\] Since the time of Newton, physicists have been very successful at using mathematics to describe the behavior of matter of all sizes, ranging from subatomic particles to galaxies. However, mathematical modeling is a new arrow in a biologist’s quiver. Many biologists would argue that living systems are much more complex than either atoms or galaxies, since even a single cell is made up of a mind-boggling number of highly dynamic, interacting entities. That is true, but new advances in experimental biology are producing data that make quantitative methods indispensable for biology. The advent of genetic sequencing in the 1970s and 80s has allowed us to determine the genomes of different species, and in the last few years next-generation sequencing has reduced sequencing costs for an individual human genome to a few thousand dollars. The resulting deluge of quantitative data has answered many outstanding questions, and also led to entirely new ones. We now understand that knowledge of genomic sequences is not enough for understanding how living things work, so the burgeoning field of systems biology investigates the interactions between genes, proteins, or other entities. The central question is to understand how a network of interactions between individual molecules can lead to large-scale results, such as the development of a fertilized egg into a complex organism. The human mind is not suited for making correct intuitive judgements about networks comprised of thousands of actors. Addressing questions of this complexity requires quantitative modeling. 0.2 Purpose of this book This textbook is intended for a college-level course for biology and pre-medicine majors, or more established scientists interested in learning the applications of mathematical methods to biology. The book brings together concepts found in mathematics, computer science, and statistics courses to provide the student a collection of skills that are commonly used in biological research. The book has two overarching goals: one is to explain the quantitative language that often is a formidable barrier to understanding and critically evaluating research results in biological and medical sciences. The second is to teach students computational skills that they can use in their future research endeavors. The main premise of this approach is that computation is critical for understanding abstract mathematical ideas. These goals are distinct from those of traditional mathematics courses that emphasize rigor and abstraction. I strongly believe that understanding of mathematical concepts is not contingent on being able to prove all of the underlying theorems. Instead, premature focus on abstraction obscures the ideas for most students; it is putting the theoretical cart before the experiential horse. I find that students can grasp deep concepts when they are allowed to experience them tangibly as numbers or pictures, and those with an abstract mindset can generalize and add rigor later. As I demonstrate in part 3 of the book, Markov chains can be explained without relying on the machinery of measure theory and stochastic processes, which require graduate level mathematical skills. The idea of a system randomly hopping between a few discrete states is far more accessible than sigma algebras and martingales. Of course, some abstraction is necessary when presenting mathematical ideas, and I provide correct definitions of terms and supply derivations when I find them to be illuminating. But I avoid rigorous proofs, and always favor understanding over mathematical precision. The book is structured to facilitate learning computational skills. Over the course of the text students accumulate programming experience, progressing from assigning values to variables in the first chapter to solving nonlinear ODEs numerically by the end of the book. Learning to program for the first time is a challenging task, and I facilitate it by providing sample scripts for students to copy and modify to perform the requisite calculations. Programming requires careful, methodical thinking, which facilitates deeper understanding of the models being simulated. In my experience of teaching this course, students consistently report that learning basic scientific programming is a rewarding experience, which opens doors for them in future research and learning. It is of course impossible to span the breadth of mathematics and computation used for modeling biological scenarios. This did not stop me from trying. The book is broad but selective, sticking to a few key concepts and examples which should provide enough of a basis for a student to go and explore a topic in more depth. For instance, I do not go through the usual menagerie of probability distributions in chapter 4, but only analyze the uniform and the binomial distributions. If one understands the concepts of distributions and their means and variances, it is not difficult to read up on the geometric or gamma distribution if one encounters it. Still, I omitted numerous topics and entire fields, some because they require greater mathematical sophistication, and others because they are too difficult for beginning programmers, e.g. sequence alignment and optimization algorithms. I hope that you do not end your quantitative journey with this book! I take an even more selective approach to the biological topics that I present in every chapter. The book is not intended to teach biology, but I do introduce biological questions I find interesting, refer the reader to current research papers, and provide discussion questions for you to wrestle with. This requires a basic explanation of terms and ideas, so most chapters contain a broad brushstrokes summary of a biological field, e.g. measuring mutation rates, epidemiology modeling, hidden Markov models for gene structure, and limitations of medical testing. I hope the experts in these fields forgive my omitting the interesting details that they spend their lives investigating, and trust that I managed to get the basic ideas across without gross distortion. 0.3 Organization of the book A course based on this textbook can be tailored to fit the quantitative needs of a biological sciences curriculum. At the University of Chicago the course I teach has replaced the last quarter of calculus as a first-year requirement for biology majors. This material could be used for a course without a calculus pre-requisite that a student takes before more rigorous statistics, mathematics, or computer science courses. It may also be taught as an upper-level elective course for students with greater maturity who may be ready to tackle the eigenvalues and differential equations chapters. My hope is that it may also prove useful for graduate students or established scientists who need an elementary but comprehensive introduction to the concepts they encounter in the literature or that they can use in their own research. Whatever path you traveled to get here, I wish you a fruitful journey through biomathematics and computation! "],
["arithmetic-and-variables-the-lifeblood-of-modeling.html", "1 Arithmetic and variables: the lifeblood of modeling 1.1 Blood circulation and mathematical modeling 1.2 Parameters and variables in models 1.3 First steps in R", " 1 Arithmetic and variables: the lifeblood of modeling You can add up the parts, but you won’t have the sum; You can strike up the march, there is no drum. Every heart, every heart to love will come But like a refugee. – Leonard Cohen, Anthem Mathematical modeling begins with a set of assumptions. In fact, one may say that a mathematical model is a bunch of assumptions translated into mathematics. These assumptions may be more or less reasonable, and they may come from different sources. For instance, many physical models are so well-established that we refer to them as laws; we are pretty sure they apply to molecules, cells, and organisms as well as to inanimate objects. Thus we may use physical laws as the foundation on which to build models of biological entities; these are often known as first-principles (theory-based) models. Other times we have experimental evidence which suggests a certain kind of relationship between quantities, perhaps we find that the amount of administered drug and the time until the drug is completely removed from the bloodstream are proportional to each other. This observation can be turned into an empirical (experiment-based) model. Yet another type of model assumption is not based on either theory or experiment, but simply on convenience: e.g. let us assume that the mutation rates in two different loci are independent, and see what the implications are. These are sometimes called toy or cartoon models. This leads to the question: how do you decide whether a model is good? It is surprisingly difficult to give a straightforward answer to this question. Of course, one major goal of a model is to capture some essential features of reality, so in most biological modeling studies you will see a comparison between experimental results and predictions of the model. But it is not enough for a model to be faithful to experimental data! Think of a simple example: suppose your experiment produced 5 data points as a function of time; it is possible to find a polynomial (of fourth degree) that passes exactly through all 5 points, by specifying the coefficients of its 5 terms. This is called data fitting and it has a large role to play in mathematical modeling of biology. However, I think you will agree that in this case we have learned very little: we just substituted 5 values in the data set with 5 values of the coefficients of the mathematical model. To heighten the absurdity, imagine a data set of 1001 points that you have modeled using a 1000-degree polynomial. This is an example of overfitting, or making the model agree with the data by making it overly complex. Substituting a complicated model for a complicated real situation does not help understand it. One necessary ingredient of a useful model is simplicity of assumptions. Simplicity in modeling has at least two virtues: simple models can be grasped by our limited minds, and simple assumptions can be tested against evidence. A simple model that fails to reproduce experimental data can be more informative than a complex model that fits the data perfectly. If a simple model fails, you have learned that you are missing something in your assumptions; but a complex model can be right for the wrong reasons, like erroneous assumptions canceling each other, or it may contain needless assumptions. This is why good modeling is a difficult skill that balances simplicity of assumptions against fidelity to empirical data . In this chapter you will learn how to do the following: distinguish variables and parameters in models describe the state space of a model perform arithmetic operations in R assign variables in R 1.1 Blood circulation and mathematical modeling Galen was one of the great physicians of antiquity. He studied how the body works by performing experiments on humans and animals. Among other things, he was famous for a careful study of the heart and how blood traveled through the body. Galen observed that there were different types of blood: arterial blood that flowed out of the heart, which was bright red, and venous blood that flowed in the opposite direction, which was a darker color. This naturally led to questions: what is the difference between venous and arterial blood? where does each one come from and where does it go? You, a reader of the 21st century, likely already know the answer: blood circulates through the body, bringing oxygen and nutrients to the tissues through the arteries, and returns back through the veins carrying carbon dioxide and waste products, as shown in figure . Arterial blood contains a lot of oxygen while venous blood carries more carbon dioxide, but otherwise they are the same fluid. The heart does the physical work of pushing arterial blood out of the heart, to the tissues and organs, as well as pushing venous blood through the second circulatory loop that goes through the lungs, where it picks up oxygen and releases carbon dioxide, becoming arterial blood again. This may seem like a very natural picture to you, but it is far from easy to deduce by simple observation. Human blood circulates throughout the body and returns to the heart, veins shown in blue and arteries in red. Circulatory System en by LadyofHats in public domain via Wikimedia Commons. 1.1.1 Galen’s theory of blood Galen came up with a different explanation based on the notion of humors, or fluids, that was fundamental to the Greek conception of the body. He proposed that the venous and arterial blood were different humors: venous blood, or natural spirits, was produced by the liver, while arterial blood, or vital spirits, was produced by the heart and carried by the arteries, as shown in figure . The heart consisted of two halves, and it warmed the blood and pushed both the natural and vital spirits out to the organs; the two spirits could mix through pores in the septum separating its right and left halves. The vital and natural spirits were both consumed by the organs, and regenerated by the liver and the heart. The purpose of the lungs was to serve as bellows, cooling the blood after it was heated by the heart. Illustration of Galen’s conception of the blood system, showing different spirits traveling in one direction, but not circulating. Reproduced by permission of Barbara Becker. Is this a good theory of how the heart, lungs, and blood work? Doctors in Europe thought so for over one thousand years! Galen’s textbook on physiology was the standard for medical students through the 17th century. The theory seemed to make sense, and explain what was observable. Many great scientists and physicians, including Leonardo DaVinci and Avicenna, did not challenge the inaccuracies such as the porous septum in the heart, even though they could not see the pores themselves. It took both better observations and a quantitative testing of the hypothesis to challenge the orthodoxy. 1.1.2 Mathematical testing of the theory William Harvey was born in England and studied medicine in Padua under the great physician Hieronymus Fabricius. He became famous, and would perform public demonstrations of physiology, using live animals for experiments that would not be approved today. He also studied the heart and the blood vessels, and measured the volume of the blood that can be contained in the human heart. He was quite accurate in estimating the correct volume, which we now know to be about 70 ml (1.5 oz). What is even more impressive is that he used this quantitative information to test Galen’s theory. Let us assume that all of the blood that is pumped out by the heart is consumed by the tissues, as Galen proposed; let us further assume that the heart beats at constant rate of 60 beats per minute, with a constant ejection volume of 70 ml. Then over the course of a day, the human body would consume about \\[\\mathrm{Volume} = 70 \\ \\mathrm {mL} \\times 60 \\ \\mathrm {(beats \\ per \\ minute)} \\times 60 \\ \\mathrm {(minutes \\ per \\ hour)} \\times 24 \\ \\mathrm {(hours \\ per \\ day)}\\] or over 6,000 liters of blood! You may quibble over the exact numbers (some hearts beat faster or slower, some hearts may be larger or smaller) but the impact of the calculation remains the same: it is an absurd conclusion. Galen’s theory would require the human being to consume and produce a quantity of fluid many times the volume of the human body (about 100 liters) in a day! This is a physical impossibility, so the only possible conclusion in that Galen’s model is wrong. This led Harvey to propose the model that we know today: that blood is not consumed by the tissues, but instead returns to the heart and is re-used again . This is why we call the heart and blood vessels part of the circulatory system of the body. This model was controversial at the time - some people proclaimed they would ``rather be wrong with Galen, than right with Harvey’’ - but eventually became accepted as the standard model. What is remarkable is that Harvey’s argument, despite being grounded in empirical data, was strictly mathematical. He adopted the assumptions of Galen, made the calculations, and got a result which was inconsistent with reality. This is an excellent example of how mathematical modeling can be useful, because it can provide clear evidence against a wrong hypothesis. 1.2 Parameters and variables in models Many biologists remain skeptical of mathematical modeling. The criticism can be summarized like this: a theoretical model either agrees with experiment, or it does not. In the former case, it is useless, because the data are already known; in the latter case, it is wrong! As I indicated above, the goal of mathematical modeling is not to reproduce experimental data; otherwise, indeed, it would only be of interest to theoreticians. The correct question to ask is, does a theoretical model help us understand the real thing? There are at least three ways in which a model can be useful: A model can help a scientist make sense of complex data, by testing whether a particular mechanism explains the observations. Thus, a model can help clarify our understanding by throwing away the non-essential features and focusing on the most important ones. A mathematical model makes predictions for situations that have not been observed. It is easy to change parameters in a mathematical model and calculate the effects. This can lead to new hypotheses that can be tested by experiments. Model predictions can lead to better experimental design. Instead of trying a whole bunch of conditions, the theoretical model can suggest which ones will produce big effects, and thus can save a lot of work for the lab scientist. In order to make a useful model of a complex living system, you have to simplify it. Even if you are only interested in a part of it, for instance a cell or a single molecule, you have to make simplifying choices. A small protein has thousands of atoms, a cell consists of millions of molecules, which all interact with each other; keeping track mathematically of every single component is daunting if not impossible. To build a useful mathematical model one must choose a few quantities which describe the system sufficiently to answer the questions of interest. For instance, if the positions of a couple of atoms in the protein you are studying determine its activity, those positions would make natural quantities to include in your model. You will find more specific examples of models later in this chapter. Once you have decided on the essential quantities to be included in the model, these are divided into variables and parameters. As suggested by the name, a variable typically varies over time and the model tracks the changes in its value, while parameters usually stay constant, or change more slowly. However, that is not always the case. The most important difference is that variables describe quantities within the system being modeled, while parameters usually refer to quantities which are controlled by something outside the system. As you can see from this definition, the same quantity can be a variable or a parameter depending on the scope of the model. Let’s go back to our example of modeling a protein: usually the activity (and the structure) of a protein is influenced by external conditions such as pH and temperature; these would be natural parameters for a model of the molecule. However, if we model an entire organism, the pH (e.g. of the blood plasma) and temperature are controlled by physiological processes within the organism, and thus these quantities will now be considered variables. Perhaps the clearest way to differentiate between variables and parameters is to think about how you would present a data set visually. We will discuss plotting graphs of functions in chapter 2, and plotting data sets in chapter 3, but the reader has likely seen many such plots before. Consider which of the quantities you would to plot to describe the system you are modeling. If the quantity belongs on either axis, it is a variable, since it is important to describe how it changes. The rest of the quantities can be called parameters. Of course, depending on the question you ask, the same quantity may be plotted on an axis or not, which is why this classification is not absolute. After we have specified the essential variables for your model, we can describe a complex and evolving biological system in terms of its state. This is a very general term, but it usually means the values of all the variables that you have chosen for the model, which are often called state variables. For instance, an ion channel can be described with the state variable of conformation, which may be in a open state or in a closed state. The range, or collection of all different states of the system is called the state space of the model. Below you will find examples of models of biological systems with diverse state spaces. 1.2.1 discrete state variables: genetics There are genes which are present in a population as two different versions, called *alleles} - let us use letters \\(A\\) and \\(B\\) to label them. One may describe the genetic state of an individual based on which allele it carries. If this individual is haploid, e.g. a bacterium, then it only carries a single copy of the genome, and its state can be described by a single variable with the state space of \\(A\\) or \\(B\\). A diploid organism, like a human, possesses two copies of each gene (unless it is on one of the sex chromosomes, X or Y); each copy may be in either state \\(A\\) or \\(B\\). This may seem to suggest that there are four different values in the genetic state space, but if the order of the copies does not matter (which is usually the case), then \\(AB\\) and \\(BA\\) are effectively the same, so the state space consists of three values: \\(AA\\), \\(BB\\), and \\(AB\\). 1.2.2 discrete state variables: population Consider the model of a population of individuals, with the variable of number of individuals (populations size) and parameters being the birth and death rates. The state space of this model is all integers between 0 and infinity. Consider the model of a population of individuals who may get infected. Assume that the total number of individuals does not change (that is, there are no births and deaths) and that these individuals can be in one of two states: healthy or sick (in epidemiology these are called susceptible or infectious). There are typically two parameters in such models: the probability of infection and the probability of recovery. Since the total population is fixed at some number \\(N\\), the space space of the model is all pairs of integers between 0 and \\(N\\) that add up to \\(N\\). 1.2.3 continuous state variables: concentration Suppose that a biological molecule is produced with a certain rate and degraded with a different rate, and we would like to describe the quantity of the molecule, usually expressed as concentration. The relevant variables here are concentration and time, and you will see those variables on the axes of many plots in biochemistry. Concentration is a ratio of the number of molecules and the volume, so the state space can be any positive real number (although practically there is a limit as to how many molecules can fit inside a given volume, but for simplicity we can ignore this). Going even further, let us consider an entire cell, which contains a large number of different molecules. We can describe the state of a cell as the collection of all the molecular concentrations, with the parameters being the rates of all the reactions going on between those molecules. The state space for this model with \\(N\\) different molecules is \\(N\\) positive real numbers. 1.2.4 multiple variables in medicine Doctors take medical history from patients and measure vital signs to get a picture of a patient’s health. These can be all be thought of as variables in a model of a person that physicians construct. Some of these variables are discrete, for instance whether there is family history of hypertension, which has only two values: yes or no. Other variables are numbers with a range, such as weight and blood pressure. The state space of this model is a combination of categorical values (such as yes/no) and numerical values (within a reasonable range). 1.2.5 Discussion questions Several biological models are indicated below. Based on what you know, divide the quantities into variables and parameters and describe the state space of the model. Note that there may be more than one correct interpretation The volume of blood pumped by the heart over a certain amount of time, depending on the heart rate and the ejection volume. The number of wolves in a national forest depending on the number of wolves in the previous year, the birth rate, the death rate, and the migration rate. The fraction of hemes in hemoglobin (a transport protein in red blood cells) which are bound to oxygen depending on the partial pressure of oxygen and the binding cooperativity of hemoglobin. The number of mutations that occur in a genome, depending on the mutation rate, the amount of time, and the length of the genome. The concentration of a drug in the blood stream depending on the dose, time after administration, and the rate of metabolism (processing) of the drug. Describing an outbreak of an infectious disease in a city in terms of the fractions of infected, healthy, and recovered people, depending on the rate of infection, rate of recovery, and the mortality rate of the disease. 1.3 First steps in R A central goal of this book is to help you, the reader, gain experience with computation, which requires learning some programming (cool kids call it “coding”). Programming is a way of interacting with computers through a symbolic language, unlike the graphic user interfaces that we’re all familiar with. Basically, programming allows you to make a computer do exactly what you want it to do. There is a vast number of computer languages with distinct functionalities and personalities. Some are made to talk directly to the computer’s “brain” (CPU and memory), e.g. Assembly, while others are better suited for human comprehension, e.g. python or Java. Programming in any language involves two parts: 1) writing a program (code) using the commands and the syntax for the language; 2) running the code by using a compiler or interpreter to translate the commands into machine language and then making the computer execute the actions. If your code has a mistake in it, the compiler or interpreter should catch it, and return an error message to you instead of executing the code. Sometimes, though, the code may pass muster with the interpreter/compiler, but it may still have a mistake (bug). This can be manifested in two different ways: either the code execution does not produce the result that you intended, or it hangs up or crashes the computer (the latter is hard to do with the kind of programming we will be doing). We will discuss errors and how to prevent and catch these bugs as you develop your programming skills. In this course, our goal is to compute mathematical models and to analyze data, so we choose a language that is designed specially for these tasks, which is called R. To proceed, you’ll need to download and install R, which is freely available here. In addition to downloading the language (which includes the interpreter that allows you to run R code on your computer) you will need to download a graphic interface for writing, editing, and running R code, called R Studio (coders call this an IDE, or an Integrated Developer Environment), which is also free and available here. 1.3.1 R Markdown and R Studio In this course you will use R using R Studio and R Markdown documents, which are text files with the extension .Rmd. Markdown is a simple formatting syntax for creating reports in HTML, PDF, or Word format by incorporating text with code and its output. More details on using R Markdown are here. In fact, this whole book is written in R Markdown files and then compiled to produce the beautiful (I hope you agree) web book that you are reading. If you open an Rmd file in R Studio, you will see a Knit button on top of the Editor window. Clicking it initiates the processing of the file into an output document (in HTML, PDF, or Word format) that includes the text as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: print(&quot;Hello there!&quot;) ## [1] &quot;Hello there!&quot; To run the code inside a single R code chunk, click the green arrow in the top right of the chunk. This will produce an output, in this case the text “Hello there!”. Inside the generated output file, for example the web book you may be reading, the output of code chunks is shown below the box with the R code and indicated by two hashtags. You can make text bold or italic like so. You can also use mathematical notation called LaTeX, which you’ll see used below to generate nice-looking equations. LaTeX commands are surrounded by dollar signs, for example $e^x$ generates \\(e^x\\). Mathematical types love LaTeX, but you can use R Markdown without it. 1.3.2 numbers and arithmetic operations When you get down to the brass tacks, all computation rests on performing arithmetic operations: addition, subtraction, multiplication, division, exponentiation, etc. The symbols used for arithmetic operations are what you’d expect: +, -, *, / are the four standard operations, and ^ is the symbol for exponentiation. For example, type 2^3 in any R code chunk and execute it: 2^3 ## [1] 8 You see that R returns the result by printing it out on the screen. The number in square brackets [1] is not important for now; it is useful when the answer contains many numbers and has to be printed out on many rows. The second number is the result of the calculation. For numbers that are either very large or very small, it’s too cumbersome to write out all the digits, so R, like most computational platforms, uses the scientific notation. For instance, if you want to represent 1.4 billion, you type in the following command; note that 10 to the ninth power is represented as e+09 and the prefix 1.4 is written without any multiplication sign: 1.4*10^9 ## [1] 1.4e+09 There are also certain numbers built into the R language, most notably \\(\\pi\\) and \\(e\\), which can be accessed as follows: pi ## [1] 3.141593 exp(1) ## [1] 2.718282 The expression exp() is an example of a function, which we will discuss in section ; it returns the value of \\(e\\) raised to the power of the number in parenthesis, hence exp(1) returns \\(e\\). Notice that although both numbers are irrational, and thus have infinitely many decimal digits, R only prints out a few of them. This doesn’t mean that it doesn’t have more digits in memory, but it only displays a limited number to avoid clutter. The number of digits to be displayed can be changed, for example to display 10 digits, type in options(digits=10). Computers are very good at computation, as their name suggests, but they have limitations. In order to manipulate numbers, they must be stored in computer memory, but computer memory is finite. There is a limit to the length of the number that is feasible to store on a computer. This has implications for both very large numbers and to very small numbers, which are close to zero, because both require many digits for storage. All programming languages have an upper limit on the biggest number it will store and work with. If an arithmetic operation results in a number larger than that limit, the computer will call it an overflow error. Depending on the language, this may stop the execution of the program, or else produce a non-numerical value, such as NaN (not a number) or Inf (infinite). Do exercise to investigate the limitations of R for large numbers. On the other hand, very small numbers present their own challenges. As with very large numbers, a computer cannot store an arbitrary number of digits after the decimal (or binary) point. Therefore, there is also the smallest number that a programming language will accept and use, and storing a smaller number produces an underflow error. This will either cause the program execution to stop, or to return the value 0 instead of the correct answer. Do exercise to investigate the limitations of R for small numbers. This last fact demonstrates that all computer operations are imprecise, as they are limited by what’s called the machine precision, which is illustrated in exercise . For instance, two similar numbers, if they are within the machine precision of one another, will be considered the same by the computer. Modern computers have large memories, and their machine precision is very good, but sometimes this error presents a problem, e.g. when subtracting two numbers. A detailed discussion of machine error is beyond the scope of this text, but anyone performing computations must be aware of its inherent limitations. 1.3.3 R Coding Exercises Calculate the value of \\(\\pi\\) raised to the 10th power. Use the scientific notation to multiply four billion by \\(\\pi\\). Use the scientific notation with large exponents (e.g. 1e+100, 1e+500, etc.) to find out what happens when you give R a number that is too large for it to handle. Approximately at what order of magnitude does R produce an overflow error? In the same fashion, find out what happens when you give R a number that is too small for it to handle. Approximately at what order of magnitude does R produce an underflow error? How close can two numbers be before R thinks they are the same? Subtract two numbers which are close to each other, like 24 and 24.001, and keep making them closer to each other, until R returns a difference of zero. Report at what value of the actual difference this happens. 1.3.4 variable assignment Variables in programming languages are used to store and access numerical or other information. After assigning} it a value for the first time (initializing), a variable name can be used to represent the value we assigned to it. Invoking the name of variable recalls the stored value from computer’s memory. There are a few rules about naming variables: a name cannot be a number or an arithmetic operator like +, in fact it cannot contain symbols for operators or spaces inside the name, or else confusion would reign. Variable names may contain numbers, but not as the first character. When writing code it is good practice to give variables informative names, like height or city_pop. The symbol ‘=’ is used to assign a value to a variable in most programming languages, and can be used in R too. However, it is customary for R to use the symbols &lt;- together to indicate assignment, like this: var1 &lt;- 5 After this command the variable var1 has the value 5, which you can see in the upper right frame in R Studio called Environment. In order to display the value of the variable as an output on the screen, use the special command print() (it’s actually a function, which we will discuss in the next chapter). The following two commands show that the value of a variable can be changed after it has been initialized: var1 &lt;- 5 var1 &lt;- 6 print(var1) ## [1] 6 While seemingly contradictory, the commands are perfectly clear to the computer: first var1 is assigned the value 5 and then it is assigned 6. After the second command, the first value is forgotten, so any operations that use the variable var1 will be using the value of 6. Entire expressions can be placed on the right hand side of an assignment command: they could be arithmetic or logical operations as well as functions, which we will discuss later on. For example, the following commands result in the value 6 being assigned to the variable var2: var1 &lt;- 5 var2 &lt;- var1+1 print(var2) ## [1] 6 Even more mind-blowing is that the same variable can be used on both sides of an assignment operator! The R interpreter first looks on the right hand side to evaluate the expression and then assigns the result to the variable name on the left hand side. So for instance, the following commands increase the value of var1 by 1, and then assign the product of var1 and var2 to the variable var2: var1 &lt;- var1 + 1 print(var1) ## [1] 6 var2 &lt;- var1-1 print(var2) ## [1] 5 var2 &lt;- var1*var2 print(var2) ## [1] 30 We have seen example of how to assign values to variables, so here is an example of how NOT to assign values, with the resulting error message: var1 + 1 &lt;- var1 The left-hand side of an assignment command should contain only the variable to which you are assigning a value, not an arithmetic expression to be performed. 1.3.5 R Coding Exercises The following commands or scripts do not work as intended. Find the errors and correct them, then run them to make sure they do what they are intended to do: 1.3.6 Exercises: The following R commands or short scripts contain errors; your job is to fix them so they runs as described. (Remove the # at the start of each line to “uncomment” the code first.) Assign the value -10 to a variable neg -&gt; -10 Assign a variable the value 5 and then increase its value by 3: 2pac &lt;- 5 2pac &lt;- 2pac + 3 Assign the values 4 and 7 to two variables, then add them together and assign the sum to a new variable: total &lt;- part1 + part2 part1 &lt;- 4 part2 &lt;- 7 Add 5 and 3 and save it into variable my.number 5 + 3 &lt;- my.number Print the value of my.number on the screen: print[my.number] Replace the value of my.number with 5 times its current value my.number &lt;- 5my.number Assign the values of 7 and 8 to variables a and b, respectively, multiply them and save the results in variable x a&lt;-7 b&lt;-8 x&lt;-ab print(x) Assign the value 42 to a variable, then increase it by 1 age &lt;- 42 age + 1 &lt;- age Assign the value 10 to variable radius, then calculate the area of the circle with that radius using the formula \\(A = \\pi r^2\\): r &lt;- 10 area &lt;- pir^2 "],
["functions-and-their-graphs.html", "2 Functions and their graphs 2.1 Dimensions of quantities 2.2 Functions and their graphs 2.3 Vectors and plotting in R 2.4 Rates of biochemical reactions", " 2 Functions and their graphs Some fathers, if you ask them for the time of day, spit silver dollars. Donald Barthelme, The Dead Father Mathematical models describe how various quantities affect each other. In the last chapter we learned that these descriptions can be written down, often in the form of an equation. For instance, we can describe the total volume of blood pumped over a period of time as the product of stroke volume, the heart rate and the number of minutes, which can be written as an equation. The different quantities have their own meaning and roles, depending on what they stand for. To better describe how these quantities are related we use the deep idea of mathematical functions. In this chapter you will learn to do the following: use dimensional analysis to deduce the meaning of quantities in a model understand the concept of function, dependent and independent variables recognize basic functional forms and the shape of their graphs use R to plot functions understand basic models of reaction rates 2.1 Dimensions of quantities What distinguishes a mathematical model from a mathematical equation is that the quantities involved have a real-world meaning. Each quantity represents a measurement, and associated with each one are the units of measurement. The number 173 is not enough to describe the height of a person - you are left to wonder 173 what? meters, centimeters, nanometers, light-years? Obviously, only centimeters make sense as a unit of measurement for human height; but if we were measuring the distance between two animals in a habitat, meters would be a reasonable unit, and it were the distance between molecules in a cell, we would use nanometers. Thus, any quantity in a mathematical model must have associated units, and any graphs of these quantities must be labeled accordingly. In addition to units, each variable and parameter has a meaning, which is called the dimension of the quantity. For example, any measurement of length or distance has the same dimension, although the units may vary. The value of a quantity depends on the units of measurement, but its essential dimensionality does not. One can convert a measurement in meters to that in light-years or cubits, but one cannot convert a measurement in number of sheep to seconds - that conversion has no meaning. Thus leads us to the fundamental rule of mathematical modeling: terms that are added or subtracted must have the same dimension. This gives mathematical modelers a useful tool called dimensional analysis, which involves replacing the quantities in an equation with their dimensions. This serves as a check that all dimensions match, as well as allowing to deduce the dimensions of any parameters for which the dimension was not specified. Example. As we saw in chapter 1, the relationship between the amount blood pumped by a heart in a certain amount of time is expressed in the following equation, where \\(V_{tot}\\) and \\(V_s\\) are the total volume and stroke volume, respectively, \\(R\\) is the heart rate, and \\(t\\) is the time: \\[ V_{tot} = V_sRt \\] The dimension of a quantity \\(X\\) is denoted by \\([X]\\); for example, if \\(t\\) has the dimension of time, we write \\([t] = time\\). The dimension of volume is \\([V_{tot}] = length^3\\), the dimension of stroke volume is \\([V_s] = volume/beat\\) and the dimension of time \\(t\\) is time, so we can re-write the equation above in dimensional form: \\[length^3 = length^3/ beat \\times R \\times time\\] Solving this equation for R, we find that it must have the dimensions of \\([R] = beats/time\\). It can be measured in beats per minute (typical for heart rate), or beats per second, beats per hour, etc. but the dimensionality of the quantity cannot be changed without making the model meaningless. There are also dimensionless quantities, or pure numbers, which are not tied to a physical meaning at all. Fundamental mathematical constants, like \\(\\pi\\) or \\(e\\), are classic examples, as are some important quantities in physics, like the Reynolds number in fluid mechanics. Quantities with a dimension can be made dimensionless by dividing them by another quantity with the same dimension and “canceling” the dimensions. For instance, we can express the height of a person as a fraction of the mean height of the population; then the height of a tall person will become a number greater than 1, and the height of a short one will become less than 1. This new dimensionless height does not have units of length - they have been divided out by the mean height. This is known as rescaling the quantity, by dividing it by a preferred scale. There is a fundamental difference between rescaling and changing the units of a quantity: when changing the units, e.g. from inches to centimeters, the dimension remains the same, but if one divides the quantity by a scale, it loses its dimension. Example. The model for a population of bacteria that doubles every hour is described by the equation, where \\(P_0\\) is initial number of bacteria and \\(P\\) is the population after \\(t\\) hours: \\[ P = P_0 2^t \\] Let us define the quantity \\(R=P/P_0\\), so we can say that population increased by a factor of \\(R\\) after \\(t\\) hours. This ratio is a dimensionless quantity because \\(P\\) and \\(P_0\\) have the same dimension of bacterial population, which cancel out. The equation for \\(R\\) can be written as follows: \\[ R= 2^t \\] According to dimensional analysis, both sides of the equation have to be dimensionless, so \\(t\\) must also be a dimensionless variable. This is surprising, because \\(t\\) indicates the number of hours the bacterial colony has been growing. This reveals the subtle fact that \\(t\\) is a rescaled variable obtained by dividing the elapsed time by the length of the reproductive cycle. Because of the assumption that the bacteria divide exactly once an hour, \\(t\\) counts the number of hours, but if they divided once a day, \\(t\\) would denote the number of days. So \\(t\\) doesn’t have units or dimensions, but instead denotes the dimensionless number of cell divisions. 2.1.1 Exercises For each biological model below determine the dimensions of the parameters, based on the given dimensions of the variables. Model of number of mutations \\(M\\) as a function of time \\(t\\): \\[ M(t) = M_0 + \\mu t\\] Model of molecular concentration \\(C\\) as a function of time \\(t\\): \\[ C(t) = C_0 e^{-kt} \\] Model of tree height \\(H\\) (length) as a function of age \\(a\\) (time): \\[ H(a) = \\frac{b a}{c + a}\\] Model of cooperative binding of ligands, with fraction of bound receptors \\(\\theta\\) as a function of ligand concentration \\(L\\): \\[ \\theta (L) = \\frac{L^n}{L^n + K_d}\\] Model of concentration of a gene product \\(G\\) (concentration) as a function of time \\(t\\): \\[ G(t) = G_m (1 - e^{-\\alpha t})\\] Michaelis-Menten model of enzyme kinetics, \\(v\\) is reaction rate (1/time) and \\(S\\) is substrate concentration: \\[ v(S) = \\frac{v_{max} S}{K_m + S}\\] Logistic model of population growth, \\(P\\) is population size and time \\(t\\): \\[ P(t) = \\frac{A e^{kt}}{1 + B(e^{kt} -1)} \\] 2.2 Functions and their graphs A relationship between two variables addresses the basic question: when one variable changes, how does this affect the other? An equation, like the examples in the last section, allows one to calculate the value of one variable based on the other variable and parameter values. In this section we seek to describe more broadly how two variables are related by using the mathematical concept of functions. Definition: A function is a mathematical rule which has an input and an output. A function returns a well-defined output for every input, that is, for a given input value the function returns a unique output value. In this abstract definition of a function it doesn’t have to be written as an algebraic equation, it only has to return a unique output for any given input value. In mathematics we usually write them down in terms of algebraic expressions. As in mathematical models, you will see two different kinds of quantities in equations that define functions: variables and parameters. The input and the output of a function are usually variables, with the input called the independent variable and the output called the dependent variable. The relationship between the input and the output can be graphically illustrated in a graph, which is a collection of paired values of the independent and dependent variable drawn as a curve in the plane. Although it shows how the two variables change relative to each other, parameters may change too, which results in a different graph of the function. While graphing calculators and computers can draw graphs for you, it is very helpful to have an intuitive understanding about how a function behaves, and how the behavior depends on the parameters. Here are the three questions to help picture the relationship (assume \\(x\\) is the independent variable and it is a nonnegative real number): what is the value of the function at \\(x=0\\)? what does the function do when \\(x\\) becomes large (\\(x \\to \\infty\\))? what does the function do between the two extremes? Below you will find examples of fundamental functions used in biological models with descriptions of how their parameters influence their graphs. 2.2.1 linear and exponential functions The reader is probably familiar with linear and exponential functions from algebra courses. However, they are so commonly used that it is worth going over them to refresh your memory and perhaps to see them from another perspective. Definition A linear function \\(f(x)\\) is one for which the difference in two function values is the same for a specific difference in the independent variable. In mathematical terms, this can be written an equation for any two values of the independent variable \\(x_1\\) and \\(x_2\\) and a difference \\(\\Delta x\\): \\[ f(x_1 + \\Delta x) - f(x_1) = f(x_2 + \\Delta x) - f(x_2) \\] The general form of the linear function is written as follows: \\[\\begin{equation} f(x) = ax + b \\label{eq:linear_funk} \\end{equation}\\] The function contains two parameters: the slope \\(a\\) and the y-intercept \\(b\\). The graph of the linear function is a line (hence the name) and the slope \\(a\\) determines its steepness. A positive slope corresponds to the graph that increases as \\(x\\) increases, and a negative slope corresponds to a declining function. At \\(x=0\\), the function equals \\(b\\), and as \\(x \\to \\infty\\), the function approaches positive infinity if \\(a&gt;0\\), and approaches negative infinity if \\(a&lt;0\\). Definition An exponential function \\(f(x)\\) is one for which the ratio of two function values is the same for a specific difference in the independent variable. Mathematically speaking, this can be written as follows for any two values of the independent variable \\(x_1\\) and \\(x_2\\) and a difference \\(\\Delta x\\): \\[ \\frac{f(x_1 + \\Delta x)}{f(x_1)} = \\frac{f(x_2 + \\Delta x)}{f(x_2)}\\] Exponential functions can be written using different symbolic forms, but they all have a constant base with the variable \\(x\\) in the exponent. I prefer to use the constant \\(e\\) (base of the natural logarithm) as the base of all the exponential functions, for reasons that will become apparent in chapter 15. This does not restrict the range of possible functions, because any exponential function can be expressed using base \\(e\\), using a transformation: \\(a^x = e^{x \\ln(a)}\\). So let us agree to write exponential functions in the following form: \\[\\begin{equation} f(x) = a e^{rx} \\label{eq:exp_funk} \\end{equation}\\] The function contains two parameters: the \\(r\\) and the multiplicative constant \\(a\\). The graph of the exponential function is a curve which crosses the y-axis at \\(y=a\\) (plug in \\(x=0\\) to see that this is the case). As \\(x\\) increases, the behavior of the graph depends on the sign of the rate constant \\(r\\). If \\(r&gt;0\\), the function approaches infinity (positive if \\(a&gt;0\\), negative if \\(a&lt;0\\)) as \\(x \\to \\infty\\). If \\(r&lt;0\\), the function decays at an ever-decreasing pace and asymptotically approaches zero as \\(x \\to \\infty\\). Thus the graph of \\(f(x)\\) is a curve either going to infinity or a curve asymptotically approaching 0, and the steepness of the growth or decay is determined by \\(r\\). Figure 2.1: Plots of two linear functions (left) and two exponential functions (right). Can you identify which linear function has the positive slope and which one negative? Which exponential function has a positive rate constant and which one negative? 2.2.2 Exercises Answer the questions below, some of which refer to the function graphs in figure ??. Which of the linear graphs in the first figure corresponds to \\(f(x) = 5x\\) and which corresponds to \\(f(x) = 10-x\\)? State which parameter allows you to connect the function with its graph and explain why. Which of the exponential graphs in the second figure corresponds to \\(f(x) = 0.1e^{0.5x}\\) and which corresponds to \\(f(x) = 12e^{-0.2x}\\)? State which parameter allows you to connect the function with its graph and explain why. Demonstrate algebraically that a linear function of the form given in equation satisfies the property of linear functions from definition . Demonstrate algebraically that an exponential function of the form given in equation satisfies the property of exponential functions from definition . Modify the exponential function by adding a constant term to it \\(f(x) = a e^{rx} + b\\). What is is the value of this function at \\(x=0\\)? How does the function defined in the previous exercise, \\(f(x) = a e^{rx} + b\\), how does it behave as \\(x \\to \\infty\\) if \\(r&gt;0\\)? How does the function \\(f(x) = a e^{rx} + b\\) behave as \\(x \\to \\infty\\) if \\(r&lt;0\\)? 2.2.3 rational and logistic functions Let us now turn to more complex functions, made up of simpler components that we understand. Consider a ratio of two polynomials, called a rational function. The general form of such functions can be written down as follows, where ellipsis stands for terms with powers lower than \\(n\\) or \\(m\\): \\[\\begin{equation} f(x) = \\frac{a_0 + ... + a_n x^n}{b_0 + ... + b_m x^m} \\label{eq:rational_funk} \\end{equation}\\] The two polynomials may have different degrees (highest power of the terms, \\(n\\) and \\(m\\)), but they are usually the same in most biological examples. The reason is that if the numerator and the denominator are ``unbalanced’’, one will inevitably overpower the other for large values of \\(x\\), which would lead to the function either increasing without bound to infinity (if \\(n&gt;m\\)) or decaying to zero (if \\(m&gt;n\\)). There’s nothing wrong with that, mathematically, but rational functions are most frequently used to model quantities that approach a nonzero asymptote for large values of the independent variable. For this reason, let us assume \\(m=n\\) and consider what happens as \\(x \\to \\infty\\). All terms other than the highest-order terms become very small in comparison to \\(x^n\\) (this is something you can demonstrate to yourself using R), and thus both the numerator and the denominator approach the terms with power \\(n\\). This can be written using the mathematical limit notation \\(\\lim_{x \\to \\infty}\\) which describes the value that a function approaches when the independent variable increases without bound: \\[ \\lim_{x \\to \\infty} \\frac{a_0 + ... + a_n x^n}{b_0 + ... + b_n x^n} = \\frac{ a_n x^n}{ b_n x^n} = \\frac{ a_n}{ b_n} \\] Therefore, the function approaches the value of \\(a_n /b_n\\) as \\(x\\) grows. Similarly, let us consider what happens when \\(x=0\\). Plugging this into the function results in all of the terms vanishing except for the constant terms, so \\[ f(0) = \\frac{ a_0}{ b_0} \\] Between 0 and infinity, the function either increases or decreases monotonically, depending on which value (\\(a_n /b_n\\) or \\(a_0/b_0\\)) is greater. Two examples of plots of rational functions are shown in figure , which shows graphs increasing from 0 to 1. Depending on the degree of the polynomials in a rational function, it may increase more gradually (solid line) or more step-like (dashed line). The following model, called the Hill equation , describes the fraction of receptor molecules which are bound to a ligand, which is a chemical term for a free molecule that binds to another, typically larger, receptor molecule. \\(\\theta\\) is the fraction of receptors bound to a ligand, \\(L\\) denotes the ligand concentration, \\(K_d\\) is the dissociation constant, and \\(n\\) called the binding cooperativity or Hill coefficient: \\[ \\theta = \\frac{L^n}{ L^n +K_d}\\] The Hill equation is a rational function, and Figure shows plots of the graphs of two such function in the right panel. This model is further explored in exercise 2.2.10. Example. A common model of population over time is the logistic function. There are variations on how it is written down, but here is one general form: \\[\\begin{equation} f(x) = \\frac{a e^{rx} }{b+e^{rx}} \\label{eq:logistic_funk} \\end{equation}\\] The numerator and denominator both contain exponential functions with the same power. If \\(r&gt;0\\) when \\(x \\to \\infty\\), the denominator approaches \\(e^{rx}\\), since it becomes much greater than \\(b\\), and we can calculate: \\[ \\lim_{x \\to \\infty} = \\frac{a e^{rx} }{e^{rx}} = a; \\; \\mathrm{if} \\; r&gt;0 \\] On the other hand, if \\(r&lt;0\\), then the numerator approaches zero as \\(x \\to \\infty\\), and so does the function \\[ \\lim_{x \\to \\infty} = \\frac{0}{b} = 0; \\; \\mathrm{if} \\; r&lt;0 \\] Notice that switching the sign of \\(r\\) has the same effect as switching the sign of \\(x\\), since they are multiplied. Which means that for positive \\(r\\), if \\(x\\) is extended to negative infinity, the function approaches 0. This is illustrated in the second plot in Figure , which shows two logistic functions increasing from 0 to a positive level, one with \\(a=20\\) (solid line) and the second with \\(a=10\\) (dashed line). The graph of logistic functions has a characteristic sigmoidal (S-shaped) shape, and its steepness is determined by the rate \\(r\\): if \\(r\\) is small, the curve is soft, if \\(r\\) is large, the graph resembles a step function. Figure 2.2: Examples of two graphs of logistic functions (left) and two Hill functions (right). 2.2.4 Exercises: For each biological model below answer the following questions in terms of the parameters in the models, assuming all are nonnegative real numbers. 1) what is the value of the function when the independent variable is 0? 2) what value does the function approach when the independent variable goes to infinity? 3) verbally describe the behavior of the functions between 0 and infinity (e.g., function increases, decreases). Model of number of mutations \\(M\\) as a function of time \\(t\\): \\[ M(t) = M_0 + \\mu t\\] Model of molecular concentration \\(C\\) as a function of time \\(t\\): \\[ C(t) = C_0 e^{-kt} \\] Model of cooperative binding of ligands, with fraction of bound receptors \\(\\theta\\) as a function of ligand concentration \\(L\\): \\[ \\theta = \\frac{L^n}{L^n + K_d}\\] Model of tree height \\(H\\) (length) as a function of age \\(a\\) (time): \\[ H(a) = \\frac{b a }{c + a}\\] Model of concentration of a gene product \\(G\\) (concentration) as a function of time \\(t\\): \\[ G(t) = G_m (1 - e^{-\\alpha t})\\] The simplified Goldman-Hodgkin-Katz model of ionic current \\(I\\) (current) as a function of membrane potential \\(V\\) (voltage): \\[ I(V) = - b V \\frac{1- c e^{-\\alpha V}} {1- e^{-\\alpha V}} \\] Michaelis-Menten model of enzyme kinetics, \\(v\\) is reaction rate (1/time) and \\(S\\) is substrate concentration: \\[ v(S) = \\frac{v_{max} S}{K_m + S}\\] Logistic model of population growth, \\(P\\) is population size and time \\(t\\): \\[ P(t) = \\frac{A e^{kt}}{1 + B(e^{kt} -1)} \\] 2.3 Vectors and plotting in R 2.3.1 writing scripts and calling functions Programming means arranging a number of commands in a particular order to perform a task. Typing them one at a time into the command line is inefficient and error-prone. Instead, the commands are written into a file called a program or script (the name depends on the type of language; since R is a scripting language you will be writing scripts), which can be edited, saved, copied, etc. To open a new script file, in R Studio, go to File menu, and choose New R Script. This will open an editor window where you can type your commands. To save the script file (do this often!!), click the Save button (with the little floppy disk icon) or select Save from the File menu. You will also see small buttons at the top of the window that say Run, Re-run, and Source. The first two will run either the current line or a selected region of the script, while the Source button will run the entire file. Now that you know how to create a script, you should never type your R code into the command line, unless you’re testing a single command to see what it does, or looking up help. R comes equipped with many functions that correspond to standard mathematical functions. As we saw in section , exp() is the exponential function that returns \\(e\\) raised to the power of the input value. Other common ones are: sqrt() returns the square root of the input value; sin() and cos() return the sine and the cosine of the input value, respectively. Note that all of these function names are followed by parentheses, which is a hallmark of a function (in R as well as in mathematics). This indicates that the input value has to go there, for example exp(5). To compute the value of \\(e^5\\), save it into a variable called var1 and then print out the value on the screen, you can create the following script: var1 &lt;- exp(5) print(var1) ## [1] 148.4132 If you run the above code chunk in R Studio you will see two things happen: a variable named var1 appears in the Environment window (top right) with the value 148.41… and the same value is printed out in the command line window (bottom left). The most important principle of the procedural brand of programming (which includes R) is this: the computer (that is, the compiler or interpreter) evaluates the commands from top to bottom, one at a time. The variables are used with the values that they are currently assigned. If one variable (var1) was assigned in terms of another (var2), and then var2 is changed later, this does not change the value of var2. Here is an illustration of how this works: var2 &lt;- 20 var1 &lt;- var2/20 print(var2) ## [1] 20 var2 &lt;- 10 print(var1) ## [1] 1 Notice that var1 doesn’t change, because the R interpreter reads the commands one by one, and does not go back to re-evaluate the assignment for var1 after var2 is changed. Learning to think in this methodical, literal manner is crucial for developing programming skills. 2.3.2 vector variables Variables may contain more than a single number, they can also store a bunch of numbers, which is then called an array. When numbers in an array are organized as a single ordered list, this is called a vector. There are several ways of producing a vector of numbers in R. 2.3.2.1 c() function The most direct method of making a vector is to put together several values by listing them inside the function c() and assigning the output to a variable, e.g. my.vec: my.vec&lt;-c(pi,45,912.8, 0) print(my.vec) ## [1] 3.141593 45.000000 912.800000 0.000000 This variable my.vec is now a vector variable that contains four different numbers. Each of those numbers can be accessed individually by referencing its position in the vector, called the index. In the R language the the index for the first number in a vector is 1, the index for the second number is 2, etc. The index is placed in square brackets after the vector name, as follows: print(my.vec[1]) ## [1] 3.141593 print(my.vec[2]) ## [1] 45 print(my.vec[3]) ## [1] 912.8 print(my.vec[4]) ## [1] 0 2.3.2.2 the colon operator Another way to generate a sequence of numbers in a particular order is to use the colon operator, which produces a vector of integers from the first number to the last, inclusive. Here are two examples: my.vec1&lt;-1:20 print(my.vec1) ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 my.vec2&lt;-0:-20 print(my.vec2) ## [1] 0 -1 -2 -3 -4 -5 -6 -7 -8 -9 -10 -11 -12 -13 -14 -15 -16 -17 -18 ## [20] -19 -20 You can also access some but not all of the values stored in a vector simultaneously. To do this, enter a vector of positive integers inside the square brackets, either using the colon operator or using the c() function. Here are two examples, the first prints out the 4th through the 10th element of the vector my.vec1, while the second prints out the 1st, 5th, and 11th elements of the vector my.vec2: print(my.vec1[4:10]) ## [1] 4 5 6 7 8 9 10 print(my.vec2[c(1,5,11)]) ## [1] 0 -4 -10 2.3.2.3 seq() function If you want to generate a sequence of numbers with a constant difference other than 1, you’re in luck: R provides a function called seq(). It takes three inputs: the starting value, the ending value, and the step (difference between successive elements). For example, to generate a list of numbers starting at 20 up to 50, with a step size of 3, type the first command; to obtain the same sequence in reverse, use the second command: my.vec1&lt;-seq(20,50,3) print(my.vec1) ## [1] 20 23 26 29 32 35 38 41 44 47 50 my.vec2&lt;-seq(50,20,-3) print(my.vec2) ## [1] 50 47 44 41 38 35 32 29 26 23 20 2.3.2.4 rep() function Sometimes you want to create a vector of repeated values. For example, you can create a variable with 20 zeros, you can use rep() like this: zeros &lt;- rep(0,20) print(zeros) ## [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 You can repeat any value, say create a vector by repeating the number pi: pies &lt;- rep(pi,7) print(pies) ## [1] 3.141593 3.141593 3.141593 3.141593 3.141593 3.141593 3.141593 You can even repeat another vector, like the vector my.vec that was assigned above: my.vecs &lt;- rep(my.vec, 5) print(my.vecs) ## [1] 3.141593 45.000000 912.800000 0.000000 3.141593 45.000000 ## [7] 912.800000 0.000000 3.141593 45.000000 912.800000 0.000000 ## [13] 3.141593 45.000000 912.800000 0.000000 3.141593 45.000000 ## [19] 912.800000 0.000000 2.3.3 calculations with vector variables NewVec &lt;- 2*my.vec print(NewVec) ## [1] 6.283185 90.000000 1825.600000 0.000000 You can also perform calculations with multiple vector variables, but this requires extra care. R can perform any arithmetic operation with two vector variables, for instance adding two vectors results in a vector containing the sum of corresponding elements of the two vectors: my.vec1&lt;-1:5 my.vec2&lt;-0:4 print(my.vec1) ## [1] 1 2 3 4 5 print(my.vec2) ## [1] 0 1 2 3 4 sum.vec&lt;-my.vec1+my.vec2 print(sum.vec) ## [1] 1 3 5 7 9 One needs to take care that the two vectors have the same number of elements (length). If you try to operate on (e.g. add) two vectors of different lengths, R will return a warning and the result will not be what you expect: my.vec1&lt;-1:2 my.vec2&lt;-0:4 print(my.vec1) ## [1] 1 2 print(my.vec2) ## [1] 0 1 2 3 4 sum.vec&lt;-my.vec1+my.vec2 ## Warning in my.vec1 + my.vec2: longer object length is not a multiple of shorter ## object length print(sum.vec) ## [1] 1 3 3 5 5 2.3.4 Exercises The following R commands or short scripts contain errors; your job is to fix them so they runs as described. Assign a vector of three numbers to a variable: date_num &lt;- (3,8,16) Assign a range of values to a vector variable and print out the third one: the.vals &lt;- 0:10 print[the.vals(3)] Assign a range of values to a vector variable and print out the fourtieth and sixty-first values: all.the.vals &lt;- 0:100 print(all.the.vals[40,61]) Take the two vectors assigned above and assign their product to another vector: product &lt;- the.vals*all.the.vals Create a vector vec1 of ten integers and print the second and the eighth elements: vec1 &lt;- 11:20 print(vec1[2:8]) Create a vector vec1 and then multiply all of its elements by 20 and assign it to another vector: vec1&lt;-seq(-3,2,0.1) vec2 &lt;- 20vec1 Create a vector vec1, a vector vec2 and print out all the elements of the first divided by the second: vec1 &lt;- 0:5 vec2 &lt;- 3:8 print[vec1/vec2] 2.3.5 Plotting with vectors curve(x^2, 0, 10, lwd = 3, xlab = &quot;x&quot;, ylab = &quot;quadratic&quot;, cex.axis = 1.5, cex.lab = 1.5) curve(20 * exp(-0.5 * x), 0, 5, lwd = 3, xlab = &quot;x&quot;, ylab = &quot;exponential&quot;, cex.axis = 1.5, cex.lab = 1.5) Figure 2.3: Two examples of plots using curve: quadratic (\\(y=x^2\\)) and exponential (\\(y=20*e^{-0.5x}\\)) There are several ways of creating plots of mathematical functions or data R. If you want to plot a mathematical function, the simplest function is curve(). You can tell that this is a function, because it uses parentheses; the first input is an expression for the function, and the next two define the range of the independent variable over which to plot the graph. Two examples of plotting a quadratic function over the range from 0 and 5, and an exponential variation over the range of 0 to 10 are shown in figure . One can change the default look of the plot produced by curve by setting different options, which are optional inputs into the curve function, One is the line width lwd which can be increased from the default value of 1 to produce thicker curves, as demonstrated in the example above. One can add labels on the x and y axes with xlab and ylab options, respectively; note that these are strings of characters, and thus must be put in quotes to differentiate them from a variable name. There is one very important option not shown above: that of overlaying a curve on top of an existing plot, which is done by typing add=TRUE. This option takes logical (Boolean) values TRUE and FALSE, which must be typed in all caps and without quotes. 2.3.5.1 plot() function In addition to curve, one can use the function plot() in R to create two dimensional graphs from two vector-valued variables of the same length, e.g. plot(x,y). The first input variable corresponds to the independent variable (e.g. x), which is plotted on the x-axis, and the second variable corresponds to the dependent variable (e.g. y) which is plotted on the y-axis. In figure you see graphs of exponential and logistic function plotted using plot(). The following chunk creates a vector variable time, then calculates a new variable quad using time in a single operation: time &lt;- 0:10 quad &lt;- (time - 5)^2 print(time) ## [1] 0 1 2 3 4 5 6 7 8 9 10 print(quad) ## [1] 25 16 9 4 1 0 1 4 9 16 25 This chunk plots the two vector variables quad as a function of time, and adds a title to the plot plot(time, quad, main = &quot;Quadratic function of time&quot;) The default plot style in R uses circles to indicate each plotted point. To change it, you need to set the option t (type), for example, setting t='l' (the lowercase letter L) produces a continuous line connecting the individual data points. plot(time, quad, main = &quot;Quadratic function of time&quot;, type = &quot;l&quot;, xlab = &quot;time&quot;, ylab = &quot;y = f(t)&quot;) plot() is a versatile function that has many options function has many options which can be changed to determine the color, the style, and other attributes of the plot. For a full list type help(plot) in the console or type plot in the search bar of the Help pane in the bottom right window. 2.3.5.2 using lines() or points() You may also want to plot multiple graphs on the same figure. The plot() function creates a new plot window, so if you want to add another plot on top of the first one, you have to use another function. There are two ones available: lines() which produces continuous curves connecting the points, and points() which plots individual symbols at every point. Let us illustrate this by plotting two different exponential functions on one plot, and two different logistic functions on the second one, which were discussed in section . When you’ve got multiple plots on the same figure, they need to be distinct and labeled. To distinguish them, below I use the option col to specify the color of the plot, and I add a legend describing the parameters of each plot to the figure . The function has a lot of options, so if you want to understand the details, type help(legend) in the prompt or go to Help tab in the lower right frame of R Studio and type legend. x &lt;- seq(0, 10, 0.5) y &lt;- 10 + 20 * exp(-0.5 * x) plot(x, y, xlab = &quot;x&quot;, ylab = &quot;exponential&quot;, col = 1, lwd = 3) y &lt;- 10 + 20 * exp(-2 * x) lines(x, y, col = 2, lwd = 3) leg.txt = c(&quot;b=10,a=20,r=-0.5&quot;, &quot;b=10,a=20,r=-2&quot;) legend(&quot;topright&quot;, leg.txt, col = 1:2, pch = c(1, NA), lty = c(0, 1), lwd = 3) x &lt;- seq(-10, 10, 1) y &lt;- 20 * exp(0.5 * x)/(1 + exp(0.5 * x)) plot(x, y, xlab = &quot;x&quot;, ylab = &quot;logistic&quot;, col = 4, lwd = 3) y &lt;- 20 * exp(1.5 * x)/(1 + exp(1.5 * x)) lines(x, y, col = 2, lwd = 3) leg.txt = c(&quot;a=20,b=1,r=0.5&quot;, &quot;a=20,b=1,r=1.5&quot;) legend(&quot;topleft&quot;, leg.txt, col = c(4, 2), pch = c(1, NA), lty = c(0, 1), lwd = 3) Figure 2.4: Overlaying multiple plots in R: two exponential functions of the form \\(y=b+ae^{rx}\\) on the left, two logistic functions of the form \\(y= ae^{rx}/(b+e^{rx})\\) on the right. 2.3.6 Exercises The following R commands or short scripts contain errors; your job is to fix them so they runs as described. Multiply a vector by a constant and add another constant and assign the result to a vector: new.vals &lt;- 5 + 8the.vals Assign range to be a sequence of values from 0 to 100 with step of 0.1, and calculate the vector variable result as the square of the vector variable range: range &lt;- seq(0,0.1,100) result &lt;- square(range) Plot result as a function of range: plot(result, range) Plot the graph of the function \\(f(x) = (45-x)/(4x+3)\\) over the range of 0 to 100: curve((45-x)/(4x+3), 0, 100) Plot a quadratic function with specified coefficients \\(a\\), \\(b\\), \\(c\\) over a given range of independent variable \\(x\\): a&lt;-10 b&lt;- -15 c&lt;- 5 y&lt;-a*x^2+b*x+c x&lt;-seq(-0.5,2,0.01) plot(x,y,type=&#39;l&#39;) Overlay two different plots of the logistic function with different values of the parameter \\(r\\): time&lt;-0:100 a&lt;-1000 b&lt;-50 r&lt;-0.1 Population&lt;-a*exp(r*time)/(b+exp(r*time)) plot(time,Population,type=&#39;l&#39;) r&lt;-10 lines(time,Population,col=2) 2.4 Rates of biochemical reactions Living things are dynamic, they change with time, and much of mathematical modeling in biology is interested in describing these changes. Some quantities change fast and others slowly, and every dynamic quantity has a rate of change, or rate for short. Usually, the quantity that we want to track over time is the variable, and in order to describe how it changes we introduce a rate parameter. If we are describing changes over time, all rate parameters have dimensions with time in the denominator. As a simple example, the velocity of a physical object describes the change in distance over time, so its dimension is \\([v] = length/time\\). On the most fundamental level, the work of life is performed by molecules. The protein hemoglobin transports oxygen in the red blood cells, while neurotransmitter molecules like serotonin carry signals between neurons. Enzymes catalyze reactions, like those involved in oxidizing sugar and making ATP, the energy currency of life. Various molecules bind to DNA to turn genes on and off, while myosin proteins walk along actin fibers to create muscle contractions. In order to describe the activity of biological molecules, we must measure and quantify them. However, they are so small and so numerous that it is not usually practical to count individual molecules (although with modern experimental techniques it is sometimes possible). Instead, biologists describe their numbers using concentrations. Concentration has dimensions of number of molecules per volume, and the units are typically molarity, or moles (\\(\\approx 6.022*10^{23}\\) molecules) per liter. Using concentrations to describe molecule rests on the assumption that there are many molecules and they are well-mixed, or homogeneously distributed throughout the volume of interest. Molecular reactions are essential for biology, whether they happen inside a bacterial cell or in the bloodstream of a human. Reaction kinetics refers to the description of the rates, or the speed, of chemical reactions. Different reactions occur with different rates, which may be dependent on the concentration of the reactant molecule. Consider a simple reaction of molecule \\(A\\) (called the substrate) turning into molecule \\(B\\) (called the product), which is usually written by chemists with an arrow: \\[ A \\xrightarrow{k} B \\] But how fast does the reaction take place? To write down a mathematical model, we need to define the quantities involved. First, we have the concentration of the molecule \\(A\\), with dimensions of concentration. Second, we have the rate of reaction, let us call it \\(v\\), which has dimension of concentration per time (just like velocity is length per time). How are the two quantities related? 2.4.1 Constant (zeroth-order) kinetics In some circumstances, the reaction rate \\(v\\) does not depend on the concentration of the reactant molecule \\(A\\). In that case, the relationship between the rate constant \\(k\\) and the actual rate \\(v\\) is: \\[\\begin{equation} v = k \\label{eq:kinetics_0th_order} \\end{equation}\\] Dimensional analysis insists that the dimension of \\(k\\) must be the dimension of \\(v\\), or concentration/time. This is known as constant, or zero-order kinetics, and it is observed at concentrations of \\(A\\) when the reaction is at its maximum velocity: for example, ethanol metabolism by ethanol dehydrogenase in human liver cannot proceed any faster than about 1 drink per hour. 2.4.2 First-order kinetics . In other conditions, it is easy to imagine that increasing the concentration of the reactant \\(A\\) will speed up the rate of the reaction. A simple relationship of this type is linear: \\[\\begin{equation} v = kA \\label{eq:kinetics_1st_order} \\end{equation}\\] In this case, the dimension of the rate constant \\(k\\) is 1/time. This is called first-order kinetics, and it usually describes reactions when the concentration of \\(A\\) is small, and there are plenty of free enzymes to catalyze more reactions. 2.4.3 Michaelis-Menten model of enzyme kinetics However, if the concentration of the substrate molecule \\(A\\) is neither small nor large, we need to consider a more sophisticated model. An enzyme is a protein which catalyzes a biochemical reaction, and it works in two steps: first it binds the substrate, at which point it can still dissociate and float away, and then it actually catalyzes the reaction, which is usually practically irreversible (at least by this enzyme) and releases the product. The enzyme itself is not affected or spent, so it is free to catalyze more reactions. Let denote the substrate (reactant) molecule by \\(A\\), the product molecule by \\(B\\), the enzyme by \\(E\\), and the complex of substrate and enzyme \\(AE\\). The classic chemical scheme that describes these reactions is this: \\[ A + E \\underset{k_{-1}}{\\overset{k_1}{\\rightleftharpoons}} AE \\xrightarrow{k_2} E + B \\] You could write three different kinetic equations for the three different arrows in that scheme. Michaelis and Menten used the simplifying assumptions that the binding and dissociation happens much faster than the catalytic reaction, and based on this they were able to write down an approximate, but extremely useful Michaelis-Menten model of an enzymatic reaction: \\[\\begin{equation} v = \\frac{v_{max} A}{K_M+A} \\label{eq:kinetics_MM_kinetics} \\end{equation}\\] Here \\(v\\) refers to the rate of the entire catalytic process, that is, the rate of production of \\(B\\), rather than any intermediate step. Here the reaction rate depends both on the concentration of the substrate \\(A\\) and on the two constants \\(v_{max}\\), called the maximum reaction rate, and the constant \\(K_M\\), called the Michaelis constant. They both depend on the rate constants of the reaction, and \\(v_{max}\\) also depends on the concentration of the enzyme. The details of the derivation are beyond us for now, but you will see in the following exercises how this model behaves for different values of \\(A\\). %Make a box? %Chemical kinetics models are important in a variety of biological fields. The concentration of a drug in the bloodstream is described by an equation that involves the rate of application of the drug and the rate of its removal, known as drug metabolism. The concentration of a sugar, such as fructose, in a cell an be modeled as a system of differential equations that describe all the reactions in the process of glycolysis. Many important molecules exist primarily to regulate each other, for instance, the product of a gene (the protein which is produced when the gene is expressed) can be used to turn on another gene, whose product may turn off the first gene. Such networks of interactions can also be described by a bunch of kinetics equations; the growing field of systems biology studies their complex behaviors. "],
["describing-data-sets.html", "3 Describing data sets 3.1 Mutations and their rates 3.2 Describing data sets 3.3 Working with data in R", " 3 Describing data sets Get your facts first, and then you can distort them as much as you please. – Rudyard Kipling, An Interview with Mark Twain Science begins with experimental measurements, which are then verified by reproducing the results. But no experimental result is perfectly reproducible because all are subject to random noise, whether it is caused by unpredictable processes or is due to measurement error. Describing collections of numbers with noise is the first step to understanding the biological systems that are being measured. In this chapter you will learn to do the following: calculate means and medians of a data set calculate variances and standard deviations produce histograms and interpret them use R to plot and analyze data sets 3.1 Mutations and their rates All Earth-based lifeforms receive an inheritance from their parent(s): a string of deoxyribonucleic acids ( DNA) called the genetic sequence, or genome of an individual. The information to produce all the necessary components to build and run the organism is encoded in the sequence of the four different nucleotides: adenine, thymine, guanine, and cytosine (abbreviated as A, T, G, C). Different parts of the genome play different roles; some discrete chunks called genes contain the instructions to build proteins, the workhorses of biology. To make a protein from a gene, the information is transcribed from DNA into messenger ribonucleic acid ( mRNA), which is then translated into a string of amino acids which constitute the protein. The genetic code determines the translation, using three nucleic acids in DNA and RNA to represent a single amino acid in a protein. Thus, a sequence of DNA always results in a specific sequence of amino acids, which determine the structure and function of the protein. Different types of substitution point mutations are distinguished by their effects on the gene products; image by Jonsta247 in public domain via Wikimedia Commons. The above processes involve copying and transferring information. As we know from experience, copying information inevitably means introducing errors. This is particularly important when passing information from parent to offspring, because then an entire organism has to develop and live based on a faulty blueprint. Changes introduced in the genome of an organism are called mutations, and they can be caused either by errors in copying DNA when making a new cell (replication) or through damage to DNA through physical means (e.g. ionizing radiation) or chemical mechanisms (e.g. exogenous molecules that react with DNA). The simplest mutation involve a single nucleotide and are called point mutations. A nucleotide may be deleted, an extra nucleotide inserted, or a new one substituted instead: the three different types of substitution mutations are shown in figure . Large-scale mutations may involve whole chunks of the genome that are cut out and pasted in a different location, or copied and inserted in another position, but they are typically much more rare than point mutations. Mutations can have different effects on the mutant organism, although acquisition of super-powers has not been observed. Usually, point mutations have either little observable effect or a negative effect on the health of the mutant. A classic example is sickle-cell disease, in which the molecules of the protein hemoglobin, responsible for carrying oxygen in the blood from the lungs to the tissues, tends to stick together and clump, resulting in sickle-shaped red blood cells. The disease is caused by a single substitution mutation in the gene that codes for one of the two components of hemoglobin, called \\(\\beta\\)-globin. The substitution of a single nucleotide in the DNA sequence changes one amino acid in the protein from glutamate to valine, which causes the proteins to aggregate. This missense}* mutation (see figure ) is carried by a fraction of the human population, and those who inherit the allele allele from both parents develop the painful and sometimes deadly disease. Such mutations that are present in some but not all of a population are called polymorphisms, to distinguish them from mutations that occurred in evolutionary lineages and differentiate species from each other. One of the central questions of evolutionary biology is how frequently do mutations occur? Since mutations are generally undesirable, most living things have developed ways to minimize the frequency of errors in copying DNA, and to repair DNA damage. But although mutations are rare, they occur spontaneously in all organisms because molecular processes such as copying a DNA molecule are subject to random noise arising from thermal motion. So mutations are fundamentally a random process and we need to use descriptive statistics to analyze data with inherent randomness. 3.2 Describing data sets 3.2.1 central value of a data set A data set is a collection of measurements. These measurements can come from many kinds of sources, and can represent all sorts of quantities. One big distinction is between numerical and categorical data sets. Numerical data sets contain numbers, either integers or real numbers. Some examples: number of individuals in a population, length, blood pressure, concentration. Categorical data sets may contain numbers, symbols, or words, limited to a discrete, usually small, number of values. The word categorical is used because this kind of data corresponds to categories or states of the subject of the experiment. Some examples: genomic classification of an individual on the basis of one locus (e.g. wild type or mutant), the state of an ion channel (open or closed), the stage of a cell in the cell cycle. A data set contains more than one measurement, the number of them is called the size of the data set and is usually denoted by the letter \\(n\\). To describe a data set numerically, one can use numbers called statistics (not to be confused with the branch of science of the same name). The most common statistics aim to describe the central value of the data set to represent a typical measurement. If you order all of the measurements from highest to lowest and then take the the middle value, you have found the median (if there is an even number of values, take the average between the middle two). Precisely half of the data values are less than the median and the other half are greater, so it represents the true ``middle’’ value of the measurement. Note that the median can be calculated either for numerical or categorical data, as long as the categories can be ordered in some fashion. The value that occurs most frequently in the data set is called its . For some data sets, particularly those which are symmetric, the mode coincides with the mean (see next paragraph) and the median, but for many others it is distinct. The mode is the most visual of the three statistics, as it can be picked out from the histogram plot of a data set (which is described in subsection 3.2.3) as the value corresponding to the maximum frequency. The mode can also be used for both categorical and numerical data. The average or mean of a data set is the sum of all the values divided by the number of values. It is also called the expected value (particularly in the context of probability, which we will discuss later) because it allows to simply predict the sum of a large number of measurements with a given mean, by multiplying the mean by the number. The mean can be calculated only for a numerical data set, since we cannot add non-numerical values. The definition of the mean of a data set \\(X\\) is the same as for the average and is usually indicated with a bar over the letter for the measured data variable: \\[\\begin{equation} \\bar X = \\frac{1}{n} \\sum_{i=1}^n x_i \\label{eq:ch3_mean_def} \\end{equation}\\] The mean, unlike the median, is not the middle value of the data set, instead it represents the center of mass of the measured values . Another way of thinking of the mean is as a weighted sum of the values in the data set. The weights represent the frequency of occurrence of each numeric value in the data set, which we will further discuss in subsection 3.2.3. The mean is the most frequently used statistic, but it is not always interpreted correctly. Very commonly the mean is reported as the most representative value of a data set, but that is often misleading. Here are at least two situations in which the mean can be tricky: 1) data sets with a small number of discrete values; 2) data sets with outliers, or isolated numbers very far from the mean. Examples of misleading means. Mean quantities for data sets with a few quantities are not the typical value, such as in the number of children born in a year per individual, also known as the birth rate. The birth rate per year in 2013 for both the United States and Russia is 1.3% per person, but you will have to look for a long time to find any individual who gave birth to 1.3% of a child. While this point may be obvious, it is often overlooked when interpreting mean values. Outliers are another source of trouble for means. For example, a single individual (let’s call him or her B.G.) with a wealth of $50 billion moves into a town of 1000 households with average wealth of $100,000. Although none of the original residents’ assets have changed, the mean wealth of the town improves dramatically, as you can calculate in one of the exercises at the end of the chapter. One can site the improved per capita (per individual) in the town as evidence of economic growth, but that is obviously misleading. In cases with such dramatic outliers, the median is more informative as representation of a typical value of the data set. 3.2.2 Exercises For the (small) data sets given below, calculate the mean and the median (by hand or using a calculator) and compare the two measures of the center. Data set of the population of the city of Chicago (in millions) in the last 4 census years (2010, 2000, 1990, 1980): {2.7, 2.9, 2.8, 3.0}. Data set of the numbers of the fish blacknose dace (Rhinichthys atratulus) collected in 6 different streams in the Rock Creek watershed in Maryland: {76, 102, 12, 55, 93, 98}. Data set of tuberculosis incidence rates (per 100,000 people) in the 5 largest metropolitan areas in the US in 2012: {5.2, 6.6, 3.2, 5.5, 4.5}. Data set of ages of mothers at birth for five individuals: {19, 20, 22, 32, 39}. Data set of ages of fathers at birth for five individuals: {22, 23, 25, 36, 40}. Data set of the number of new mutations found on maternal chromosomes for five individuals: {9, 10, 11, 26, 15}. Data set of the number of new mutations found on paternal chromosomes for five individuals: {39, 43, 51, 53, 91}. Consider the hypothetical town with 1000 households with mean and median wealth of $100,000 and one person with assets for $50 billion. Calculate the mean value of the combined data set, and compare it to the new median value. Suppose you’d like to add a new observation to a data set; e.g. the 6-th largest metropolitan area (Philadelphia) to the tuberculosis incidence data set, which is 3.0. Calculate the mean of the 6-values data set, without using the 5 values in the original data set, but only using the mean of the 5-value data set and the new value. Generalize this to calculating the sample mean for any \\(n\\)-value data set, given the mean of the \\(n-1\\) values, plus one new value. 3.2.3 spread of a data set The center of a data set is obviously important, but so is the spread around the center. Sometimes the spread is caused by noise or error, for example in a data set of repeated measurements of the same variable under the same conditions. Other times the variance is due to real changes in the system, or due to inherent randomness of the system, and the size of the spread, as well as the shape of the histogram are important for understanding the mechanism. The simplest way to describe the spread of a numerical data set is to look at the difference between the maximum and minimum values, called the range. However, it is obviously influenced by outliers, since the extreme values are used. To describe the typical spread, we need to use all the values in the data set, and see how far each one is from the center, measured by the mean. There is a problem with the naive approach: if we just add up all the differences of data values from the mean, the positives will cancel the negatives, and we’ll get an artificially low spread. One way to correct this is to take the absolute value of the differences before adding them up. However, for somewhat deep mathematical reasons, the standard measure of spread uses not absolute values, but squares of the differences, and then divides that sum not by the number of data points \\(n\\) but by \\(n-1\\). This is called the variance of a data set, and it is calculated by the formula: \\[\\begin{equation} Var(X) = \\frac{1}{n-1} \\sum_{i=1}^n (\\bar X - x_i)^2 \\end{equation}\\] The variance is a sum of square differences, so its dimension is the square of the dimensions of the measurements in \\(X\\). In order to obtain a measure of the spread comparable to the values of \\(X\\), we take the square root of variance and call it the standard deviation of the data set \\(X\\): \\[\\begin{equation} \\sigma(X) = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (\\bar X - x_i)^2} \\end{equation}\\] Just as the mean is a weighted average of all of the values in the data set, the variance is a weighted average of all the squared deviations of the data from the mean. 3.2.4 Exercises: For the (small) data sets below, calculate the range, variance, and standard deviation (by hand or using a calculator). Compare the range and the standard deviation for each case: which one is larger? by how much? Data set of the population of the city of Chicago (in millions) in the last 4 census years (2010, 2000, 1990, 1980): {2.7, 2.9, 2.8, 3.0}. Data set of the numbers of the fish blacknose dace (Rhinichthys atratulus) collected in 6 different streams in the Rock Creek watershed in Maryland: {76, 102, 12, 55, 93, 98}. Data set of tuberculosis incidence rates (per 100,000 people) in the 5 largest metropolitan areas in the US in 2012: {5.2, 6.6, 3.2, 5.5, 4.5}. Data set of ages of mothers at birth for five individuals: {19, 20, 22, 32, 39}. Data set of ages of fathers at birth for five individuals: {22, 23, 25, 36, 40}. Data set of the number of new mutations found on maternal chromosomes for five individuals: {9, 10, 11, 26, 15}. Data set of the number of new mutations found on paternal chromosomes for five individuals: {39, 43, 51, 53, 91}. Consider the hypothetical town with 1000 households with mean and median wealth of $100,000 and one person with assets for $50 billion. Calculate the mean value of the combined data set, and compare it to the new median value. Suppose that a data set has a fixed range (e.g. all values have to lie between 0 and 1). What is the greatest possible standard deviation for any data set within the range? Hint: think about how to place the points as far from the mean as possible. How do the data sets above relate to your prediction?} 3.2.5 describing data sets in graphs Data sets can be presented visually to indicate the frequency of different values. This can be done in a number of ways, depending on the kind of data set. For a data set with only a few values, e.g. a categorical data set, a good way to represent it is with a pie chart. Each category is represented by a slice of the pie with the area of the same share of the pie as the fraction of the data set in the category. There is some evidence, however, that pie charts can be misleading to the eye, so R does not recommend using them. For a numerical data set it is useful to plot the frequencies of a range of values, which is called a histogram. Its independent axis has the values of the data variable, and the dependent axis has the frequency of those values. If the data set consists of real numbers that range across an interval, that interval is divided into subintervals (usually of equal size), called bins, and the number of measurements in each bin is indicated on the y-axis. In order to be visually informative, there should be a reasonable number (usually no more than a few dozen, although it varies) of bins. The most frequent/common measurements are represented as the highest bars or points on the histogram. Histograms can denote either the counts of measurements in each bin, or to show the fraction of the total number of measurements in each bin. The only difference between those two kinds of histogram is the scale of the y-axis, and, confusingly, both can be called frequencies. Figure 3.1: Length of bacteria Bacillus subtilis measured under the microscope as discrete values with step of 0.5; data from citep{watkins-intro-stats} A histogram of the measured lengths of the bacterium Bacillus subtilis is shown in figure . The data set was measured in increments in half a micron, with numbers varying between 1.5 and 4.5 microns. The histogram shows that the most common measurement (the mode) is 2 \\(\\mu m\\). Adding up all of the frequencies in the histogram tells us that there are approximately 200 total values in the data set. This allows us to find the median value by counting the frequencies of the first few bins until we get to 100 (the median point), which resides in the bin for 2.5 \\(\\mu m\\). It is a little bit more difficult to estimate the mean, but it should be clear that the center of mass of the histogram is also near 2.5 (it is actually 2.49). Finally, the hardest task is estimating the spread of the data set, such as the the standard deviation, based on the histogram. The range of the data set is \\(4.5-1.5 = 3\\), so we know for sure that it is less than 1.5. The histogram shows that the deviations from the mean value of 2.5 range from 2 (rarely) to 0.5 (most prevalent). This should give you an idea that the weighted average of the deviations is less than 1. Indeed, the correct standard deviation is about 0.67. There are different ways of plotting data sets that have more than one variable. For instance, a data set measured over time is called a time series. If the values are plotted with the corresponding times on the x-axis, then it is called a time plot. This is useful to show the changes of the values of your variable over time. If the data set doesn’t undergo any significant changes over time, it makes more sense to represent it as a pie chart or histogram. More generally, one may plot two variables measured together on a single plot, which is called a scatterplot. We will explore such plots and the relationships between two measured variables in chapter 4. 3.2.6 Exercises Answer the following questions, based on the histograms in figure (mutation data) and in figure (heart rate data). How many people in the mutation data have fathers either younger than 20 or older than 40? How many have more than 80 new mutations? Estimate the median and mean of the two variables in the mutation data set. State the range of each data set, and estimate the standard deviation of the two variables in the mutation data set. How many people in the heart rate data have heart rates greater than 80 bpm? How many have body temperature less that 97 F? Estimate the median and mean of the two variables in the heart rate data set. State the range of each data set, and estimate the standard deviation of the two variables in the heart rate data set. my_data &lt;- read.table(&quot;data/HR_temp.txt&quot;, header = TRUE) hist(my_data$HR, col = &quot;gray&quot;, main = &quot;Heart rate data&quot;, xlab = &quot;heart rate (bpm)&quot;) hist(my_data$Temp, col = &quot;gray&quot;, main = &quot;Body temperature data&quot;, xlab = &quot;temperature (F)&quot;) Figure 3.2: Histograms of heart rates and body temperatures Figure 3.3: Histograms of paternal ages and the number of new mutations from 73 families; data from citep{kong_rate_2012} 3.3 Working with data in R 3.3.1 reading in data into data frames One way to input data into R is to read in a text file, where several variables are stored in columns. For instance, the file HR_temp.txt contains three variables: body temperature (in Fahrenheit), sex (1 for male, 2 for female), and heart rate (in beats per minute). The values for the variables are arranged in columns, while first row of the file contains the names of the variables (Temp, Sex, and HR, respectively). Note that the data file has to be saved into the same folder as the .Rmd file week1.Rmd for this to work. vitals &lt;- read.table(file = &quot;data/HR_temp.txt&quot;, header = TRUE) plot(vitals$HR, vitals$Temp, main = &quot;Body temp as function of heart rate&quot;, xlab = &quot;heart rate (bpm)&quot;, ylab = &quot;body temperature (F)&quot;) mTemp &lt;- mean(vitals$Temp) sdTemp &lt;- sd(vitals$Temp) abline(mTemp, 0) mean(vitals$HR) ## [1] 73.76154 sd(vitals$HR) ## [1] 7.062077 The R command read.table() reads this file and and puts it into a data frame called data. The three variables are stored inside the data frame, and can be accessed by appending the dollar sign and variable name to the data frame, so data$HR refers to only the heart rates, and data$Temp refers to the body temperatures. The plot shows the relationship of the two data variables, and the funtion abline(98.6,0) plots a line with the intercept 98.6a and slope 0 on top of the scatterplot. You can also load data from a package, e.g. HistData, which contains many classic data sets. Got to the Packages tab in the lower right window in R Studio, click Install and type HistData. We will use the data set called Galton that contains the heights of parents (the mean of mother’s and father’s) and their children, in variables parent and child. The script below plots the two variables, with parent as the independent (explanatory) variable and child as the dependent (response) variable. library(HistData) plot(Galton$parent, Galton$child, main = &quot;Height of child vs average height of parents&quot;, xlab = &quot;parent height (inches)&quot;, ylab = &quot;child height (inches)&quot;) summary(Galton$parent) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 64.00 67.50 68.50 68.31 69.50 73.00 summary(Galton$child) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 61.70 66.20 68.20 68.09 70.20 73.70 3.3.2 descriptive statistics One can also calculate basic descriptive statistics as follows: paste(&quot;The mean parental height is:&quot;, mean(Galton$parent)) ## [1] &quot;The mean parental height is: 68.3081896551724&quot; paste(&quot;The mean child height is:&quot;, mean(Galton$child)) ## [1] &quot;The mean child height is: 68.0884698275862&quot; paste(&quot;The standard deviation of parental height is:&quot;, sd(Galton$parent)) ## [1] &quot;The standard deviation of parental height is: 1.78733340172202&quot; paste(&quot;The standard deviation of child height is:&quot;, sd(Galton$child)) ## [1] &quot;The standard deviation of child height is: 2.51794136627677&quot; Why do you think the standard deviation of parental height is much smaller? R has histogram function hist(), which does a passable job of representing the distribution of a variable such as child height or parent height. Compare the width of the two distributions and consider why they are different. hist(Galton$child) hist(Galton$parent) 3.3.3 Exercises: The following code chunks contain errors. Find and fix them so they work as intended. Calculate the mean and standard deviation of the heart rates of the first 30 individuals in the data frame vitals: mean(vitals$HR[30]) ## [1] 64 sd(vitals$HR[30]) ## [1] NA Calculate the mean and standard deviation of the ratio of heart rates to body temperatures for the data set vitals: mean(vitals$HR/Temp) sd(vitals$HR/Temp) Plot a scatterplot of the child heights as the response variable and the parent heights and the explanatory variable, and overlay the line y=x on top. plot(parent, child, main = &#39;Height of child vs average height of parents&#39;, xlab= &#39;child height (inches)&#39;, ylab= &#39;parent height (inches)&#39;) abline(1,0) Calculate the median of both parent and child heights: median(Galton) Plot the histogram of parent heights of the first half of the group: hist(Galton$parent/2) Plot the histogram for the ratio of parent and child heights for the entire data set and calculate its mean and variance: hist(Galton$parent/child) mean(Galton$parent/child) sd(Galton$parent/child) "],
["linear-regression.html", "4 Linear regression 4.1 Linear relationship between two variables} 4.2 Linear least-squares fitting 4.3 Linear regression using R 4.4 Regression to the mean 4.5 Linear regression: parameters, plots, and residuals", " 4 Linear regression The place in which I’ll fit will not exist until I make it. James Baldwin In the last two chapters we learned to use data sets which fall into a few categories. We now turn to data which can be measured as a range of numerical values. We can ask a similar question of numerical data that we asked of categorical: how can we tell whether two variables are related? And if they are, what kind of relationship is it? This takes us into the realm of data fitting, raising two related questions: what is the best mathematical relationship to describe a data set? and what is the quality of the fit? You will learn to do the following in this chapter: define the quality of the fit between a line and a two-variable data set calculate the parameters for the best-fit line based on statistics of the data set use R to calculate and plot best-fit line for a data set understand the meaning of correlation and covariance understand the phenomenon of regression to the mean 4.1 Linear relationship between two variables} Although there is always error in any real data, there may be a relationship between the two variables that is not random: say, when one goes up, the other one tends to go up as well. These relationships may be complicated, so in this chapter we will focus on the the simplest and most common type of relationship: linear, where a change in one variable is associated with a proportional change in the other, plus an added constant. This is expressed mathematically using the familiar equation for a linear function, with parameters slope (\\(m\\)) and intercept (\\(b\\)): \\[ y = mx + b\\] Let us say you have measured some data for two variables, which we will call, unimaginatively, \\(x\\) and \\(y\\). This data set consists of pairs of numbers: one for \\(x\\), one for \\(y\\), for example, the heart rate and body temperature of a person go together. They cannot be mixed up between different people, as the data will lose all meaning. We can denote this a list of \\(n\\) pairs of numbers: \\((x_i, y_i)\\) (where \\(i\\) is an integer between 1 and \\(n\\)). Since this is a list of pairs of numbers, we can plot them as separate points in the plane using each \\(x_i\\) as the x-coordinate and each \\(y_i\\) as the y-coordinate. This is called a scatterplot of a two-variable data set. For example, two scatterplots of a data set of heart rate and body temperature are shown in figure . In the first one, the body temperature is on the x-axis, which makes it the explanatory variable; in the second one, the body temperature is on the y-axis, which makes it the response variable. data &lt;- read.table(&quot;data/HR_temp.txt&quot;, header = TRUE) plot(data$Temp, data$HR, main = &quot;heart rates vs. body temps&quot;, cex = 1.5, cex.axis = 1.5, cex.lab = 1.5) plot(data$HR, data$Temp, main = &quot;body temps vs. heart rates&quot;, cex = 1.5, cex.axis = 1.5, cex.lab = 1.5) Figure 4.1: Scatterplot of heart rates and body temperatures: a) with heart rate as the explanatory variable; b) with body temperature as the explanatory variable. 4.2 Linear least-squares fitting 4.2.1 sum of squared errors It is easy to find the best-fit line for a data set with only two points: its slope and intercept can be found by solving the two simultaneous linear equations, e.g. if the data set consists of \\((3,2.3), (6, 1.7)\\), then finding the best fit values of \\(m\\) and \\(b\\) means solving the following two equations: \\[\\begin{eqnarray*} 3m + b &amp;=&amp; 2.3 \\\\ 6m + b &amp;=&amp; 1.7 \\end{eqnarray*}\\] These equations have a unique solution for each unknown: \\(m=-0.2\\) and \\(b=2.9\\) (you can solve it using basic algebra). However, a data set with two points is very small and cannot serve as a reasonable guide for finding a relationship between two variables. Let us add one more data point, to increase our sample size to three: \\((3,2.3), (6, 1.7), (9, 1.3)\\). How do you find the best fit slope and intercept? take two points and find a line, that is the slope and the intercept, that passes through the two. It should be clear why this is a bad idea: we are arbitrarily ignoring some of the data, while perfectly fitting two points. So how do we use all the data? Let us write down the equations that a line with slope \\(m\\) and intercept \\(b\\) have to satisfy in order to fit our data points: \\[\\begin{eqnarray*} 3m + b &amp;=&amp; 2.3 \\\\ 6m + b &amp;=&amp; 1.7 \\\\ 9m + b &amp;=&amp; 1.3 \\end{eqnarray*}\\] This system has no exact solution, since there are three equations and only two unknowns. We need to find \\(m\\) and \\(b\\) such that they are a ``best fit’’ to the data, not the perfect solution. To do that, we need to define what we mean by the goodness of fit. One simple way to asses how close the fit is to the data is to subtract the predicted values of \\(y\\) from the data, as follows: \\(e_i = y_i - (mx_i + b)\\). The values \\(e_i\\) are called the errors or residuals of the linear fit. If the values predicted by the linear model (\\(mx_i+b\\)) are close to the actual data \\(y_i\\), then the error will be small. However, if we add it all up, the errors with opposite signs will cancel each other, giving the impression of a good fit simply if the deviations are symmetric. A more reasonable approach is to take absolute values of the deviations before adding them up. This is called the total deviation, for \\(n\\) data points with a line fit: \\[ TD = \\sum_{i=1}^n | y_i - mx_i - b | \\] Mathematically, a better measure of total error is a sum of squared errors, which also has the advantage of adding up nonnegative values, but is known as a better measure of the distance between the fit and the data (think of Euclidean distance, which is also a sum of squares) : \\[ SSE = \\sum_{i=1}^n ( y_i - mx_i - b )^2 \\] Thus we have formulated the goal of fitting the best line to a two-variable data set, also known as linear regression: find the values of slope and intercept that result in the lowest possible sum of squared errors. There is a mathematical recipe which produces these values, which will be described in the next section. Any model begins with assumptions and in order for linear regression to be a faithful representation of a data set, the following must be true: the variables have a linear relationship all of the measurements are independent of each other there is no noise in the measurements of the explanatory variable the noise in the measurements of the response variable is normally distributed with mean 0 and identical standard deviation The reasons why these assumptions are necessary for linear regression to work are beyond the scope of the text, and they are elucidated very well in the book Numerical Recipes . However, it is important to be aware of them because if they are violated, the resulting linear fit may be meaningless. It’s fairly clear that if the first assumption is violated, you are trying to impose a linear relationship on something that is actually curvy. The second assumption of independence is very important and often overlooked. The mathematical reasons for it have to do with properly measuring the goodness of fit, but intuitively it is because measurements that are linked can introduce a new relationship that has to do with the measurements, rather than the relationship between the variables. Violation of this assumption can seriously damage the reliability of the linear regression. The third assumption is often ignored, since usually the explanatory variable is also measured and thus has some noise. The reason for it is that the measure of goodness of fit is based only on the response variable, and there is no consideration of the noise in the explanatory variable. However, a reasonable amount of noise in the explanatory variable is not catastrophic for linear regression. Finally, the last assumption is due to the statistics of maximum-likelihood estimation of the slope and intercept, but again some deviation from perfect normality (bell-shaped distribution) of the noise, or slightly different variation in the noise is to be expected. 4.2.2 best-fit slope and intercept Definition 4.1 The covariance of a data set of pairs of values \\((X,Y)\\) is the sum of the products of the corresponding deviations from their respective means: \\[ Cov(X,Y) = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar X) (y_i - \\bar Y) \\] Intuitively, this means that if two variable tend to deviate in the same direction from their respective means, they have a positive covariance, and if they tend to deviate in opposite directions from their means, they have a negative covariance. In the intermediate case, if sometimes they deviate together and other times they deviate in opposition, the covariance is small or zero. For instance, the covariance between two independent random variables is zero, as we saw in section . It should come as no surprise that the slope of the linear regression depends on the covariance, that is, the degree to which the two variables deviate together from their means. If the covariance is positive, then for larger values of \\(x\\) the corresponding \\(y\\) values tend to be larger, which means the slope of the line is positive. Conversely, if the covariance is negative, so is the slope of the line. And if the two variables are independent, the slope has to be close to zero. The actual formula for the slope of the linear regression is : \\[\\begin{equation} m = \\frac{Cov(X,Y)}{Var(X)} \\end{equation}\\] I will not provide a proof that this slope generates the minimal sum of squared errors, but that is indeed the case. To find the intercept of the linear regression, we make use of one other property of the best fit line: in order for it to minimize the SSE, it must pass through the point \\((\\bar X, \\bar Y)\\). Again, I will not prove this, but note that the point of the two mean values is the central point of the ``cloud’’ of points in the scatterplot, and if the line missed that central point, the deviations will be larger. Assuming that is the case, we have the following equation for the line: \\(\\bar Y = m\\bar X + b\\), which we can solve for the intercept \\(b\\): \\[\\begin{equation} b = \\bar Y - \\frac{Cov(X,Y) \\bar X}{Var(X)} \\end{equation}\\] 4.2.3 Execises Body leanness (B) and heat loss rate (H) in boys; partial dataset from B(\\(m^2/kg\\)) H(\\(^\\circ C /min)\\) 7.0 0.103 5.0 0.091 3.6 0.014 3.3 0.024 2.4 0.031 2.1 0.006 Use the data set in table to answer the following questions: Compute the means and standard deviations of each variable. Compute the covariance between the two variables. Calculate the slope and intercept of the linear regression for the data with \\(B\\) as the explanatory variable. Make a scatterplot of the data set with \\(B\\) as the explanatory variable and sketch the linear regression line with the parameters you computed. Calculate the slope and intercept of the linear regression the data with \\(H\\) as the explanatory variable. Make a scatterplot of the data set, with \\(H\\) as the explanatory variable and sketch the linear regression line with the parameters you computed. 4.2.4 correlation and goodness of fit} The correlation between two random variables is a measure of how much variation in one corresponds to variation in the other. If this sounds very similar to the description of covariance, it’s because they are closely related. Essentially, correlation is a normalized covariance, restricted to lie between -1 and 1. Here is the definition: Definition 4.2 The (linear) correlation of a dataset of pairs of data values (X,Y) is: \\[ r = \\frac{Cov(X,Y)}{\\sqrt{{Var(X)}{Var(Y)}}} = \\frac{Cov(X,Y)}{\\sigma_X \\sigma_Y} \\] If the two variables are identical, \\(X=Y\\), then the covariance becomes its variance \\(Cov(X,Y) = Var(X)\\) and the denominator also becomes the variance, and the correlation is 1. This is also true if \\(X\\) and \\(Y\\) are scalar multiples of each other, as you can see by plugging in \\(X= cY\\) into the covariance formula. The opposite case if \\(X\\) and \\(Y\\) are diametrically opposite, \\(X = -cY\\), which has the correlation coefficient of -1. All other cases fall in the middle, neither perfect correlation nor perfect anti-correlation. The special case if the two variables are independent, and thus their covariance is zero, has the correlation coefficient of 0. This gives a connection between correlation and slope of linear regression: \\[\\begin{equation} m = r \\frac{\\sigma_Y}{\\sigma_X} \\label{eq:slope_corr} \\end{equation}\\] Whenever linear regression is reported, one always sees the values of correlation \\(r\\) and squared correlation \\(r^2\\) displayed. The reason for this is that \\(r^2\\) has a very clear meaning of the the fraction of the variance of the dependent variable \\(Y\\) explained by the linear regression \\(Y=mX+b\\). Let us unpack what this means. According to the stated assumptions of linear regression, the response variable \\(Y\\) is assumed to be linear relationship with the explanatory variable \\(X\\), but with independent additive noise (also normally distributed, but it doesn’t play a role for this argument). Linear regression captures the linear relationship, and the remaining error (residuals) represent the noise. Thus, each value of \\(Y\\) can be written as \\(Y = R + \\hat Y\\) where \\(R\\) is the residual (noise) and the value predicted by the linear regression is \\(\\hat Y =mX+b\\). The assumption that \\(R\\) is independent of \\(Y\\) means that \\(Var(Y) = Var (\\hat Y) + Var (R)\\) because variance is additive for independent random variables, as we discussed in section . By the same reasoning \\(Cov(X,\\hat Y + R) = Cov(X,\\hat Y) + Cov(X,R)\\). These two covariances can be simplified further: \\(Cov(X,R) = 0\\) because \\(R\\) is independent random noise. \\(X\\) and the predicted \\(\\hat Y\\) are perfectly correlated, so \\(Cov(X,\\hat Y) = Cov(X,mX+b) = Var(X) = Var(\\hat Y)\\). This leads to the derivation of the meaning of \\(r^2\\): \\[\\begin{equation} \\begin{aligned} r^2 = \\frac{Cov(X,Y)^2}{Var(X) Var(Y)} &amp;= \\frac{(Cov(X,\\hat Y) + Cov(X,R) )^2}{Var(X) Var(Y)} = \\\\ =\\frac{Var(X)Var(\\hat Y)}{Var(X) Var(Y)} &amp;= \\frac{Var(\\hat Y)}{Var(Y)} \\end{aligned} \\label{eq:ch8_frac_var} \\end{equation}\\] One should be cautious when interpreting results of a linear regression. First, just because there is no linear relationship does not mean that there is no other relationship. Figure shows some examples of scatterplots and their corresponding correlation coefficients. What it shows is that while a formless blob of a scatterplot will certainly have zero correlation, so will other scatterplots in which there is a definite relationship (e.g. a circle, or a X-shape). The point is that correlation is always a measure of the linear relationship between variables. The second caution is well known, as that is the danger of equating correlation with a causal relationship. There are numerous examples of scientists misinterpreting a coincidental correlation as meaningful, or deeming two variables that have a common source as causing one another. For example, one can look at the increase in automobile ownership in the last century and the concurrent improvement in longevity and conclude that automobiles are good for human health. It is well-documented, however, that a sedentary lifestyle and automobile exhaust do not make a person healthy. Instead, increased prosperity has increased both the purchasing power of individuals and enabled advances in medicine that have increase our lifespans. To summarize, one must be careful when interpreting correlation: a weak one does not mean there is no relationship, and a strong one does not mean that one variable causes the changes in the other. There is another important measure of the quality of linear regression: the residual plot. The residuals are the differences between the predicted values of the response variable and the actual value from the data. As stated above, linear regression assumes that there is a linear relationship between the two variables, plus some uncorrelated noise added to the values of the response variable. If that were true, then the plot of the residuals would look like a vaguely spherical blob, with a mean value of 0 and no discernible trend (e.g. no increase of residual for larger \\(x\\) values). Visually assessing residual plots is an essential check on whether linear regression is a reasonable fit to the data in addition to the \\(r^2\\) value. 4.2.5 Exercises Figure shows scatterplots of the rate of oxygen consumption (VO) and heart rate (HR) measured in two macaroni penguins running on a treadmill (really). The authors performed linear regression on the data and found the following parameters: \\(VO =0.23HR - 11.62\\) (penguin A) and \\(VO =0.25HR - 20.93\\) (penguin B). The datasets have the standard deviations: \\(\\sigma_{VO} = 6.77\\) and $_{HR} = 28.8 $ (penguin A) and \\(\\sigma_{VO} = 8.49\\) and \\(\\sigma_{HR} = 30.6\\) (penguin B). Find the dimensions and units of the slope and the intercept of the linear regression for this data (the units of HR and VO are on the plot). Data set B has a larger slope than data set A. Does this mean the correlation is higher in data set B than in A? Explain. Calculate the correlation coefficients for the linear regressions of the two penguins; explain how much variance is explained in each case. Re-calculate the slopes of the two linear regressions if the explanatory and response variables were reversed. Does changing the order of variable affect the correlation? 4.3 Linear regression using R We now have the tools to compute the parameters of the best-fit line, provided we can calculate the means, variances, and covariance of the two variable data set. Of course, the best way to do all this is to let a computer handle it. The function for calculating linear regression in R is lm(), which outputs a bunch of information to a variable called myfit in the script below. The slope, intercept, and other parameters can be printed out using the summary() function. In the script below you see a bunch of information, but we are concerned with the ones in the first column correspond to the best fit intercept (-166.2847) and the slope (2.4432). You can check that they correspond to our formulas by computing the covariance, the variances, and the means of the two variables: my_data &lt;- read.table(&quot;data/HR_temp.txt&quot;, header = TRUE) myfit &lt;- lm(HR ~ Temp, my_data) summary(myfit) ## ## Call: ## lm(formula = HR ~ Temp, data = my_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -16.6413 -4.6356 0.3247 4.8304 15.8474 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -166.2847 80.9123 -2.055 0.04190 * ## Temp 2.4432 0.8235 2.967 0.00359 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.858 on 128 degrees of freedom ## Multiple R-squared: 0.06434, Adjusted R-squared: 0.05703 ## F-statistic: 8.802 on 1 and 128 DF, p-value: 0.003591 m &lt;- cov(data$HR, data$Temp)/var(my_data$Temp) print(m) ## [1] 2.443238 b &lt;- mean(my_data$HR) - m * mean(data$Temp) print(b) ## [1] -166.2847 Here Temp and HR are the explanatory and response variables, respectively, and my_data is the name of the data frame they are stored in. The best fit parameters are stored in myfit, and the line can be plotted using abline(myfit). The script below shows how to calculate a linear regression line and then plot it over a scatterplot in R, and the result is shown in figure a. plot(my_data$Temp, my_data$HR, main = &quot;scatterplot and linear regression line&quot;, cex = 1.5, cex.axis = 1.5, cex.lab = 1.5) abline(myfit) HRresiduals &lt;- resid(myfit) plot(data$Temp, HRresiduals, main = &quot;residuals plot&quot;, cex = 1.5, cex.axis = 1.5, cex.lab = 1.5) abline(0, 0) Figure 4.2: Linear regression for a data set of heart rates and body temperatures (a); and the residuals (b). However, what does this mean about the quality of the fit? Just because we found a line to draw through a scatterplot does not mean that this line is meaningful. In fact, looking at the plot, there does not seem to be much of a relationship between the two variables. There are various statistical measures for the significance of linear regression, the most important one relies on the correlation between the two data sets. Look again at the summary statistics for the data set of heart rates and temperatures. There are several different statistics here, and the one that we care about is the \\(r^2\\), which is reported here as ‘Multiple R-squared’. This number tells us that the linear regression accounts for only about 6% of the total variance of the heart rate. In other words, there is no significant linear relationship in this data set. As mentioned in section , the other important check is plotting the residuals of the data set, after the linear fit is subtracted. You see the result in figure b, showing that the residuals do not have any pronounced pattern. So it is reasonable to conclude that linear regression was a reasonable model to which to fit the data. The low correlation is because data seem to have little to no relationship, not because there is some complicated nonlinear relationship. 4.4 Regression to the mean The phenomenon called regression to the mean is initially surprising. Francis Galton first discovered this by comparing the heights of parents and their offspring. Galton took a subset of parents who are taller than average and observed that their children were, on average, shorter than their parents. He also compared the heights of parents who are shorter than average, and found that their children were on average taller than their parents. This suggests the conclusion that in long run everyone will converge closer to the average height - hence “regression to mediocrity”, as Galton called it . But that is not the case! The parents and children in Galton’s experiment had a very similar mean and standard deviation. This appears to be a paradox, but it is easily explained using linear regression. Consider two identically distributed random variables \\((X,Y)\\) with a positive correlation \\(r\\). The slope of the linear regression is \\(m = r \\sigma_Y/\\sigma_X\\) and since \\(\\sigma_Y=\\sigma_X\\), the slope is simply \\(r\\). Select a subset with values of \\(X\\) higher than \\(\\bar X\\), and consider the mean value of \\(Y\\) for that subset. If the slope \\(m&lt;1\\) (the correlation is not perfect), then the mean value of \\(Y\\) for that subset is less than the mean value of \\(X\\). Similarly, for a subset with values of \\(X\\) lower than \\(\\bar X\\), the mean value of \\(Y\\) for that subset is greater than the mean value of \\(X\\), again as long as the slope is less than 1. Figure shows Galton’s data set (available in R by installing the package ‘HistData’) along with the linear regression line and the identity like (\\(y=x\\)). If each child had exactly the same height as the parents, the scatterplot would lie on the identity line. Instead, the linear regression lines have slope less than 1 for both the plot with the parental heights as the explanatory variable and for the plot with the variables reversed. The correlation coefficient \\(r\\) does not depend on the order of the variables; so using the equation we can see the difference in slopes is explained by the two data sets having different standard deviations, and reversing the explanatory and response variables results in reciprocation of the ratio of standard deviations. The children’s heights have a higher standard deviation, which is likely an artifact of the experiment. In the data set the heights of the two parents were averaged to take them both into account, which substantially reduces the spread between male and female heights. To summarize, although the children of taller parents are shorter on average than their parents, and the children of shorter parents are taller than their parents, the overall standard deviation does not decrease from generation to generation. 4.5 Linear regression: parameters, plots, and residuals Here is an example of a linear regression performed and the line plotted over the basic R plot. Note that lm() uses the following syntax to indicate which variable is which: lm(Y ~ X) (where Y is the response variable and X is the explanatory variable.) myfit &lt;- lm(child ~ parent, Galton) summary(myfit) ## ## Call: ## lm(formula = child ~ parent, data = Galton) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.8050 -1.3661 0.0487 1.6339 5.9264 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 23.94153 2.81088 8.517 &lt;2e-16 *** ## parent 0.64629 0.04114 15.711 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.239 on 926 degrees of freedom ## Multiple R-squared: 0.2105, Adjusted R-squared: 0.2096 ## F-statistic: 246.8 on 1 and 926 DF, p-value: &lt; 2.2e-16 print(paste(&quot;The best-fit slope is: &quot;, myfit$coefficients[2])) ## [1] &quot;The best-fit slope is: 0.646290581993716&quot; print(paste(&quot;The best-fit intercept is: &quot;, myfit$coefficients[1])) ## [1] &quot;The best-fit intercept is: 23.9415301804085&quot; The summary outputs a whole bunch of information that is returned by the lm() function, as the object myfit. The most important are the intercept and slope, which may be printed out as shown above, and the R-squared parameter, also called the coefficient of determination. The value of R-squared is not accessible directly in myfit, but it is printed out in the summary (use multiple R-squared for our assignments.) The actual best-fit line can be plotted as follows over a scatterplot of the data; notice that abline can take myfit as an input and use the slope and intercept: #Overlay the best-fit line on the base R plot plot(Galton$parent, Galton$child, xlab=&#39;mid-parent height (inches)&#39;, ylab=&#39;child height (inches)&#39;) abline(myfit) After performing linear regression it is essential to check that the residuals obey the assumptions of linear regression. The residuals are the difference between the predicted response variable values and the actual values of the response variable, in this case the child height. The residuals are contained in the object myfit as variable residuals: plot(Galton$parent, myfit$residuals, xlab=&#39;mid-parent height (inches)&#39;, ylab=&#39;residuals (inches)&#39;) abline(0,0) It appears that the residuals meet the assumptions of being independent of measurement (shapeless scatterplot), are centered at zero, and look roughly normally distributed, although that can be checked more carefully using other tools. 4.5.1 Exercises: Calculate descriptive statistics (mean and standard deviation) of the residuals from the linear regression above. What do you expect them to be, and how do they differ from the expectation? Using this calculation, check that the coefficient of determination really captures the fraction of total variance explained by linear regression Perform linear regression on the Galton data set with the response and explanatory variables switched, and report which parameters changed and how. Plot the residuals from your new linear regression and calculate and report their descriptive statistics. What do you expect them to be, and how do they differ from the expectation? Using this calculation, check that the coefficient of determination really captures the fraction of total variance explained by linear regression. "],
["intro.html", "5 Introduction", " 5 Introduction You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 5. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter ??. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 5.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 5.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 5.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 5.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package (Xie 2020) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). References "],
["references.html", "References", " References Xie, Yihui. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. http://yihui.org/knitr/. ———. 2020. Bookdown: Authoring Books and Technical Documents with R Markdown. https://CRAN.R-project.org/package=bookdown. "]
]
